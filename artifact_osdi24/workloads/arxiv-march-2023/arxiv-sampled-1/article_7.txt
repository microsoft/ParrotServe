

theoremTheorem
corCorollary
exampleExample





























































Distance Evaluation to the Set of Defective Matrices
    
Alexei Yu. Uteshev[The corresponding author], Elizaveta A. Kalinina, Marina V. Goncharova 



1010

St. Petersburg State University

1010Faculty of Applied Mathematics 

1010St. Petersburg, Russia

99^1 {alexeiuteshev,ekalinina,marina.yashina}@gmail.com





101099
    March 30, 2023
==============================================================================================================================================================================================================================================================================

empty




We treat the problem of the Frobenius distance evaluation from a given matrix A âˆˆâ„^nÃ— n with distinct eigenvalues to the manifold  of matrices with multiple eigenvalues. On restricting considerations to the  
rank 1 real perturbation matrices, we prove that the distance in question equals âˆš(z_âˆ—) where 
z_âˆ— is a  positive (generically, the least positive) zero of the algebraic equation

    â„±(z) = 0,   â„±(z):= ğ’Ÿ_Î»( 
    [ (Î» I - A)(Î» I - A^âŠ¤)-z I_n ] )/z^n

and ğ’Ÿ_Î» stands for the 
discriminant of the polynomial treated with respect to Î». In the framework of this approach we also provide the procedure for finding the nearest to A matrix with multiple eigenvalue. Generalization of the problem to the case of complex perturbations is also discussed. Several examples are presented clarifying the computational aspects of the approach.




Keywords: Wilkinson's problem, defective matrix, multiple eigenvalues, distance equation


MSC 2010: 68W30, 15A18 , 12D10, 58C40




































Â§ INTRODUCTION











The origin of the problem of finding the distance from a matrix A âˆˆâ„‚^nÃ— n
to the set ğ”» of matrices with multiple eigenvalues can be traced back to WilkinsonÂ <cit.> who posed it in relation to the sensitivity analysis of matrix eigenvalues. The desired distance  further will be treated  with respect to either the 2-norm or to the Frobenius norm in â„‚^nÃ— n and will be 
denoted d(A, ğ”»). It is usually referred to as the Wilkinson
distance of A <cit.>. Alternatively, 
d(A, ğ”») can be defined as the 
infA-B where B belongs to the subset of defective matrices, i.e. those possessing at least one eigenvalue 
whose geometric multiplicity  is less than its algebraic multiplicity.





Starting from  Wilkinson's worksÂ <cit.>, 
the problem of evaluation of d(A, ğ”») has been
studied intensively inÂ <cit.>. The most recent result is presented in the workÂ <cit.>. 
We briefly trace the developed approaches. Most of them are in the framework of singular value analysis of appropriate parameter dependent matrices.

The following theorem gives the min-max representation of d(A,ğ”») obtained by MalyshevÂ <cit.>.

 Let Aâˆˆâ„‚^nÃ— n.
Let the singular values of the matrix

    [[ A-Î» I_n   Î³ I_n;  ğ•†_nÃ— n A-Î» I_n ]]

be ordered like
Ïƒ_1(Î»,Î³)â‰¥Ïƒ_2(Î»,Î³)â‰¥â€¦â‰¥Ïƒ_2n(Î»,Î³)
â‰¥ 0. Then the  2-norm distance  d(A,ğ”») can be evaluated as 

    d(A,ğ”»)=min_Î»âˆˆâ„‚max_Î³â‰¥0Ïƒ_2n-1(Î»,Î³)  .



The straight computation of this distance is quite difficult, so to find this distance, in many works the notion of pseudospectraÂ <cit.> is used.

Definition. For both the 2-norm and the Frobenius norm, the Îµ-pseudospectra of a matrix A
is

    Î›_Îµ(A)={Ïƒ_min<Îµ}

where Îµ>0 and Ïƒ_min stands for the smallest
singular value of the matrix A-zI.

Equivalently,

    Î›_Îµ(A)={ zâˆˆâ„‚ | (A+E-zI)=0,  Eâˆˆâ„‚^nÃ— nE <Îµ}  .

If Î›_Îµ has n components, then A+E has n
distinct eigenvalues for all perturbations
Eâˆˆâ„‚^nÃ— n and hence A+E is not defective.

In subsequent papers, the pseudospectra approach is used to find the distance to the nearest defective matrix. 

InÂ <cit.>, a geometric solution to the problem of finding d(A,ğ”») in Frobenius norm is given. The nearest defective
matrix is related to the critical points of 
the minimal singular value Ïƒ_min(x,y) of the matrix A-(x+ğ¢ y)I that could
be obtained by examination of pseudospectra of A. For an
approximation of a multiple eigenvalue of the nearest defective
matrix, the averaging heuristic by Puiseux series is proposed. Also
an iterative method for finding this eigenvalue
together with the minimal perturbation is presented.

InÂ <cit.>, it is proposed to find the smallest
perturbation E such that the components of the pseudospectra of
A+E coalesce. The problem is reformulated as follows. One needs to
find
zâˆˆâ„‚,Îµâˆˆâ„,Îµ>0 and U,Vâˆˆâ„‚^n,such that[Hereinafter ^âŠ¤ stands for the transpose while ^ğ–§ stands for the Hermitian transpose.]

    (A-zI)V-Îµ U=ğ•†_nÃ— 1,Îµ V-(A-zI)^ğ–§ U=ğ•†_nÃ— 1,  U^ğ–§ V=0   .

The algorithm to solve the system of equations presented in this
work is rather expensive because it requires the repeated
calculation of pseudospectra. Also any condition of coalescence of
two pseudospectral curves is necessary.

InÂ <cit.>, a new computational approach to approximating the nearest defective matrix by a variant of Newton's method is  suggested. 

The Implicit Determinant Method based on standard Newton's method
is used to solve the systemÂ (<ref>) inÂ <cit.>.

There are several works considering generalizations of Wilkinson's problem for the cases of prescribed eigenvalues or their  multiplicities Â <cit.>,
 and matrix pencilsÂ <cit.>.

The approaches developed in the above cited papers could be characterized as related to the Numerical Linear Algebra. The present paper aims at solving the stated problem for the case of Frobenius norm within the framework of symbolic computation approach. Namely, we reduce the problem to that of the univariate polynomial equation solving. As a matter of fact, the manifold ğ”» of  matrices with multiple eigenvalues in the â„^n^2 space of their entries is an algebraic one, i.e. it is represented by a multivariate polynomial equation. If we slightly modify  Wilkinson's problem to that of finding d^2(A,ğ”»), then the constrained optimization problem becomes an algebraic one in the sense that both the objective function and the constraint be polynomials.  Application of the Lagrange multipliers method reduces the problem to that of system of algebraic equations solving. The latter can be resolved, at least in theory, via the analytical procedure of elimination of variables consisting in the multivariate resultant computation or the GrÃ¶bner basis construction. Application of these procedures to the system of equations of the treated problem, complemented with z-d^2(A,ğ”»)=0, results in a univariate equation â„±(z)=0 whose zero set contains all the  critical values of the squared distance function. This equation will be further referred to as the distance equation and its computation is the priority of the present paper. 

This approach has been developed in <cit.>.
Unfortunately, soon after that publication, a significant gap in reasoning was discovered. It was assumed that the value d(A, ğ”») could be provided by only the rank 1 perturbation matrix E_âˆ— and that the nearest to A matrix B_âˆ—=A+E_âˆ— in ğ”» might possess only a double real eigenvalue.
In Section <ref>, an example of the order 4 matrix A is given where the nearest  in ğ”»  matrix
  possesses a pair of double complex-conjugate eigenvalues. 
As yet we failed to manage this scenario for the general statement of the problem; neither do we able to claim that it is a zero probability event. 

We confine ourselves here to considering the case where the critical values of d^2(A,ğ”») are provided only by the rank 1 perturbation matrices. For this case,  
the practical implementations of the elimination of variables procedure mentioned above  can be reduced to just only two bivariate equations. 
One of these equations follows quite naturally from the developed in <cit.> approach. This is 

    Î¦(Î»,z)=0   Î¦(Î»,z):=
    [ (Î» I - A)(Î» I - A^âŠ¤)-z I_n ]
       .

The more difficulties causes the deduction of the second equation. It happens to be 
    âˆ‚Î¦(Î»,z)/ âˆ‚Î» =0   .

To obtain the distance equation, it is then sufficient to eliminate the variable Î» from the obtained system. This can be managed with the aid of discriminant computation, i.e. the function of the coefficients of a polynomial responsible for the existence of a multiple zero for this polynomial.
We recall some basic features of this function in Section <ref>.

In Section <ref>, we prove the main result of the paper, namely that the value d^2(A,ğ”») is in the set of non-negative zeros of the distance equation. If A âˆ‰ğ”» then generically d^2(A,ğ”») equals the least positive zero z_âˆ— of this equation. We also detail here the structure of the matrix B_âˆ— nearest  to A in ğ”». It appears that the multiple eigenvalue of  B_âˆ— coincide with the multiple zero of the polynomial Î¦(Î»,z_âˆ—).

In Section <ref>, computational aspects of the proposed approach are discussed via solving the  problem for the two families of matrices treated in  the literature. 

In Section <ref>, we address to the generalization of Wilkinson's problem to the case of complex perturbations. Here the results are presented in a very concise manner with the potential intention of returning to them in future articles.

Notation is kept to correlate with <cit.>. For a matrix A âˆˆâ„^nÃ— n,
f_A(Î») denotes its characteristic polynomial,

d(A, ğ”») denotes the distance from A to the set ğ”» of matrices possessing a multiple eigenvalue.  E_âˆ— and B_âˆ— = A+ E_âˆ— stand for, respectively, the (minimal) perturbation matrix and the nearest to A  matrix in ğ”» (i.e. d(A,ğ”»)=A- B_âˆ—); we then term by Î»_âˆ— the multiple eigenvalue of B_âˆ—. I (or I_n) denotes the identity matrix (of the corresponding order). ğ’Ÿ (or ğ’Ÿ_Î») denotes the discriminant of a polynomial (with subscript indicating the variable).

Remark. All the computations were performed in CAS Maple 15.0 with those approximate done within the accuracy 10^-40. In the paper they are presented rounded to 10^-6.



Â§ ALGEBRAIC PRELIMINARIES








It is well-known that in the (N+1)-dimensional space of the polynomial 

    F(x)=a_0x^N + a_1x^N-1 +â€¦+a_N âˆˆâ„‚[x],   a_0 0, N â‰¥ 2

coefficients, the manifold of polynomials with multiple zeros 
is given by the equation

    D(a_0,a_1,â€¦,a_N)=0    
    D:=ğ’Ÿ_x(F(x))

denotes the discriminant of the polynomial. Discriminant is formally defined as a symmetric function of the zeros {Î»_1,â€¦, Î»_N } of the polynomial F(x)

    D_x(F(x))= a_0^2N-2âˆ_1â‰¤ j < k â‰¤ N (Î»_k - Î»_j)^2   .

This representation gives rise to further transformation of the discriminant into the homogeneous polynomial D(a_0,a_1,â€¦,a_N) of the order 2N-2 with respect to the coefficients of F(x). Such a transformation  can be implemented through a preliminary representation of discriminant in an appropriate determinantal form. We will follow the approach based on the Hankel matrix formalism <cit.>.


For this aim, find first the Newton sums s_0,s_1,â€¦,s_2N-2 of the polynomial F(x) with the aid of recursive formulas

    s_0=N, s_1=-a_1/a_0,


    s_k={[ -(a_1s_k-1+a_2s_k-2+â€¦+a_k-1s_1+a_kk)/a_0,                                    kâ‰¤ N ,;      -(a_1s_k-1+a_2s_k-2+â€¦+a_Ns_k-N)/a_0,                                   k > N , ].

and compose the Hankel matrix

    S=[s_j+k]_j,k=0^N-1 =
    [[    s_0    s_1    s_2      â€¦  s_N-2  s_N-1;    s_1    s_2    s_3      â€¦  s_N-1    s_N;    s_2    s_3    s_4      â€¦    s_N  s_N+1;      â€¦                    â€¦;  s_N-1    s_N  s_N+1      â€¦ s_2N-3 s_2N-2 ]]_NÃ— N .

Denote by S_1,â€¦, S_N= S its leading principal minors.

   One has

    ğ’Ÿ(F)=a_0^2N-2 S_N   .

The condition 

    S_N=0, â€¦, S_N-k+1=0, S_N-k 0

is the necessary and sufficient for the polynomial F(x) to possess k 
common zeros with F^' (x).
In particular, if S_N=0, S_N-1 0, then F(x) possesses a unique multiple zero and the multiplicity of this zero equals 2. This zero can be computed via the formula

    Î» = 
    s_1-1/S_N-1|
    [    s_0    s_1      â€¦  s_N-3  s_N-1;    s_1    s_2      â€¦  s_N-2    s_N;      â‹®                           â‹®;  s_N-2  s_N-1      â€¦ s_2N-1 s_2N-3 ]|    .

The determinant in the right-hand side is constructed by deleting the last row and the last but one column in 
S.



Consequently, the set ğ”» of matrices with multiple eigenvalues is given by the equation

    ğ’Ÿ_Î»( (Î» I-B) ) =0   .


For the case of polynomials with real coefficients, the sequence of leading principal minors of the matrix S permits one to establish the exact number of real zeros for F(x) <cit.>.

 Let 

    S_N=0, â€¦, S_N-k+1=0, S_N-k 0,â€¦,S_1  0

Then the number of distinct pairs of complex-conjugate zeros for F(x) âˆˆâ„[x] equals

    ğ’±(1,S_1,â€¦, S_N-k)

where ğ’± denotes the number of variations of sign in the given sequence.


In the space â„^N+1 of polynomials (<ref>) with real coefficients, the discriminant manifold (<ref>) separates the domains of vectors providing the coefficients of polynomials with the same number of real zeros. 

The last comment of the present section relates to application of discriminant to one problem from Elimination Theory. Consider a bivariate polynomial F(x,y)âˆˆâ„[x,y],  F â‰¥ 2. The discriminant furnishes the tool for eliminating the variable x from the system of equations

    F(x,y)=0, âˆ‚ F(x,y)/ âˆ‚ x=0   .
    

Namely, if (x_0,y_0) is a solution to the system (<ref>), then y_0  is necessarily a zero of the algebraic univariate equation

    ğ’´(y)=0   ğ’´(y):=ğ’Ÿ_x(F(x,y))   .

The reverse statement is subject to an extra assumption. If y_1âˆˆâ„‚ is a zero for ğ’´(y), then there exists a multiple zero for the polynomial F(x,y_1).  Under the assumption that y_1 is a simple zero for ğ’´(y),
x_1 is a  unique multiple zero and its  multiplicity equals 2. Then it can be expressed as a rational function of y_1  using the result of Theorem <ref>. These considerations are valid for all the solutions of the system (<ref>) provided that ğ’Ÿ_y(ğ’´(y)) 0.






Â§ DISTANCE EQUATION










In terms of the discriminant manifold referred to in the previous section, the problem of evaluation of d^2 (A,ğ”») is equivalent to that of constrained optimization

    minB-A^2    ğ’Ÿ_Î»(f_B(Î»))=0, Bâˆˆâ„^nÃ— n  .

Here the constraint is an algebraic equation with respect to the entries of the matrix B.
Traditional application of the Lagrange multipliers method reduces the problem to that of solving a system of n^2+1 nonlinear algebraic equations. Under the additional assumption the matrix B_âˆ—âˆˆâ„^nÃ— n providing a solution to this system possesses only one multiple eigenvalue and its multiplicity equals 2, it is possible to  reduce the number of variables in the constrained optimization approach. The following result is presented in <cit.>:



The value d^2(A,ğ”») belongs to the set of critical values of the objective function

    G(U):=U^âŠ¤A A^âŠ¤ U - ( U^âŠ¤AU )^2

for the constrained optimization problem under constraints 

    U^âŠ¤U=1, U âˆˆâ„^n   .

If U_âˆ— be the point providing d^2(A,ğ”»), then the perturbation can be computed as

    E_âˆ—=U_âˆ— U_âˆ—^âŠ¤ (Îº I-A)    Îº:= U_âˆ—^âŠ¤ A U_âˆ—  .



The new optimization problem still have significant number of variables. We aim to eliminate all of them but introduce an extra one responsible for the critical values of the objective function. 

Stationary points of the function
(<ref>) under the constraints
(<ref>) can be found via Lagrange  method applied to the function G(U)- Î¼ (U^âŠ¤U-1). This results into the system

    AA^âŠ¤U-(U^âŠ¤ A U)(A+A^âŠ¤)U-Î¼ U = ğ•†_nÃ— 1  .

Denote

    Î»:=U^âŠ¤ A U   .

 Then the equation (<ref>) has a nontrivial solution with respect to  U if and only if
 
    (AA^âŠ¤-Î» (A+A^âŠ¤)-Î¼ I)=0   .

 Under this condition, multiplication of 
  (<ref>) by U^âŠ¤ yields
 
    U^âŠ¤AA^âŠ¤U=2Î»^2+Î¼  .

 Wherefrom it follows that the critical values of the objective function
  (<ref>) are given by 
 
    z=Î»^2+Î¼  .

 Substitution this into (<ref>) results in the equation connecting  z and Î»:

    Î¦(Î»,z)=0

 where

    Î¦(Î»,z):=
    [ A A^âŠ¤- Î» (A+A^âŠ¤)+(Î»^2-z) I ]


    =[ (Î» I - A)(Î» I - A)^âŠ¤-z I ]

Zeros z_1,â€¦, z_n of the polynomial Î¦(Î»,z)  with respect to the variable z are evidently real since they are the squares of the singular values for the matrix Î» I-A.

Our further task is to deduce an extra equation connecting 
Î» and z.

 The value d^2(A,ğ”») belongs to the set of non-negative zeros of 
the polynomial

    â„±(z)â‰¡ğ’Ÿ_Î»(Î¦(Î»,z))/z^n   .




Proof. Under the condition (<ref>), there exists  a nontrivial solution  for (<ref>) with respect to the column  U

    (Î» I-A)(Î» I-A)^âŠ¤U=z  U    .

This equality means that U is the right 
singular vector for the matrix Î» I - A corresponding to the singular value âˆš(z). The corresponding left singular vector for that matrix can be found from the equality

    âˆš(z)V:=(Î» I - A)^âŠ¤U   .

Dual relationship is valid for U:

    âˆš(z)U=(Î» I - A)V   .

From the conditions (<ref>) and (<ref>)




it follows that

    U^âŠ¤(Î» I - A)U=0   .

Multiply (<ref>) from the left by U^âŠ¤. From (<ref>), it follows that

    âˆš(z)=U^âŠ¤(Î» I - A)V   .

Multiply (<ref>) from the left by V^âŠ¤ and utilize (<ref>):

    âˆš(z)V^âŠ¤V=V^âŠ¤(Î» I - A)^âŠ¤U=âˆš(z)  .

Wherefrom the two alternatives follow

    V^âŠ¤V=1     âˆš(z)=0   .

Similarly, multiplication of (<ref>) from the left by U^âŠ¤ and further application of (<ref>) yields

    âˆš(z)U^âŠ¤V=0   .

This also leads to two alternatives:

    U^âŠ¤V=0     âˆš(z)=0   .

Ignore the case âˆš(z)=0. 

    V^âŠ¤V=1,  U^âŠ¤V=0   .

Consider the equation (<ref>) as a definition of the âˆš(z) as the function of Î». Differentiate this relation with respect to Î»:

    d âˆš(z)/d Î»=U^âŠ¤V+d  U^âŠ¤/d Î» (Î» I - A)V+U^âŠ¤ 
    (Î» I - A)  d  V/d Î»  .

With the aid of (<ref>) and (<ref>) transform this into

    U^âŠ¤V+âˆš(z)[ d  U^âŠ¤/d Î» U + V^âŠ¤d  V/d Î»]   .

Due to (<ref>) and (<ref>), we arrive at 

    d âˆš(z)/d Î» = 0   .


Equation (<ref>) defines implicit function z(Î»). Differentiation of the identity Î¦(Î»,z(Î»))â‰¡ 0 with respect to Î» yields the identity

    Î¦^'_Î»(Î»,z)+Î¦^'_z(Î»,z) d  z/d Î»â‰¡ 0   .

Under the condition (<ref>), the variables Î» and z are linked by an extra relationship

    Î¦^'_Î»(Î»,z)=0   .

Together with (<ref>),  the deduced condition composes the system of algebraic equations

    Î¦ (Î»,z)=0, Î¦^'_Î»(Î»,z)=0    .

According with the results of Section <ref>, elimination of Î» from this system can be implemented with the aid of the discriminant computation, i.e. the variable z should  satisfy the equation

    ğ’Ÿ_Î» (Î¦ (Î»,z))=0   .

To prove the validity of (<ref>), it is necessary to additionally confirm that the left-hand side of the last equation is divisible by z^n. This is indeed the case, since the polynomial Î¦(Î»,0) possesses n multiple zeros coinciding with the eigenvalues of the matrix A. 

With â„±(z) given by (<ref>), the distance equation â„±(z) =0 is now well-defined and in Section <ref> we discuss some of related features and computational aspects. 

To conclude the present section, we have to detail the properties of the Î»-component for the solution of the system (<ref>).
Let the polynomial â„±(z) defined by (<ref>) possess a positive real zero z_0 and this zero be simple. Then the polynomial Î¦ (Î»,z_0) has a unique multiple zero and multiplicity of this zero equals 2. We denote by Î»_0. It is evidently real and can be expressed as a rational function of z_0 via, for instance, formula (<ref>). 

The less evident conclusion is as follows: this multiple zero coincides with the multiple eigenvalue of the matrix in ğ”» providing the critical value z_0 for the function d^2(A,ğ”»). 


 
For any real solution (Î»_0,z_0) of the systemÂ (<ref>)  where
z_0 0, there exists the rank 1 perturbation E_0 such that 
E_0=âˆš(z_0) and the matrix B_0=A+E_0 possesses the multiple eigenvalue 
Î»_0.


Proof. The number âˆš(z_0) is a singular value  for the matrix Î»_0 I - A. We intend to prove that the matrix from the theorem statement is defined by the formula

    E_0:=âˆš(z_0)U_0 V_0^âŠ¤  ,

where U_0 and V_0 are respectively the left and the right singular vectors of the unit norm for the matrix
 Î»_0 I-A corresponding to âˆš(z_0).

Indeed, the matrix B_0=A+E_0  has Î»_0 as the eigenvalue corresponding to the eigenvector V_0: 

    B_0V_0=(A+E_0)V_0 
    (<ref>)= 
    AV_0+âˆš(z_0)U_0
    (<ref>)
    =AV_0+(Î»_0I-A)V_0=Î»_0V_0   .

If (B_0-Î»_0 I)<n-1 then the theorem is proved. Assume that (B_0-Î»_0 I)=n-1. Let us prove the existence of a column W such  that

    (B_0-Î»_0 I)W=V_0   .

The necessary and sufficient condition for resolving this equation consists in the fulfillment of the equality

    (B_0-Î»_0I)(B-Î»_0I)^+V_0=V_0

where ^+ stands for the Moore-Penrose inverse of the matrix. It can be easily verified that

    (B_0-Î»_0I)(B_0-Î»_0I)^+=I-U_0U_0^âŠ¤

(by assumption, (B_0-Î»_0 I)=n-1), and the condition (<ref>) is fulfilled:

    (B_0-Î»_0I)(B_0-Î»_0I)^+V_0=
    (I-U_0U_0^âŠ¤)V_0
    (<ref>)=
    V_0   .


The columns V_0 and W are linearly independent. 
Indeed, if

    Î± V_0+ Î² W=ğ•†_n Ã— 1   {Î±, Î²}âŠ‚â„

then on multiplying this equality from the left by
B_0-Î»_0 I it follows that
 Î² V_0 = ğ•†_n Ã— 1, and thus Î²=0. But then Î±=0 since V_0 is a nonzero column. 
 
Hence, 
 
    (B_0-Î»_0I)^2V_0=  ğ•†,  (B_0-Î»_0I)^2W=  ğ•†

 for the linear independent V_0 and W. Consequently,
 (B_0-Î»_0 I)^2â‰¤ n-2 and this gives evidence that Î»_0 should be a multiple eigenvalue for B_0. 
If A âˆ‰ğ”»,  then

    d(A,ğ”») = âˆš(z_âˆ—)  ,

where z_âˆ— is the minimal positive zero of the polynomial (<ref>) provided that this zero is not a multiple one. Minimal perturbation is evaluated by the formula

    E_âˆ—=U_âˆ—U_âˆ—^âŠ¤ (Î»_âˆ—I-A)   .

Here Î»_âˆ— is the multiple zero for the polynomial Î¦(Î»,z_âˆ—) and U_âˆ—âˆˆâ„^n, U_âˆ—=1 is the left singular vector of the matrix Î»_âˆ—I-A corresponding to the singular value âˆš(z_âˆ—).


The significance of condition for simplicity of the minimal positive zero z_âˆ— can be explained as follows. Since we are looking for only real perturbations, formula (<ref>) yields such a matrix if Î»_âˆ— is real. For the matrices of the order nâ‰¥ 4, it might happen that the system (<ref>) possesses a solution (z_âˆ—, Î»_âˆ—) with an imaginary Î»_âˆ— (we give an example of such a matrix in Section <ref>). Then the system necessarily possesses the solution (z_âˆ—,Î»_âˆ—). This implies (v. the last comment from Section <ref>) that  z_âˆ— should be a multiple zero for (<ref>). Therefore, the condition for simplicity of z_âˆ—is sufficient to prevent such an occasion. Formal verification of this condition can be replaced by a more general one relating the discriminant of â„±(z): 

    ğ’Ÿ_z(â„±(z)) 0   .






Â§ PROPERTIES OF THE DISTANCE EQUATION










 The distance equation for the matrix A=[a_jk ]_j,k=1^2 is found in the form

    â„±(z):=16[ (a_11-a_22)^2+(a_12+a_21)^2 ]Â·{[4z- ğ’Ÿ (f_A(Î»)) ]^2-16(a_12-a_21)^2z }=0  .

Polynomial in braces has only real zeros with respect to z since its discriminant equals

    256(a_12-a_21)^2[(a_11-a_22)^2+ (a_12+a_21)^2 ] â‰¥ 0   .






Some terms in the canonical representation of the polynomial (<ref>) can be explicitly expressed via the entries of the matrix A:

    Î¦(Î»,z)â‰¡Î»^2n- 2 (A) Î»^2n-1 +(-nz+ (AA^âŠ¤)+p_2  )  Î»^2n-2 +â€¦ +
    (AA^âŠ¤-zI)   .

Here p_2 is the coefficient of Î»^n-2 in the characteristic polynomial  f_A + A^âŠ¤(Î»):= (Î» I - A-A^âŠ¤). It happens that this polynomial is also responsible for the order of the distance equation.

  One has 

    â„±(z)â‰¡ 4^n [ ğ’Ÿ_Î» (f_A+A^âŠ¤(Î»)) ]^2 z^n(n-1) +  z   .
    


Proof. Let {Î¼_1,â€¦, Î¼_n } be the spectrum of the matrix A+A^âŠ¤ while Pâˆˆâ„^nÃ— n be an orthogonal matrix reducing it to the diagonal form: 

    P^âŠ¤(A+A^âŠ¤) P=  (Î¼_1,â€¦, Î¼_n)    .

Apply the same transformation to the determinant (<ref>):

    Î¦(Î»,z) â‰¡[
    P^âŠ¤AA^âŠ¤P +  (Î»^2-Î¼_1 Î»-z,â€¦,
    Î»^2-Î¼_n Î»-z)   .
    ]

The leading term  of the polynomial ğ’Ÿ_Î» (Î¦(Î»,z)) with respect to z coincide with that of 

    ğ’Ÿ_Î»(âˆ_j=1^n (Î»^2-Î¼_j Î»-z))   .

The set of zeros of the polynomial under the discriminant sign is as follows

    {1/2(Î¼_j Â±âˆš(Î¼_j^2+4z)) }_j=1^n   .

Using the definition (<ref>) of the discriminant, one gets 

    ğ’Ÿ_Î»(âˆ_j=1^n (Î»^2-Î¼_j Î»-z)) = 
    âˆ_j=1^n (4 z + Î¼_j^2) 
    âˆ_1â‰¤ j < k â‰¤ n[ z^2 (Î¼_k-Î¼_j)^4]   .

Coefficient of the monomial z^n^2 in the right-hand side can be recognized, via (<ref>),  as the square of the  discriminant of the characteristic polynomial of A+ A^âŠ¤. 

As for the determining the structure of the free term of â„±(z), our successes are restricted to the following 

Hypothesis. If computed symbolically with respect to  the entries of A, â„±(0) has a factor [ğ’Ÿ_Î» (f_A(Î»)) ]^2.

According to Theorem <ref>, the polynomial â„±(z) can be constructed in the form of determinant of a suitable Hankel matrix.
For this aim, compute first the Newton sums {s_j(z)}_j=0^4n-2 for  the polynomial Î¦(Î»,z) treated with respect to Î». Direct utilization of the formulas (<ref>) requires the canonical representation (<ref>) for the polynomial Î¦(Î»,z) while initially we have just only its representation in the determinantal form (<ref>). Fortunately, the Newton sums can be computed in an alternative way. 
Indeed, 

    Î¦(Î»,z) â‰¡ (Î» I_2n- W)    
    W:=[ [      A^âŠ¤ âˆš(z) I_n; âˆš(z) I_n        A ]]

and it is known that the Newton sums of the characteristic polynomial of a matrix can be computed as the traces of matrix powers:

    s_j(z) â‰¡(W^j)    j âˆˆ{0,1,â€¦}

Thus, one has

    s_2(z)=2 ((A^2)+nz), s_3(z)=2((A^3)+3  z (A)), â€¦

Compose the Hankel matrix

    S(z):=[ s_j+k(z) ]_j,k=0^2n-1

and compute the sequence of its leading principal minors S_1(z),â€¦,S_2n(z). Due to 
(<ref>) and (<ref>),  

    S_2n(z)â‰¡ğ’Ÿ_Î»(Î¦(Î»,z))â‰¡â„±(z) z^n   .

Evidently, the polynomial Î¦(Î»,0) possesses only n double zeros, and they all are distinct provided that A âˆ‰ğ”». Consequently, due to Theorem <ref>, one  has
S_n+1(0)=0,â€¦, S_2n(0)=0.


 Polynomial â„±(z) does not have negative zeros.
The number of its positive zeros lying within the interval
[0,z_0], z_0>0 is not less than 

    | ğ’±(1,S_1(z_0),â€¦,S_2n(z_0)) - ğ’±(1,S_1(0),â€¦,S_n(0)) |   .
 




Proof. The first claim of the theorem follows from the positive definiteness of the matrix (Î» I - A)(Î» I - A)^âŠ¤-z I for z< 0.

By Theorem <ref>, the number ğ’±(1,S_1(z_0),â€¦,S_2n(z_0)) equals the number of complex-conjugate pairs of zeros for the polynomial Î¦(Î»,z_0). When the parameter z varies from 0 to z_0, the discriminant ğ’Ÿ_Î» (Î¦(Î»,z)) vanishes at any value of z where a pair of real zeros of Î¦(Î»,z) transforms to a pair complex-conjugate ones or vice versa. The discriminant vanishes at these values. 

Theorem <ref> claims that the degree of the distance equation generically equals n(n-1). One can immediately watch that for the skew-symmetric matrix A this estimation is not valid. Moreover, for this type of matrices, polynomial â„±(z) vanishes identically. Some other types of matrices that permit explicit representation  
for the polynomial Î¦(Î»,z), and, as a consequence, for the value d(A, ğ”»), in terms of the spectrum of A can be found in <cit.>. We summarize those results in the following 

  Let all the eigenvalues Î»_1,â€¦,Î»_n of A be distinct. One has:

    Î¦(Î»,z)â‰¡âˆ_j=1^n [(Î»-c)^2-(Î»_j-c)^2-z]    A= + cI_n  ,

where c âˆˆâ„ is an arbitrary scalar;

    Î¦(Î»,z)â‰¡âˆ_j=1^n (Î»^2-z+1-2Î»(Î»_j))    A   ;


    Î¦(Î»,z)â‰¡âˆ_j=1^n [(Î»-Î»_j)^2-z ]     A  .

  
  
For the case (<ref>), Î¦(Î»,z) has a multiple zero if nâ‰¥ 2. For the case (<ref>), 
 Î¦(Î»,z) has a multiple zero if nâ‰¥ 3. For the both cases, the distance d(A, ğ”») is attained at the continuum of matrices in ğ”» <cit.>.
 
 Find d(A,ğ”») for the skew-symmetric matrix

    A=[ [   0  -4   2  -1;   4   0   7   3;  -2  -7   0  11;   1  -3 -11   0 ]]   .



Solution. Here

    Î¦(Î»,z)â‰¡( Î»^4-2 Î»^2z+200 Î»^2+z^2-200
     z+3249 )^2  ,

and ğ’Ÿ_Î» (Î¦(Î»,z)) â‰¡ 0. However, if we take 

    ğ’Ÿ_Î» (âˆš(Î¦(Î»,z)))=
    ğ’Ÿ_Î»(Î»^4-2 Î»^2z+200 Î»^2+z^2-200
     z+3249)

the result is the true distance equation

    11667456256 z^2-2333491251200 z+37907565375744=0   .

Its least positive zero equals 

    100-âˆš(6751) =1/4(âˆš(314)-âˆš(86))^2
 
where Â± 1/2 ğ¢ (âˆš(314)-âˆš(86)) are the eigenvalues of A. 

Remark. Similar trick works also for the case of orthogonal matrices.



Â§ EXAMPLES AND COMPUTATIONAL ASPECTS










Once the canonical form of the distance equation is computed, Wilkinson's problem is nearly solved.
Indeed, for a univariate algebraic equation, the  exact number of real zeros, as well as their location, could be trustworthy determined via purely algebraic procedures. 

Remark. Theorem <ref> claims that generically the degree of the distance equation equals n(n-1). The both examples below fall into this genericity. For instance,  one has â„±(z)=870 
for n=30.


   Find d(F_n,ğ”») for  Frank's matrix <cit.>

    F_n=[
    [   n n-1 n-2   â€¦   2   1; n-1 n-1 n-2   â€¦   2   1;   0 n-2 n-2   â€¦   2   1;   0   0 n-3   â€¦   2   1;   â‹®   â‹®       â‹±   â‹®   â‹®;   0   0   0   â€¦   1   1 ]]   .



Solution. For n=3, one has

    Î¦(Î»,z)=
    Î»^6-12 Î»^5+ ( -3 z+48 ) Î»^4
    + ( 24 z-74 ) Î»^3


    + ( 3 z^2-73 z+48
     ) Î»^2+ ( -12 z^2+70 z-12 ) Î»-
    z^3+25 z^2-33 z+1

and

    â„±(z)=
    23839360000 z^6-476315200000 z^5+3522206312000 z^4-
    11668368222400 z^3


    +16297635326400 z^2-6895772352000 z+
    230443315200   .

Distance equation has only real zeros, namely

    z_1 â‰ˆ 0.036482,  z_2 â‰ˆ 0.648383, z_3 â‰ˆ 2.316991,  
    z_4 â‰ˆ 4.954165, z_5 â‰ˆ 5.274176,  z_6 = 27/4=6.75   .

Thus, d(F_3,ğ”»)=âˆš(z_1)â‰ˆ 0.191004. To find the corresponding perturbation via (<ref>), first evaluate the multiple zero for Î¦(Î»,z_1) via (<ref>):

    Î»_âˆ—â‰ˆ 0.602966   .

Then evaluate the unit left singular vector of the matrix Î»_âˆ— I - A corresponding to âˆš(z_1):

    U_âˆ—â‰ˆ[0.639244,  -0.751157,  -0.164708]^âŠ¤

Finally, 

    E_âˆ—â‰ˆ[[ -0.019161 -0.041159  0.113343;  0.022516  0.048365 -0.133186;  0.004937  0.010605 -0.029204 ]]   .

The nearest to F_3 matrix in ğ”» 

    B_âˆ—=F_3+E_âˆ—â‰ˆ[[ 2.980838 1.958840 1.113343; 2.022516 2.048365 0.866813; 0.004937 1.010605 0.970795 ]]

possesses the spectrum {Î»_âˆ—, Î»_âˆ—, 6-2Î»_âˆ—â‰ˆ 4.794067 }.

  For n>3, the set of nonreal zeros for the distance equation becomes nonempty, and its cardinality, relative to that of real, increases fastly with n.




n      d(F_n,ğ”»)  â‰ˆ       coefficient size      number of real zeros     timing (s) 

 5     4.499950  Ã— 10^-3     âˆ¼ 10^50      12     - 

 10     3.925527 Ã— 10^-8     âˆ¼ 10^300     30     - 

 12     1.849890 Ã— 10^-10     âˆ¼ 10^480     34     0.13 

  20     3.757912 Ã— 10^-21     âˆ¼ 10^1690     62       5 

 30     1.638008 Ã— 10^-36     âˆ¼ 10^4450     102       30 
.


The results for F_10 and F_12 confirm estimations d_10â‰ˆ 3.93Â·10^-8 and d_12â‰ˆ 1.85Â· 10^-10 given inÂ <cit.>. 

 Find d(K_n,ğ”») for Kahan's matrix <cit.>

    K_n=[
    [       1      -c      -c       â€¦      -c      -c;       0       s     -sc       â€¦     -sc     -sc;       0       0     s^2       â€¦   -s^2c   -s^2c;                       â‹±       â€¦                ;       0       0       0       â‹±   s^n-2 -s^n-2c;       0       0       0       â€¦       0   s^n-1 ]]    s^2+c^2=1   .



Solution. We present computational results for two specialization of parameter values. The first one is s= 3/5, c=4/5:





n      d(K_n,ğ”»)  â‰ˆ      
coefficient size      number of real zeros 
    timing (s) 

 5     1.370032 Ã— 10^-3      âˆ¼ 10^310      8     - 

 10     5.470834 Ã— 10^-6     âˆ¼ 10^2970     48     - 

 15     2.246949 Ã— 10^-8      âˆ¼ 10^10590      138     6.7 

  20     9.245309 Ã— 10^-11     âˆ¼ 10^25730      288       145.4 

 25     3.984992Ã— 10^-10     âˆ¼ 10^52910       258       218.23  

 30     1.240748Ã— 10^-11     âˆ¼ 10^92460     464     937.66
 




The second test series correspond to a specialization s^n-1=1/10 treated inÂ <cit.>. For this case, an extra difficulty results from approximation of the entries of the matrix K_n as rational numbers. This results in increasing the length of the coefficients of the distance equation. Compared with the previous case, the timing increases drastically, i.e. more than 10^2 times for the same specializations of n.   





n      d(K_n,ğ”»)  â‰ˆ       number of real zeros  

 6     4.704940 Ã— 10^-4     10  

 10     1.538157 Ã— 10^-5     18  

 15     4.484974 Ã— 10^-7      28  

  20     1.904858 Ã— 10^-8      38  





The results for K_6,K_15 and K_20 confirm estimations  given inÂ <cit.>. 

It should be emphasized however that computation of the whole sets of real zeros for the distance equation is redundant for evaluation of d(A,ğ”»). We need to find just only  the least positive zero of â„±(z). For this aim, the determinantal representation (<ref>) for this polynomial might be sufficient for the real zero localization. According to Theorem <ref>, the lower estimate for the number of real zeros of â„±(z) lying within the interval [0,z_0], z_0 >0 is given by the number (<ref>). If this number is not zero then at least one real zero for â„±(z) lies in [0,z_0], and the next step in its localization might be the treatment of the matrix  S(z_0/2). 

Experiments with the Frank's matrix (<ref>) demonstrate 
the unambiguity of the zero isolation process. For the matrix F_10, one has
ğ’±(1,S_1(0),â€¦,S_10(0))=0, i.e. all the eigenvalues of A are real. Then (<ref>) coincides with 

    ğ’±_z_0:= ğ’±(1,S_1(z_0),â€¦,S_10(z_0),â€¦, S_20(z_0))   .

Some specializations for z_0



z_0      10^-3       10^-9     2Ã— 10^-15
    10^-15 

ğ’±_z_0     5      3     1     0 



demonstrate that the number of real zeros of â„±(z) lying in any interval [0,z_0] happens to be equal to ğ’±_z_0. For instance, there are precisely 
5 zeros within the interval [0,10^-3], namely

    1.540976Ã— 10^-15,  7.739368Ã—  10^-15, 7.463686 Ã— 10^-13,  1.403045 Ã— 10^-9, 1.412301 Ã— 10^-5  .

However, for the case of the matrix

    [ [  1  1 -2;  2  1  0; -3  1  1 ]]

variations ğ’±_0.4=0,  ğ’±_0.5=1,  ğ’±_2.25=0 permit one to locate single zeros within the intervals [0.4, 0.5] and [0.5,2.25] but are unable to detect this number for [0.4, 2.25].




Â§ COUNTEREXAMPLES









We exemplify here two cases


  (a)  The minimal positive zero of the distance equation not always provides the value d^2(A,ğ”») even if we restrict ourselves to the rank 1 perturbation matrices;

  (b) The distance d(A,ğ”») is not always provided by the rank 1 perturbations.



 For the matrix

    A(Ïµ)=
    [ [  0  1  1  0; -1  0  0  1;  Ïµ  0  0  1;  0  0 -1  0 ]]   ,

find d(A(Ïµ),ğ”») for Ïµ >0.


Solution. Distance equation is provided by the polynomial

    â„±(z)â‰¡ 65536Ïµ^8 [(Ïµ+2)^4 z^2 -2Ïµ(Ïµ+8)(Ïµ+2)^2 z + Ïµ^2(Ïµ-8)^2]^2
    Â·[(Ïµ+1)z-3Ïµ-1]^4


    Ã— (z^2-3 z+1)  
    [z^2-( Ïµ^2+3 ) z+(Ïµ+1)^2]   .

Its zeros are

    z_1=Ïµ(âˆš(Ïµ)-âˆš(8))^2/(Ïµ+2)^2, z_2=Ïµ(âˆš(Ïµ)+âˆš(8))^2/(Ïµ+2)^2, z_3=3Ïµ+1/Ïµ+1,


    z_4=3-âˆš(5)/2â‰ˆ 0.381966,  z_5=3+âˆš(5)/2â‰ˆ 2.618033   ,


    z_6=1/2(Ïµ^2+3-|Ïµ-1| âˆš(Ïµ^2+2 Ïµ^2+5)), z_7=
    1/2(Ïµ^2+3+|Ïµ-1| âˆš(Ïµ^2+2 Ïµ^2+5))

are all real. Zero z_4 is simple, it coincides with the square of a singular value of the matrix A, and the polynomial Î¦(Î»,z_4) has the real double zero Î»_4=0. The corresponding value of the distance function from A to ğ”» does not depend on Ïµ, it equals[Amazing coincidence with the reciprocal to the golden ratio!]

    âˆš(z_4)=âˆš(5)-1/2â‰ˆ 0.618033   .

The corresponding perturbation and matrix in ğ”» are as follows:

    E_4=1/10[[        0   âˆš(5)-5 (3âˆš(5)-5        0;        0        0        0        0;        0        0        0        0;        0  -2 âˆš(5)   5-âˆš(5)        0 ]], B_4=1/10[[       0  5+âˆš(5) 5+3âˆš(5)       0;     -10       0       0      10;     10Ïµ       0       0      10;       0  -2âˆš(5) -5-âˆš(5)       0 ]]   .

Double eigenvalue of B_4 is just 0.

Next, we do not need to treat the zeros z_6, z_7 and z_3, since they are greater than z_4. Also z_2 >z_1, therefore, the two zeros that can compete for the distance value are z_1 and z_4.  It can be verified that 

    z_1 â‰¤ z_4    Ïµâ‰¤Ïµ_2  Ïµ_2 = 2âˆš(2)(âˆš(5)+3)âˆš(âˆš(5)+2)+7âˆš(5)+15â‰ˆ 61.133652   .

It looks like d(A,ğ”»)=âˆš(z_1) for Ïµâ‰¤Ïµ_2. However, this is not true for some subinterval in [0,Ïµ_2]. Indeed, z_1 is a double zero for â„±(z), and polynomial  Î¦(Î»,z_1) possesses two double zeros:

    Î»_1,2=Â±âˆš(K(Ïµ))/Ïµ+2   K(Ïµ):=âˆš(2)(Ïµ-âˆš(2)âˆš(Ïµ)+2)(âˆš(Ïµ)+1/âˆš(2))(âˆš(Ïµ)+âˆš(5)+1/âˆš(2))(âˆš(Ïµ)-âˆš(5)-1/âˆš(2))   .

These zeros are real only for  

    Ïµâ‰¥Ïµ_1  Ïµ_1:=3-âˆš(5)â‰ˆ 0.763932   .
 
For the values Ïµ < Ïµ_2, the minimal positive zero of the distance equation is not responsible for the distance from A to ğ”».

It seems that d(A,ğ”»)=âˆš(z_4) for Ïµ < Ïµ_2. However, this statement is also invalid for some subinterval of the parameter values. The matrix

    E(Ïµ):=
    Ïµ (8-Ïµ)/(Ïµ^2+16)^2[ [    0 -4 Ïµ  Ïµ^2    0; -4 Ïµ    0    0  Ïµ^2;  -16    0    0  4 Ïµ;    0  -16  4 Ïµ    0 ]]

represents a rank 2 perturbation that provides for the matrix A(Ïµ) + E(Ïµ) a pair of double eigenvalues 

    Î»_1,2 =Â±1/Ïµ^2+16âˆš(( Ïµ^2+4 Ïµ-16 )  ( 3 Ïµ^
    2+4 Ïµ+16 ))   .

These eigenvalues are non-real for Ïµ < 2(âˆš(5)-1) â‰ˆ 2.472136.  For these parameter values, one has

    E(Ïµ) =âˆš(2)Ïµ (8-Ïµ)/Ïµ^2+16

and this value is lesser than âˆš(z_1) for Ïµ < Ïµ_c where Ïµ_c denotes the least positive zero of the polynomial 

    Ïµ^8-80 Ïµ^7-368 Ïµ^6-1024 Ïµ
    ^5+64 Ïµ^4-9216 Ïµ^3-16384 Ïµ^2-
    32768 Ïµ+65536   ;

i.e. Ïµ_c â‰ˆ 1.055249.



[t]125mm


    < g r a p h i c s >







FigureÂ 1.  




Summarizing:

    d(A(Ïµ), ğ”»)=
    {[  âˆš(2)Ïµ (8-Ïµ)/(Ïµ^2+16)           Ïµâˆˆ [0, Ïµ_c]; âˆš(Ïµ)|âˆš(Ïµ)-âˆš(8)|/(Ïµ+2)        Ïµâˆˆ [ Ïµ_c, Ïµ_2];            (âˆš(5)-1)/2               Ïµ > Ïµ_2 ].

The plot is displayed in Fig. 1 (the first formula â€” red, the second one â€” blue, the third one â€” green). 

Remark. As it is mentioned in Introduction, the case where d(A,ğ”») is achieved at the rank 2 matrix (i.e. the nearest in ğ”» matrix possesses two double imaginary eigenvalues) is beyond our consideration. We are not able even to conjecture whether this is a zero probability event or not. 





Â§ COMPLEX PERTURBATIONS









The method proposed above can be extended to the case of complex perturbations. For a real matrix A, we are now looking for the distance to the nearest complex matrix B with multiple eigenvalue:

    d_C(A,ğ”»):=minB-A   ğ’Ÿ_Î»(f_B(Î»))=0, Bâˆˆâ„‚^nÃ— n  .


Warning. The present section should be considered as a draft of a separate publication to be prepared sometime afterwards. We skip here the details of algebraic backgrounds, proofs of theoretical results and do not bother ourselves with mentioning that the validity of some of the declared results is subject to several extra assumptions preventing the appearance of  
troubles similar to those dealt with in the previous section.

Consider the polynomial

    Î˜(a,b,z)=[((a+b ğ¢)I-A)((a-bğ¢)I-A^âŠ¤)-zI]

and generate the system of algebraic equations

    Î˜=0, âˆ‚Î˜ /âˆ‚
    a =0,  âˆ‚Î˜/ âˆ‚ b=0   .

We are looking for the real solutions to this system.
Since 

    Î˜(a,0,z)  (<ref>)â‰¡Î¦(a,z)   ,
 
this solution set includes that for the system (<ref>).


  If the system (<ref>) possesses a solution (a_0,b_0,z_0) with b_0 0 then it has the solution 
(a_0,-b_0,z_0).


Proof. 
Polynomial Î˜(a,b,z) is even in b:

    Î˜(a,-b,z)=[ ((a+ğ¢ b)I-A^âŠ¤)((a-ğ¢ b)I-A) - zI ]


    =[ {((a+ğ¢ b)I-A^âŠ¤)((a-ğ¢ b)I-A)}^âŠ¤ - zI ]


    =[((a-ğ¢ b)I-A^âŠ¤) ((a+ğ¢ b)I-A)  - zI ]=Î˜(a,b,z)   .

Consequently Î˜^'_a is even in b while Î˜^'_b  is odd b. The latter becomes even on dividing by b. 

Our aim is to eliminate the variables a and b from the system (<ref>), i.e. to find the bivariate discriminant 
ğ’Ÿ_a,b(Î˜) for the polynomial Î˜(a,b,z) treated with respect to these variables. 

The discriminant ğ’Ÿ_x,y(F)  of a polynomial F(x,y,z) âˆˆâ„‚[x,y,z] is formally defined as the result of elimination of variables x and y from the system of equations

    F=0, âˆ‚ F / âˆ‚ x=0,  âˆ‚ F / âˆ‚ y=0   .

This is a polynomial in z and its vanishment at z=z_0 âˆˆâ„‚ is the necessary and sufficient condition for the existence of solution (x_0,y_0,z_0) âˆˆâ„‚^3 to the system (<ref>), or equivalently, for the existence of the multiple zero (x_0,y_0) for the polynomial F(x,y,z_0). Constructive computation of discriminant 
can be implemented in several ways, and we will exemplify below the procedure based of the BÃ©zout construction of the resultant <cit.>.


 The discriminant ğ’Ÿ_a,b(Î˜(a,b,z))  is factorized as follows:

    ğ’Ÿ_a,b(Î˜(a,b,z))â‰¡ z^n(n+1)/2â„±(z) â„±(z)     .

Here  â„±(z) is defined by (<ref>), while 

    â„±(z) âˆˆâ„[z],  â„±(z) =n(n-1)(n-2)/2   ,

(For n=2 polynomial â„±(z) is just a constant).


According to Section <ref>, the distance equation â„±(z)=0 is responsible for the rank 1 real perturbation that provides the distance d(A,ğ”»).
It turns out that the equation

    â„±(z) = 0
 
 is responsible for the rank 1 imaginary perturbation. Its real zero z_0 corresponds to a pair of multiple zeros 
of the polynomial Î˜(a,b, z_0), and these zeros are either in the form  (a_0, Â±Î²_0) or in the form
(a_0, Â±ğ¢Î²_0) with real Î²_0. We are definitely interested only in the real solutions for the 
system (<ref>).





 Let the system (<ref>) possess a real solution (a_0,b_0,z_0) with z_0 >0,b_0 0.
Denote U_0âˆˆâ„‚^n,  U_0=1 the left singular vector for the matrix 
(a_0+ğ¢ b_0)I-A corresponding to the singular value 
âˆš(z_0).
Then  the rank 1 perturbation

    E_0=U_0 U_0^ğ–§ ((a_0+ğ¢ b_0) I-A)

is such that E_0= âˆš(z_0) and the matrix B_0=A+E_0 âˆˆâ„‚^nÃ— n possesses the double eigenvalue a_0+ğ¢ b_0.


Remark. Evidently, the matrix E_0 provides for the matrix 
B_0=A+E_0 the double
eigenvalue a_0-ğ¢ b_0.

In view of Theorem <ref>, the distance d_C(A,ğ”») results from the competition between the least positive zero of â„±(z) and that minimal positive zero of 
â„±(z) that corresponds to the real solution for the system (<ref>).


Computation of the polynomial â„±(z) can be simplified if we take into account Theorem <ref>. Substitute 

    ğ”Ÿ:=b^2
 
in the polynomials of the system (<ref>) and denote

    Î(a,ğ”Ÿ,z):=Î˜(a,b,z),  Î_a(a,ğ”Ÿ,z):=Î˜^'_a(a,b,z),  Î_ğ”Ÿ(a,ğ”Ÿ,z):=Î˜^'_b(a,b,z)/b    .


 The result of elimination of variables a and ğ”Ÿ from the system

    Î=0, Î_a=0, Î_ğ”Ÿ=0

is the equation 

    z^n(n-1)/2â„±(z)=0   .



If z_0 is a positive zero of â„±(z), the corresponding real solution to the system (<ref>) might have the ğ”Ÿ-component either positive or negative. We are interested only in the positive variant.

 Find d_C(A,ğ”») for

    A= [ [   0   1   0;   0   0   1; -91 -55 -13 ]]   .



Solution. First compute the polynomial  â„±(z) via (<ref>):

    â„±(z) := 33076090700402342058246544 z^6-377039198861306289080145178864  z^5


    +937864902703881321034450183916 z^4-771868276098720970149792503999 z^3


    +211070978787821517684022650624 z^2


    -510584100140452518540394496 z+319295875259784560640000   .

Its real zeros  are as follows

    z_1â‰ˆ 0.739336,  0.765571, 0.980468,  11396.658548   .

Next compose the polynomial Î(a,ğ”Ÿ,z):

    Î(a,ğ”Ÿ,z)=-z^3+(3a^2+3ğ”Ÿ+26a+11477)z^2


    -(3 a^4+6 a^2ğ”Ÿ+3 ğ”Ÿ^2+52a^3+52ağ”Ÿ+11756a^2+11536ğ”Ÿ+11466 a+19757)z


    +( a^2+ğ”Ÿ+14 a+49 )  ( (a^2+ğ”Ÿ+6 a+13)^2-
    16 ğ”Ÿ)   .

Now we trace briefly the procedure  of elimination of a and ğ”Ÿ from the system (<ref>). Consider the monomial sequence

    ğ•„:={ğ”ª_j(a,ğ”Ÿ)}={1,a,ğ”Ÿ, ğ”Ÿ^2}  .

It is possible to reduce the polynomial ğ”ª_j  Î modulo Î_a and Î_ğ”Ÿ, i.e. to 
find the polynomials {Î²_jk(z) }_j,k=1^4 âŠ‚â„[z] and {p_11(a,ğ”Ÿ,z), p_j2(a,ğ”Ÿ,z)}_j=1^4 âŠ‚â„[a,ğ”Ÿ,z] satisfying the identity

    m_j  Îâ‰¡Î²_j1(z)+ Î²_j2(z) a + Î²_j3(z) ğ”Ÿ + Î²_j4(z) ğ”Ÿ^2 +  p_j1Î_a  + p_j2Î_ğ”Ÿ  jâˆˆ{1,2,3,4}  .

For instance, 

    Î²_11(z)= -17718805921 z^2+610367232 z+22937600,  Î²_12(z)= -39353600  z+5324800,


    Î²_13(z)=146694400 z-512000, Î²_14(z)=-307200, â€¦,


    Î²_44(z)=
    - 76550493273549926400 z^3+ 162810741053705011200 z^2- 1867736871075840000 z- 50331648000000  .

Compose the BÃ©zout matrix 

    ğ”…(z):= [ Î²_jk(z) ]_j,k=1^4   .

Then

    ğ”…(z) â‰¡  z^3â„±(z)

where

    â„±(z) =  412324266119803814719539025 z^3+
    33923334498676415590177600 z^2


    +691077589890510378371072 z-
    899669298077697638400   .

For any zero z_0 of this polynomial, the corresponding a  and ğ”Ÿ components of the solution to the system (<ref>) can be obtained in the following way. Denote by {ğ”…_4j}_J=1^4 the cofactors of 
ğ”… corresponding to the entries of the last row of the matrix ğ”…. Then the a-component of solution is connected with the z-component as

    a=ğ”…_42/ğ”…_41= 43719663040898080379 z^2+2929017747573439808 z+
    29336262189312000/2(624300876564482975 z^2-226254560538037856 z-3469512291865600)

while the ğ”Ÿ-component as

    ğ”Ÿ=ğ”…_43/ğ”…_41=  3083432482762007609519 z^3+ 1101690698089389073600 z^2+ 67186386329988787456 z- 129087561954918400/16(624300876564482975 z^2-226254560538037856 z-3469512291865600)

Polynomial â„±(z) possesses a single real zero, namely[All the decimals in the following approximation are error-free.]

    z_1 â‰ˆ 0.0012268490707391199222512104943   ,

and substitution of this value into the last formulas yields

    a= a_1 â‰ˆ -4.403922040624116177182912013601,  ğ”Ÿ = ğ”Ÿ_1 â‰ˆ  0.750705046015830894563798035515   .

Since ğ”Ÿ_1>0, one may claim that  

    d_C(A,ğ”»)=âˆš(z_1)â‰ˆ 0.035026405335676681771543151648   .

The two  perturbations in â„‚^3Ã— 3 providing this distance correspond to the solutions

    (a_1,b_1,z_1)   (a_1,-b_1,z_1)    b_1=âˆš(ğ”Ÿ_1)â‰ˆ 0.866432366671415902596255690462   .

of the system (<ref>). Let us compute via (<ref>) the one  corresponding to (a_1,-b_1,z_1). 
The unit left singular vector of
(a_1-ğ¢ b_1)I-A corresponding to the singular value 
âˆš(z_1) is as follows 

    U_1 â‰ˆ[ 
     0.930609,  
     0.360923+
     0.039918 ğ¢, 
     0.045052+
     0.008866 ğ¢]^âŠ¤

and the minimal perturbation

    E_1â‰ˆ[[  0.001289-0.000442 ğ¢ -0.007120+0.000832 ğ¢  0.031666+0.002551 ğ¢;  0.000519-0.000116 ğ¢ -0.002797+0.000017 ğ¢  0.012172+0.002348 ğ¢;  0.000067-0.000009 ğ¢ -0.000353-0.000028 ğ¢  0.001509+0.000425 ğ¢ ]]  .

The spectrum of the matrix A+E_1 is 

    { a_1-ğ¢ b_1, a_1-ğ¢ b_1 ,-13-2(a_1-ğ¢ b_1) â‰ˆ -4.192156-1.732865 ğ¢}  .



To test the performability of the algorithm sketched in the present section, we chose the next  matrix from the Matlab gallery('grcar',6).

 Find d_C(A,ğ”»)  for

    A= [ [  1  1  1  1  0  0; -1  1  1  1  1  0;  0 -1  1  1  1  1;  0  0 -1  1  1  1;  0  0  0 -1  1  1;  0  0  0  0 -1  1 ]]   .



Solution. Here the minimal zero of â„±(z) equals
z_1â‰ˆ 0.116565 
and that of â„±(z) equals

    z_1 â‰ˆ 0.04630491415327188209539627157   .

The latter corresponds to the real solution for the system (<ref>):

    (a_1,Â± b_1, z_1)   a_1 â‰ˆ 
     0.753316,  b_1 â‰ˆ -1.591155   .

Thus, one obtains

    d_C(A,ğ”») = âˆš(z_1)â‰ˆ 0.2151857666140395125353   .

This confirms estimation d_C(A,ğ”») â‰ˆ 0.21519
fromÂ <cit.>. 

For the solution (a_1,b_1, z_1), the spectrum of the nearest to A matrix in ğ”» is as follows

    { 0.361392-1.944783 ğ¢,1.139422-1.239762 ğ¢,1.502453-0.616966 ğ¢,1.490100+0.619201 ğ¢,a_1+ğ¢ b_1, a_1+ğ¢ b_1 }  .







Â§ CONCLUSION



We have investigated Wilkinson's problem for the distance evaluation from a given matrix to the set of matrices possessing multiple eigenvalues. The proposed approach consists in the construction of distance equation with the zero set containing the critical values of the squared distance function. This construction is realized in the ideology of symbolic computations, i.e. the procedure consists of a finite number of elementary algebraic operations on the entries of the matrix. 

The representation of the distance equation with the aid of the discriminant function should not be taken as a complete surprise. Indeed, the Wilkinson's  problem is the one of evaluation the distance to the discriminant manifold in the space of matrix entries. Hence, in view of this circumstance, the appearance of the discriminant in a solution to the problem is somehow natural. The more astonishing is the emergence of the discriminant in nearly any problem of distance evaluation from a point to an algebraic manifold in a multidimensional space <cit.>.


Direction for further research is clearly related the stuff of Section <ref>, i.e. the problem of existence the rank 2 minimal perturbation providing d(A,ğ”»). 







99


AhmadAlam Ahmad, Sk.S., Alam, R.: On Wilkinson's problem for matrix pencils. ELA, 30, pp. 632â€“648 (2015)


AkFrSp Akinola, R. O., Freitag, M. A., Spence A.: The calculation of the distance to a nearby defective matrix. Numerical Linear Algebra with Applications. 21:3, pp. 403â€“414 (2014) 

AlamBora Alam, R., Bora, S.: On sensitivity of eigenvalues and
eigendecompositions of matrices. Linear Algebra Appl. 396, pp. 273â€“301 (2005)

AlamBoraByersOverton Alam, R., Bora, S., Byers, R., Overton, M. L.: Characterization and construction of the nearest defective matrix via coalescence of pseudospectral components. Linear Algebra Appl. 435, pp. 494â€“513 (2011)

ArmGraVel Armentia, G., Gracia, J.-M., Velasco, F.-E.: Nearest matrix with a prescribed eigenvalue of bounded multiplicities. Linear Algebra Appl., 592, 188â€“209 (2020)

BikkerUteshev Bikker P., Uteshev A.Yu.: On the BÃ©zout construction of the resultant. J.Symbolic Comput., 1999, 28,  No 1. 45â€“88 (1999)

Demmel Demmel, J.W.: Computing stable eigendecompositions of matrices. Linear Algebra Appl., 79, pp. 163â€“193 (1986)

Demmel1 Demmel, J.W.: On condition numbers and the distance to the nearest ill-posed problem, Numer.Math. 51, pp. 251â€“289 (1987)

Frank
Frank, W.L.: Computing eigenvalues of complex matrices by determinant evaluation and by methods of Danilewski and Wielandt. J. Soc. Indust. Appl. Math. 6(4), pp. 378â€“392 (1958).

Gantmacher Gantmacher, F.R.: The Theory of Matrices. 
Chelsea, New York (1959)

Gracia Gracia, J.-M.: Nearest matrix with two prescribed eigenvalues. Linear Algebra Appl. 401, pp. 277â€“294 (2005)

Kahan Kahan, W.: Numerical linear algebra. Canad. Math. Bull. 9, pp. 757â€“801 (1966)

Kalinina_Uteshev_CASC22 Kalinina, E., Uteshev, A.: Distance evaluation to the set of matrices with multiple eigenvalues.  LNCS, 13366. Springer, Cham, pp.206â€“224 (2022)



KokLogKar
Kokabifar, E., Loghmani, G.B., Karbassi, S.M.: Nearest matrix with prescribed eigenvalues and its applications. J. Comput. Appl. Math.
298,  pp. 53â€“63 (2016)

Lippert Lippert, R.A.: Fixing multiple eigenvalues by a minimal perturbation. Linear Algebra Appl. 432, pp. 1785â€“1817 (2010)

LipEdel
Lippert, R.A., Edelman, A.: The computation and sensitivity of double eigenvalues, in: Z. Chen, Y. Li, C.A. Micchelli, Y. Xu (Eds.), Advances in Computational Mathematics: Proc. Gaungzhou
International Symposium, Dekker, New York, pp. 353â€“393 (1999)

Malyshev Malyshev, A.: A formula for the 2-norm distance from a matrix to the set of matrices with multiple eigenvalues. Numer. Math. 83, pp. 443â€“454 (1999)

Mengi Mengi, E.: Locating a nearest matrix with an eigenvalue of prespecified algebraic multiplicity. Numer. Math. 118, pp. 109â€“135 (2011) 

Ruhe Ruhe, A.: Properties of a matrix with a very ill-conditioned eigenproblem. Numer. Math. 15, pp. 57â€“60 (1970)

TrEmb Trefethen, L. N., Embree, M.: Spectra and Pseudospectra, Princeton University Press, Princeton, NJ (2005)


UteshevCherkasov Uteshev, A.Yu., Cherkasov, T.M.:  The search for the maximum of a polynomial.J. Symbolic Comput. 25 (5). pp. 587â€“618 (1998) 

UteshevYashina2015 Uteshev, A.Yu., Yashina M.V.: Metric problems for quadrics in multidimensional space. J.Symbolic Comput.,  68, Part I, pp. 287â€“315 (2015)


Wilkinson Wilkinson, J.H.: The Algebraic Eigenvalue Problem, Oxford University Press, New York (1965)

Wilkinson2 Wilkinson, J.H.: Note on matrices with a very ill-conditioned eigenproblem. Numer. Math. 19, 176â€“178 (1972)

Wilkinson1 Wilkinson, J.H.: On neighbouring matrices with quadratic elementary divisors, Numer. Math. 44, 1â€“21 (1984)

Wilkinson3 Wilkinson, J.H.: Sensitivity of eigenvalues, Util. Math. 25, 5â€“76 (1984)




