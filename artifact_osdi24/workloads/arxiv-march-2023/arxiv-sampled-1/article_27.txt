














 
Intern Project
    
Yichuan Deng. University of Science and Technology of China.
Zhihang Li. Huazhong Agriculture University. 
Zhao Song. Adobe Research

    
==========================================================================================================================================

????


[

???













equal*


Aeiau Zzzzequal,to
Bauiu C.Â Yyyyequal,to,goo
Cieua Vvvvvgoo
Iaesut Saoeued
Fiuea Rrrrto
Tateu H.Â Yaseheed,to,goo
Aaoeu Iasohgoo
Buiui Eueued
Aeuia Zzzzed
Bieea C.Â Yyyyto,goo
Teoau Xxxxed
Eee Pppped


toDepartment of Computation, University of Torontoland, Torontoland, Canada
gooGoogol ShallowMind, New London, Michigan, USA
edSchool of Computation, University of Edenborrow, Edenborrow, United Kingdom

Cieua Vvvvvc.vvvvv@googol.com
Eee Ppppep@eden.co.uk




Machine Learning, ICML

0.3in
]

 







  Intern Project
    
Yichuan Deng. University of Science and Technology of China.
Zhihang Li. Huazhong Agriculture University. 
Zhao Song. Adobe Research

    
==========================================================================================================================================


Matrix sensing is a problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. The goal is to reconstruct the original matrix as accurately as possible, given only a set of linear measurements obtained by sensing the matrix <cit.>. In this work, we focus on a particular direction of matrix sensing, which is called rank-1 matrix sensing <cit.>. 
We present an improvement over the original algorithm in <cit.>. 
It is based on a novel analysis and sketching technique that enables faster convergence rates and better accuracy in recovering low-rank matrices. The algorithm focuses on developing a theoretical understanding of the matrix sensing problem and establishing its advantages over previous methods. The proposed sketching technique allows for efficiently extracting relevant information from the linear measurements, making the algorithm computationally efficient and scalable. 

Our novel matrix sensing algorithm improves former result <cit.> on in two senses,

    
  * We improve the sample complexity from O(Ïµ^-2 dk^2) to O(Ïµ^-2 (d+k^2)).
    
  * We improve the running time from O(md^2 k^2) to O(m d^2 k).   

The proposed algorithm has theoretical guarantees and is analyzed to provide insights into the underlying structure of low-rank matrices and the nature of the linear measurements used in the recovery process. 
It advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem. 

  
  empty










Matrix sensing is a problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. The goal is to reconstruct the original matrix as accurately as possible, given only a set of linear measurements obtained by sensing the matrix <cit.>. In this work, we focus on a particular direction of matrix sensing, which is called rank-1 matrix sensing <cit.>. 
We present an improvement over the original algorithm in <cit.>. 
It is based on a novel analysis and sketching technique that enables faster convergence rates and better accuracy in recovering low-rank matrices. The algorithm focuses on developing a theoretical understanding of the matrix sensing problem and establishing its advantages over previous methods. The proposed sketching technique allows for efficiently extracting relevant information from the linear measurements, making the algorithm computationally efficient and scalable. 

Our novel matrix sensing algorithm improves former result <cit.> on in two senses,

    
  * We improve the sample complexity from O(Ïµ^-2 dk^2) to O(Ïµ^-2 (d+k^2)).
    
  * We improve the running time from O(md^2 k^2) to O(m d^2 k).   

The proposed algorithm has theoretical guarantees and is analyzed to provide insights into the underlying structure of low-rank matrices and the nature of the linear measurements used in the recovery process. 
It advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem. 







Â§ INTRODUCTION




The matrix sensing problem is a fundamental problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurement. This problem arises in various applications such as image and video processing <cit.> and sensor networks <cit.>. 
Mathematically, matrix sensing can be formulated as a matrix view of compressive sensing problem <cit.>. The rank-1 matrix sensing problem was formally raised in <cit.>. 

The matrix sensing problem has attracted significant attention in recent years, and several algorithms have been proposed to solve it efficiently. In this paper, we provide a novel improvement over the origin algorithm in <cit.>, with improvement both on running time and sample complexity. 
 

Matrix sensing is a fundamental problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. Specifically, given a matrix W_*âˆˆâ„^d Ã— d of rank k that is not directly accessible, we aim to recover W_* from a set of linear measurements b âˆˆ^n applied to the ground truth matrix W^* where

    b_i=[A_i^âŠ¤ W_*],Â Â Â âˆ€ i=1, â€¦, m,

where A_i are known linear operators. The measurements b_i are obtained by sensing the matrix W_* using a set of linear measurements, and the goal is to reconstruct the original matrix W_* as accurately as possible. This problem arises in various applications such as image and video processing, sensor networks, and recommendation systems.

The matrix sensing problem is ill-posed since there may exist multiple low-rank matrices that satisfy the given linear measurements. However, the problem becomes well-posed under some assumptions on the underlying matrix, such as incoherence and restricted isometry property (RIP) <cit.>
, which ensure unique and stable recovery of the matrix. A well-used method to solve this problem is to use convex optimization techniques that minimize a certain loss function subject to the linear constraints. Specifically, one can solve the following convex optimization problem:

    min_W_*Â (W_*)
    s.t.   Â [A_i^âŠ¤ W_*] = b_i, âˆ€ i=1,â€¦,m.

However, this problem is NP-hard <cit.> and intractable in general, and hence, various relaxation methods 
have been proposed, such as nuclear norm minimization and its variants, which provide computationally efficient solutions with theoretical guarantees. In this work, we focus on the rank-one independent measurements. Under this setting, the linear operators A_i can be decomposed into the form of A_i = x_iy_i^âŠ¤, where x_i âˆˆ^d, y_i âˆˆ^d are all sampled from zero-mean multivariate Gaussian distribution N(0, I_d). 

Our work on improving the matrix sensing algorithm is based on a novel analysis and sketching technique 
that enables faster convergence rates and better accuracy in recovering low-rank matrices. We focus on developing a theoretical understanding of the proposed algorithm and establishing its advantages over previous methods. Our analysis provides insights into the underlying structure of the low-rank matrices and the nature of the linear measurements used in the recovery process. The proposed sketching technique allows us to efficiently extract relevant information from the linear measurements, making our algorithm computationally efficient and scalable. Overall, our contribution advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem. 




 Â§.Â§ Our Result

To summarize, we improve both the running time of original algorithm <cit.> from O(md^2k^2) to O(md^2k), and the sample complexity from O(Ïµ^-2dk^2) to O(Ïµ^-2(d + k^2)). Formally, we get the following result, 
 



Let Ïµ_0 âˆˆ (0,0.1) denote the final accuracy of the algorithm. Let Î´âˆˆ (0,0.1) denote the failure probability of the algorithm. Let Ïƒ_1^* denote the largest singular value of ground-truth matrix W_* âˆˆ^d Ã— d. Let Îº denote the condition number of ground-truth matrix W_* âˆˆ^d Ã— d. 
Let Ïµâˆˆ (0, 0.001/(k^1.5Îº)) denote the RIP parameter. Let m = Î˜ (Ïµ^-2 (d+k^2)log(d/Î´)). Let T = Î˜(log(k ÎºÏƒ_1^* /Ïµ_0)) . There is a matrix sensing algorithm (AlgorithmÂ <ref>) that takes O(m T) samples, runs in T iterations, and each iteration takes O(md^2 k) time, finally outputs a matrix W âˆˆ^d Ã— d such that

    (1-Ïµ_0) W_* â‰¼ W â‰¼ (1+Ïµ_0) W_*

holds with probability at least 1-Î´.


 





 Â§.Â§ Related Work




  
Matrix Sensing


The matrix sensing problem has attracted significant attention in recent years, and several algorithms have been proposed to solve it efficiently. One of the earliest approaches is the convex optimization-based algorithm proposed by CandÃ¨s and Recht in 2009 <cit.>, which minimizes the nuclear norm of the matrix subject to the linear constraints. This approach has been shown to achieve optimal recovery guarantees under certain conditions on the linear operators, such as incoherence and RIP.
Since then, various algorithms have been proposed that improve upon the original approach in terms of computational efficiency and theoretical guarantees. For instance, the iterative hard
thresholding algorithm (IHT) proposed by Blumensath and Davies in 2009 <cit.>, and its variants, such
as the iterative soft thresholding algorithm (IST), provide computationally efficient solutions with
improved recovery guarantees. 
In the work by Recht, Fazel, and Parrilo 

<cit.>, they gave some measurement operators satisfying the RIP and proved that, with O(k d log d) measurements, a rank-k matrix W_* âˆˆ^d Ã— d can be recovered. 
Moreover, later works have proposed new approaches that exploit additional structure in the low-rank matrix, such as sparsity or group sparsity, to further improve recovery guarantees and efficiency. For instance, the sparse plus low-rank (S + L) approach proposed by 
<cit.>, and its variants, such as the robust principal component analysis (RPCA) and the sparse subspace clustering (SSC), provide efficient solutions with improved robustness to outliers and noise. More recently, <cit.> considers the non-square matrix sensing under RIP assumptions, and show that matrix factorization 
does not introduce any spurious
local minima 
under RIP. <cit.> studies the technique of discrete-time mirror descent utilized to address the unregularized empirical risk in matrix sensing.




  
Compressive Sensing
 
Compressive sensing has been a widely studied topic in signal processing and theoretical computer science field <cit.>. <cit.> gave a fast algorithm (runs in time O(klog n log( n/k)) for generall in puts and O(klog n log(n/k)) for at most k non-zero Fourier coefficients input) for k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. <cit.> provided an algorithm such that it uses O_d(k log N loglog N) samples of signal and runs in time O_d(klog^d+3 N) for k-sparse approximation to the Fourier transform of a length of N signal. Later work <cit.> proposed a new technique for analysing noisy hashing schemes that arise in Sparse FFT, which is called isolation on average, and applying it, it achieves sample-optimal results in klog^O(1)n time for estimating the values of a list of frequencies using few samples and computing Sparse FFT itself. 
<cit.> gave the first sublinear-time â„“_2/â„“_2 compressed sensing which achieves the optimal number of measurements without iterating. After that, <cit.> provided an algorithm which uses O(k log k log n) samples to compute a k-sparse approximation to the d-dimensional Fourier transform of a length n signal. 
Later by <cit.> provided an efficient Fourier Interpolation algorithm that improves the previous best algorithm <cit.> on sample complexity, time complexity and output sparsity. And in <cit.> they presented a unified framework for the problem of band-limited signal reconstruction and achieves high-dimensional Fourier sparse recovery and high-accuracy Fourier interpolation. 
Recent work <cit.> designed robust algorithms for super-resolution imaging that are efficient in terms of both running time and sample complexity for any constant dimension under the same noise model as <cit.>, based on new techniques in Sparse Fourier transform. 







  
Faster Iterative Algorithm via Sketching

Low rank matrix completion is a well-known problem in machine learning with various applications in practical fields such as recommender systems, computer vision, and signal processing. Some notable surveys of this problem are provided in <cit.>. While Candes and Recht <cit.> first proved the sample complexity for low rank matrix completion, other works such as <cit.> and <cit.> have provided improvements and guarantees on convergence for heuristics. In recent years, sketching has been applied to various machine learning problems such as linear regression <cit.>, low-rank approximation <cit.>, weighted low rank approximation, matrix CUR decomposition <cit.>, and tensor regression <cit.>, leading to improved efficiency of optimization algorithms in many problems. For examples, linear programming <cit.>, matrix completion <cit.>, empirical risk minimization <cit.>, training over-parameterized neural network <cit.>, discrepancy algorithm <cit.>, frank-wolfe method <cit.>, and reinforcement learning <cit.>.

 



  
Roadmap.
We organize the following paper as follows. In SectionÂ <ref> we provide the technique overview for our paper. In SectionÂ <ref> we provide some tools and existing results for our work. In SectionÂ <ref> we provide the detailed analysis for our algorithm. In SectionÂ <ref> we argue that our measurements are good. In SectionÂ <ref> we provide analysis for a shrinking step. 
In SectionÂ <ref> we provide the analysis for our techniques used to solve the optimization problem at each iteration. 

 



Â§ TECHNIQUE OVERVIEW


 

In this section, we provide a detailed overview of the techniques used to prove our results. Our approach is based on a combination of matrix sketching and low-rank matrix recovery techniques. Specifically, we use a sketching technique that allows us to efficiently extract relevant information from linear measurements of the low-rank matrix. We then use this information to recover the low-rank matrix using a convex optimization algorithm. With these techniques, we are able to improve previous results in both sample complexity and running time. From the two perspective, we give the overview of our techniques here. 



 Â§.Â§ Tighter Analysis Implies Reduction to Sample Complexity

Our approach achieves this improvement by using a new sketching technique that compresses the original matrix into a smaller one while preserving its low-rank structure. This compressed version can then be used to efficiently extract relevant information from linear measurements of the original matrix.

To analyze the performance of our approach, we use tools from random matrix theory and concentration inequalities. Specifically, we use the Bernstein's inequality for matrices to establish bounds on the error of our recovery algorithm. 
We first define our measurements and operators, for each i âˆˆ [m], let x_i, y_i denotes samples from (0, I_d). We define

    
  * A_i := x_i y_i^âŠ¤;
    
  * b_i := x_i^âŠ¤ W_* y_i;
    
  * W_0 := 1/mâˆ‘_i = 1^m b_i A_i;
    
  * B_x := 1/mâˆ‘_i = 1^m (y_i^âŠ¤ v)^2 x_ix_i^âŠ¤;
    
  * B_y := 1/mâˆ‘_i = 1^m (x_i^âŠ¤ v)^2 y_iy_i^âŠ¤;
    
  * G_x := 1/mâˆ‘_i=1^m (y_i^âŠ¤ v)(y_i^âŠ¤ v_)x_ix_i^âŠ¤;
    
  * G_x := 1/mâˆ‘_i=1^m (x_i^âŠ¤ v)(x_i^âŠ¤ v_)y_iy_i^âŠ¤.

We need to argue that our measurements are good under our choices of m, here the word â€œgoodâ€ means that 

    
  * W_0 - W_*â‰¤ÏµÂ·W_*;
    
  * B_x- Iâ‰¤Ïµ and B_y - Iâ‰¤Ïµ;
    
  * G_xâ‰¤Ïµ and G_yâ‰¤Ïµ.

In our analysis we need to first bound Z_i and [Z_iZ_i^âŠ¤], where Z_i := x_ix_i^âŠ¤ U_*Î£_*Y_*^âŠ¤ y_iy_i^âŠ¤. With an analysis, we are able to show that (LemmaÂ <ref> and LemmaÂ <ref>)

    [Z_iâ‰¤ C^2 k^2 log^2(d/Î´)Ïƒ^4 Â·Ïƒ_1^*]    Â â‰¥ 1 - Î´/(d) 
    [Z_iZ_i^âŠ¤]   Â â‰¤ C^2k^2Ïƒ^4(Ïƒ_1^*)^2.

Now, applying these two results and by Bernstein's inequality, we are able to show that our operators are all â€œgoodâ€ (TheoremÂ <ref>). 




 Â§.Â§ Induction Implies Correctness


To get the final error bounded, we use an inductive strategy to analyze. Here we let U_* and V_* be the decomposition of ground truth W_*, i.e., W_* = U_* V_*. We show that, when iteratively applying our alternating minimization method, if U_t and V_t are closed to U_* and V_* respectively, then the output of next iteration t+1 is close to U_* and V_*. Specifically, we show that, if (U_t, U_*) â‰¤1/4Â·(V_t, V_*), then it yields

    (V_t+1, V_*) â‰¤1/4Â·(U_t, U_*).

Similarly, from the other side, if (V_t+1, V_*) â‰¤1/4Â·(U_t, U_*), we have

    (U_t+1, U_*) â‰¤1/4Â·(V_t+1, V_*).

This two recurrence relations together give the guarantee that, if the starting error U_0 - U_* and V_0 - V_*, the distance from V_t and U_t to V_* and U_*, respectively.

To prove the result, we first define the value of Ïµ_d as /10. Then, by the algorithm, we have the following relationship between V_t+1 and V_t+1 R^-1,

    V_t+1 = V_t+1 R^-1 = (W_*^âŠ¤ U_t - F)R^-1,

where the second step follows from the definition of V and defining F as DefinitionÂ <ref>. Now we show that, F and R^-1 can be bound respectively,

    F   Â â‰¤ 2Ïµ k^1.5Â·Ïƒ_1^* Â·(U_t, U_*)   LemmaÂ <ref>
    R^-1   Â â‰¤ 10/Ïƒ_k^*   Â LemmaÂ <ref>

Note that the bound of ^-1 need (U_t, U_*) â‰¤1/4Â·(V_t, V_*))

With these bounds, we are able to show the bound for (V_t+1, V_*). We first notice that, (V_t+1, V_*) can be represented as (V_*,)^âŠ¤ V_t+1, where V_*,âˆˆ^d Ã— (d-k) is a fixed orthonormal basis of the subspace orthogonal to span(V_*). Then we show that (ClaimÂ <ref>)

    (V_*,)^âŠ¤ V_t+1 = -(V_*, )^âŠ¤ FR^-1.

Now, by turning (V_t+1, V_*) to the term of F and R, and using the bound for F and R^-1, we are finally able to reach the bound 

    (V_t+1, V_*) 
        =    Â FR^-1
    â‰¤   Â FÂ·R^-1
    â‰¤   Â  2Ïµ k^1.5Â·Ïƒ_1^* Â·(U_t, U_*) Â·R^-1
    â‰¤   Â  2Ïµ k^1.5Â·Ïƒ_1^* Â·(U_t, U_*) Â· 10/Ïƒ_k^* 
    â‰¤   Â  0.01 Â·(U_t, U_*).

By a similar analysis, we can show Eq.(<ref>). 

Now applying them and with a detailed analysis, we have the claimed proved. Finally, when we prove that the initialization of the parameters are good, we can show that, the final output W_T satisfies

    W_T - W_*â‰¤Ïµ_0.




 Â§.Â§ Speeding up with Sketching Technique

Now we consider the running time at each iteration. 
At each iteration of our algorithm, we need to solve the following optimization problem: 

    min_V âˆˆ^d Ã— kâˆ‘_i = 1^m ([A_i^âŠ¤ UV^âŠ¤] - b)^2.

When this problem is straightforwardly solved, it costs O(md^2k^2) time, which is very expensive. So from another new direction, we give an analysis such that, this problem can be converted to a minimization problem where the target variable is a vector. To be specific, we show that, above optimization question (<ref>) is equivalent to the following (LemmaÂ <ref>),

    min_v âˆˆ^dkMv - b_2^2,

where the matrix M âˆˆ^m Ã— dk
is defined to be the reformed matrix of U^âŠ¤ A_i's, i.e.,

    M_i,* := (U^âŠ¤ A_i), Â Â âˆ€ i âˆˆ [m].

When working on this form of optimization problem, inspired by a recent work <cit.>, we apply the fast sketch-to-solve low-rank matrix completion method. With this technique, we are able to reduce the running time to O(md^2k) (TheoremÂ <ref>), which is much more acceptable. 




Â§ PRELIMINARY


In this section, we provide preliminaries to be used in our paper.  In SectionÂ <ref> we introduce notations we use. In SectionÂ <ref> and SectionÂ <ref> we provide some randomness facts and algebra facts respectively. In SectionÂ <ref> we introduce the important definition of restricted isometry property. In SectionÂ <ref> we provide results fro rank-one estimation. In SectionÂ <ref> we introduce the rank-one independent Gaussian operator. In SectionÂ <ref> we state our notations for angles and distances. In SectionÂ <ref> we provide some matrix concentration results. 



 Â§.Â§ Notations



Let x âˆˆ^n and w âˆˆ_â‰¥ 0^n, we define the norm x_w := (âˆ‘_i=1^n w_i x_i^2)^1/2.  

For n > k, for any matrix A âˆˆ^n Ã— k, we denote the spectral norm of A by A, i.e., A  := sup_xâˆˆ^k A x _2 /  x _2.


We denote the Frobenius norm of A by A _F, i.e., A _F : = (âˆ‘_i=1^n âˆ‘_j=1^k A_i,j^2 )^1/2.

For any square matrix A âˆˆ^n Ã— n, we denote its trace by [A], i.e., [A] := âˆ‘_i=1^n A_i,i.

For any A âˆˆ^n Ã— d and B âˆˆ^n Ã— d, we denote âŸ¨ A , B âŸ© = [A^âŠ¤ B].

Let A âˆˆ^n Ã— d and x âˆˆ^d be any matrix and vector, we have that

    A x _2^2 = âŸ¨ A x, A x âŸ© = âŸ¨ x , A^âŠ¤ A x âŸ© = x^âŠ¤ A^âŠ¤ A x.


Let the SVD of A âˆˆ^n Ã— k to be UÎ£ B^âŠ¤, where U âˆˆ^n Ã— k and V âˆˆ^k Ã— k have orthonormal columns and Î£âˆˆ^k Ã— k be diagonal matrix. We say the columns of U are the singular vectors of A. We denote the Moore-Penrose pseudoinverse matrix of A as A^â€ âˆˆk Ã— n, i.e., A^â€  := VÎ£^-1U^âŠ¤. We call the diagonal entries Ïƒ_1, Ïƒ_2, â€¦, Ïƒ_k of Î£ to be the eigenvalues of A. We assume they are sorted from largest to lowest, so Ïƒ_i denotes its i-th largest eigenvalue, and we can write it as Ïƒ_i(A). 




For A âˆˆ^n_1 Ã— d_1, B âˆˆ^n_2 Ã— d_2. We define kronecker product âŠ— as (A âŠ— B)_i_1+(i_2-1)n_1, j_1 + (j_2-1)n_2
 
for all i_1 âˆˆ [n_1], j_1 âˆˆ [d_1], i_2 âˆˆ [n_2] and j_2 âˆˆ [d_2].

For any non-singular matrix A âˆˆ^n Ã— n, we define A=QR its QR-decomposition, where Q âˆˆ^n Ã— n is an orthogonal matrix and R âˆˆ^n Ã— n is an non-singular lower triangular matrix. For any full-rank matrix A âˆˆ^n Ã— m, we define A=QR its QR-decomposition, where Q âˆˆ^m Ã— n is an orthogonal matrix and R âˆˆ^n Ã— n is an non-singular lower triangular matrix. We use R=QR(A) âˆˆ^n Ã— n to denote the lower triangular matrix obtained by the QR-decomposition of A âˆˆ^m Ã— n. 

Let A âˆˆ^kÃ— k be a symmetric matrix. The eigenvalue decomposition of A is A = UÎ› U^âŠ¤, where Î› is a diagonal matrix. 



If a matrix A is positive semidefinite (PSD) matrix, we denote it as A â‰½ 0, which means x^âŠ¤ A x â‰¥ 0 for all x. 

Similarly, we say A â‰½ B if x^âŠ¤  Ax â‰¥ x^âŠ¤ B x for all vector x. 
 

For any matrix U âˆˆ^n Ã— k, we say U is an orthonormal basis if U_i=1 for all i âˆˆ [k] and for any iâ‰  j, we have âŸ¨ U_i, U_j âŸ© = 0. Here for each i âˆˆ [k], we use U_i to denote the i-th column of matrix U.

For any U âˆˆ^n Ã— k (suppose n > k)which is an orthonormal basis, 
we define U_âˆˆ^n Ã— (n-k) to be another orthonormial basis that, 

    U U^âŠ¤ + U_ U_^âŠ¤ = I_n

and

    U^âŠ¤ U_ =  0^k Ã— (n-k)

where we use 0^k Ã— (n-k) to denote a k Ã— (n-k) all-zero matrix. 

We say a vector x lies in the span of U, if there exists a vector y such that x = U y.

We say a vector z lies in the complement of span of U, if there exists a vector w such that z = U_ w. Then it is obvious that âŸ¨ x,z âŸ© = x^âŠ¤ z =z^âŠ¤ x =0.

For a matrix A, we define Ïƒ_min(A) := min_x A x _2 /  x _2. Equivalently,  Ïƒ_min(A) := min_x:  x _2=1 A x _2.

Similarly, we define Ïƒ_max(A) := max_x  A x _2 /  x _2. Equivalently,  Ïƒ_max(A) := max_x:  x _2=1 A x _2 

Let A_1, â‹¯, A_n denote a list of square matrices. Let S denote a block diagonal matrix S = [ A_1            ;     A_2        ;           â‹±    ;             A_n ]. Then S  = max_iâˆˆ [n] A_i. 

We use [] to denote probability. We use [] to denote expectation.



Let a and b denote two random variables. Let f(a) denote some event that depends on a (for example f(a) can be a=0 or a â‰¥ 10.). Let g(b) denote some event that depends on b. We say a and b are independent if [f(a) Â andÂ  g(b)] = [f(a)] Â·[g(b)]. We say a and b are not independent if [ f(a) Â andÂ  g(b)] â‰ [f(a)] Â·[g(b)]. Usually if a and b are independent, then we also have [ab] = [a] Â·[b]. 

We say a random variable x is symmetric if [x = u] = [x=-u].
 

For any random variable x âˆ¼ N(Î¼,Ïƒ^2). This means [x ] = Î¼ and [x^2] = Ïƒ^2.

We use O(f) to denote f Â·(log f).




We use (a,b,c) to denote the time of multiplying an a Ã— b matrix with another b Ã— c matrix.
 
  
We use Ï‰ to denote the exponent of matrix multiplication, i.e., n^Ï‰ =(n,n,n).



 Â§.Â§ Randomness Facts




We have

    
  * Part 1. Expectation has linearity, i.e., [ âˆ‘_i=1^n x_i ] = âˆ‘_i=1^n [x_i].
    
  * Part 2. For any random vectors x and y, if x and y are independent, then for any fixed function f, we have _x,y[f(x) f(y)] = _x[f(x) ] Â·_y[ f(y)].
    
  * Part 3. Let Aâˆˆ^d Ã— d denote a fixed matrix. For any fixed function f : ^d â†’^d Ã— d, we have _x[f(x) Â· A ] = _x [f(x)] Â· A.
    
  * Part 4. Given n events A_1, A_2, â‹¯ A_n. For each i âˆˆ [n], if [ A_i ] â‰¥ 1-Î´_i. Then taking a union bound over all the n events, we have [ A_1 Â andÂ  A_2 â‹¯ A_n] â‰¥ 1- âˆ‘_i=1^n Î´_i.





 Â§.Â§ Algebra Facts


We state some standard facts and omit their proofs, since they're very standard.


We have


    
  * For any orthonormal basis U âˆˆ^n Ã— k, we have U x _2 =  x _2.
    
  * For any orthonornal basis U âˆˆ^n Ã— k, we have U _F â‰¤âˆš(k).
    
  * For any diagonal matrix Î£âˆˆ^k Ã— k and any vector x âˆˆ^k, we have Î£ x _2 â‰¥Ïƒ_min(Î£)  x _2. 
    
  * For symmetric matrix A, we have Ïƒ_min(A) = min_z :  z _2=1 z^âŠ¤ A z.
    
  * For symmetric matrix A, we have Ïƒ_min(A)  z_2^2 â‰¤ z^âŠ¤ A z for all vectors z.
    
  * For symmetric matrix A, we have Ïƒ_max(A)  z_2^2 â‰¥ z^âŠ¤ A z for all vectors z.
    
  * For any matrix A, we have Aâ‰¤ A _F.
    
  * For any square matrix A âˆˆ^k Ã— k and vector x âˆˆ^k, we have x^âŠ¤ A x = âˆ‘_i=1^k âˆ‘_j=1^k x_i A_i,j x_j = âˆ‘_i=1^k x_i A_i,i x_i + âˆ‘_iâ‰  j x_i A_i,j x_j.
    
  * For any square and invertible matrix R, we have R^-1 = Ïƒ_min(R)^-1
    
  * For any matrix A and for any unit vector x, we have A â‰¥ A x _2.
    
  * For any matrix A, A A^âŠ¤ =  A^âŠ¤ A.







 Â§.Â§ Restricted Isometry Property




    A linear operator ğ’œ: ^dÃ— dâ†’^m satisfies RIP iff, for âˆ€ W âˆˆ^d Ã— d  
    s.t. (W)â‰¤ k, the following holds:
    
    (1-Ïµ_k) Â·W_F^2â‰¤ A(W)_F^2â‰¤(1+Ïµ_k) Â·W_F^2

    where Ïµ_k > 0 is a constant dependent only on k.




 Â§.Â§ Rank-one Estimation


The goal of matrix sensing is to design a linear operator ğ’œ:^d Ã— dâ†’^m and a recovery algorithm so that a low-rank matrix W_*âˆˆ^d Ã— d can be recovered exactly using ğ’œ(W_*). 


Given a ground-truth matrix W_* âˆˆ^d Ã— d. Let (x_1, y_1) , â‹¯,  (x_m, y_m) âˆˆ^dÃ—^d denote m pair of feature vectors. Let b âˆˆ^m be defined

    b_i = x_i^âŠ¤ W_* y_i, Â Â Â âˆ€ i âˆˆ [m].

The goal is to use b âˆˆ^m and { (x_i,y_i)}_i âˆˆ [m]âŠ‚^d Ã—^d to recover W_* âˆˆ^d Ã— d.




We propose two different kinds of rank-one measurement operators based on Gaussian distribution.



 Â§.Â§ Rank-one Independent Gaussian Operator



We formally define Gaussian independent operator, here.

Let (x_1, y_1) , â‹¯, (x_m, y_m) âŠ‚^d Ã—^d denote i.i.d. samples from  Gaussian distribution.

For each i âˆˆ [m], we define A_i âˆˆ^d Ã— d as follows

    A_i := x_i y_i^âŠ¤ .
 

We define A_GIâˆˆ^d Ã— m d as follows: 

    ğ’œ_GI := [ A_1 A_2   â‹¯ A_m ] .

Here GI denotes Gaussian Independent. 
 










 Â§.Â§ Matrix Angle and Distance



We list several basic definitions and tools in literature, e.g., see <cit.>.

Let X, Y âˆˆ^n Ã— k denote two matrices.

For any matrix X, and for orthonormal matrix Y (Y^âŠ¤ Y = I_k) we define

    
  * tanÎ¸(Y,X) :=  Y_^âŠ¤ X ( Y^âŠ¤ X )^-1

For orthonormal matrices Y and X (Y^âŠ¤ Y = I_k and X^âŠ¤ X = I_k), we define

    
  * cosÎ¸ (Y,X) := Ïƒ_min (Y^âŠ¤ X). 
     
        
  * It is obvious that cos (Y,X) = 1/  (Y^âŠ¤ X)^-1 and cos(Y,X) â‰¤ 1.
    
    
  * sinÎ¸(Y,X) :=  (I - Y Y^âŠ¤) X.
     
        
  * It is obvious that sinÎ¸(Y,X) =  Y_ Y_^âŠ¤ X  =  Y_^âŠ¤ X and sinÎ¸(Y,X) â‰¤ 1.
        
  * From LemmaÂ <ref>, we know that sin^2Î¸(Y,X) + cos^2Î¸(Y,X) = 1. 
    
    
  * (Y,X) := sinÎ¸(Y,X)
  







Let X, Yâˆˆ^nÃ— k be orthogonal matrices, then 

    tanÎ¸(Y,X) = sinÎ¸(Y,X)/cosÎ¸(Y,X).







Let X, Yâˆˆ^nÃ— k be orthogonal matrices, then 

    sin^2Î¸(Y, X) + cos^2Î¸(Y,X) =1.







 Â§.Â§ Matrix Concentration




  Given a finite sequence { X_1, â‹¯ X_m }âŠ‚^n_1 Ã— n_2 of independent, random matrices all with the dimension of n_1 Ã— n_2.

    Let Z = âˆ‘_i=1^m X_i.

  Assume that
  
    [X_i] = 0, âˆ€ i âˆˆ [m],  X_i â‰¤ M, âˆ€ i âˆˆ [m]


Let [Z] be the matrix variances statistic of sum

    [Z] = max{âˆ‘_i=1^m [X_iX_i^âŠ¤]  , âˆ‘_i=1^m [X_i^âŠ¤ X_i] }

Then it holds that

    [ Z ] â‰¤ (2 [Z] Â·log(n_1+n_2))^1/2 + M log(n_1 + n_3) /3

Further, for all t>0

    [  Z â‰¥ t ] â‰¤ (n_1 + n_2) Â·exp( -t^2/2/[Z] + M t/3 )







Â§ ANALYSIS


Here in this section, we provide analysis for our proposed algorithm. In SectionÂ <ref>, we provide definitions in our algorithm analysis. In SectionÂ <ref> we define the operators to be used. In SectionÂ <ref> we provide our main theorem together with its proof. In SectionÂ <ref> we introduce our main induction hypothesis. 



 Â§.Â§ Definitions






We define W_* âˆˆ^d Ã— d as follows

    W_* = U_* Î£_* V_*^âŠ¤

where U_* âˆˆ^n Ã— k are orthonormal columns,  
and V_* âˆˆ^n Ã— k are orthonormal columns.
Let Ïƒ_1^*, Ïƒ_2^*, â‹¯Ïƒ_k^* denote the diagonal entries of diagonal  matrix Î£_* âˆˆ^d Ã— d.



Let W_* be defined as DefinitionÂ <ref>. We define Îº to the condition number of W_*, i.e.,

    Îº : = Ïƒ_1/Ïƒ_k.

It is obvious that Îºâ‰¥ 1.



For each i âˆˆ [m], let x_i,y_i denote samples from N(0,I_d).

For each i âˆˆ [m], we define

    A_i = x_i y_i^âŠ¤

and

    b_i = x_i^âŠ¤ W_* y_i.





 Â§.Â§ Operators




For each i âˆˆ [m], let A_i and b_i be defined as DefinitionÂ <ref>. 

 We define W_0 := 1/mâˆ‘_i=1^m b_i A_i.

We say initialization matrix W_0 âˆˆ^d Ã— d is an Ïµ-good operator if 

    W_0 - W_* â‰¤ W_* Â·Ïµ.





 
For any vectors u,v, we define  

    
  * B_x:=1/mâˆ‘_l=1^m(y_l^âŠ¤ v)^2x_lx_l^âŠ¤
    
  * B_y:=1/mâˆ‘_l=1^m(x_l^âŠ¤ u)^2 y_ly_l^âŠ¤
  
 We say B = (B_x,B_y) is Ïµ-operator if the following holds: 

 
 
  * B_x-Iâ‰¤Ïµ 
 
  * B_y-Iâ‰¤Ïµ
 



For any vectors u,v âˆˆ^d. 
We define  

    
  * G_x:=1/mâˆ‘_l=1^m(y_l^âŠ¤ v)(y_l^âŠ¤ v_)x_lx_l^âŠ¤ 
    
  * G_y:=1/mâˆ‘_l=1^m(x_l^âŠ¤ u)(x_l^âŠ¤ u_ ) y_ly_l^âŠ¤

 u,u_âˆˆ^d,v,v_âˆˆ^d are unit vectors, s.t., u^âŠ¤ u_=0 and v^âŠ¤ v_=0. 
 We say G = (G_x,G_y) is Ïµ-operator if the following holds

 
 
  * G_xâ‰¤Ïµ,
 
  * G_yâ‰¤Ïµ.
 




 Â§.Â§ Main Result



We prove our main convergence result as follows:  

Let W_* âˆˆ^d Ã— d be defined as DefinitionÂ <ref>. 



Also, let ğ’œ:^d Ã— dâ†’^m be a linear measurement operator parameterized by m matrices, i.e., ğ’œ={A_1,A_2,â‹¯,A_m} where A_l=x_l y_l^âŠ¤. Let ğ’œ(W) be as given by

    b=ğ’œ(W)=
        [ [ A_1^âŠ¤ W] [ A_2^âŠ¤ W]          â‹¯  [A_m^âŠ¤ W] ]^âŠ¤



If the following conditions hold 

    
  * Ïµ= 0.001 / (k^1.5Îº ) 
    
  * T =  100log( Îº k / Ïµ_0)
    
  * Let {(b_i,A_i)}_iâˆˆ [m] be an Ïµ-init operator (DefinitionÂ <ref>).
    
  * Let B be an Ïµ-operator (DefinitionÂ <ref>).  
    
  * Let G be an Ïµ-operator(DefinitionÂ <ref>).

Then, after T-iterations of the alternating minimization method (Algorithm <ref>), we obtain W_T=U_T V_T^âŠ¤ s.t., 

    W_T-W_*â‰¤Ïµ_0.



 

 

We first present the update equation for VÌ‚_t+1âˆˆ^d Ã— k. 



 

Also, note that using the initialization property (first property mentioned in TheoremÂ <ref>), we get, 

    W_0 -W_*â‰¤ÏµÏƒ_1^* â‰¤Ïƒ_k^*/100 .


Now, using the standard sin theta theorem for singular vector perturbation <cit.>, we get: 

    (U_0,U_*) â‰¤   Â 1/100
    (V_0,V_*) â‰¤   Â 1/100


After T iteration (via LemmaÂ <ref>), we obtain

    (U_T,U_*) â‰¤   Â  (1/4)^T 
    (V_T,V_*) â‰¤   Â  (1/4)^T

which implies that

    W_T - W_* â‰¤Ïµ_0











 Â§.Â§ Main Induction Hypothesis




    We define Ïµ_d: = 1/10.
    We assume that Ïµ= 0.001 / (k^1.5Îº ).
    For all t âˆˆ [T], we have the following results.
    
    
        
  * Part 1. If (U_t,U_*) â‰¤1/4(V_t, V_*) â‰¤Ïµ_d, then we have
        
            
  * (V_t+1, V_*) â‰¤1/4(U_t,U_*) â‰¤Ïµ_d
        
        
  * Part 2. If (V_t+1, V_*) â‰¤1/4(U_t,U_*) â‰¤Ïµ_d, then we have
        
            
  * (U_t+1,U_*) â‰¤1/4(V_t+1, V_*) â‰¤Ïµ_d
        
    



Proof of Part 1.

Recall that for each i âˆˆ [n], we have

    b_i = x_i^âŠ¤ W_* y_i = âŸ¨ x_i y_i^âŠ¤ , W_* âŸ©  = âŸ¨ A_i, W_* âŸ© = [A_i^âŠ¤ W_*].


 

Recall that  

    VÌ‚_t+1=   Â min_Vâˆˆ^dÃ— kâˆ‘_i=1^m(b_i-x_i^âŠ¤ U_t V^âŠ¤ y_i)^2
    
    =   Â min_Vâˆˆ^dÃ— kâˆ‘_i=1^m(x_i^âŠ¤ W_* y_i-x_i^âŠ¤ U_t V^âŠ¤ y_i)^2


 

Hence, by setting gradient of this objective function to zero. Let F âˆˆ^d Ã— k be defined as DefinitionÂ <ref>.

We have V_t+1âˆˆ^d Ã— k can be written as follows: 

    VÌ‚_t+1 = W_*^âŠ¤ U_t - F

where F âˆˆ^d Ã— k is the error matrix

    F = [ F_1 F_2   â‹¯ F_k ]

where F_i âˆˆ^d for each i âˆˆ [k].  
 



Then, using the definitions of F âˆˆ^d Ã— k and DefinitionÂ <ref>,  
we get:


    [
        [ F_1;   â‹®; F_k ]]
        =B^-1(BD-C)SÂ·(V_*)

where (V_*) âˆˆ^dk is the vectorization of matrix V_* âˆˆ^d Ã— k.
 
 

Now, recall that in the t+1-th iteration of AlgorithmÂ <ref>, V_t+1âˆˆ^d Ã— k is obtained by QR decomposition of VÌ‚_t+1âˆˆ^d Ã— k. Using notation mentioned above,

    VÌ‚_t+1=V_t+1R


where R âˆˆ^k Ã— k denotes the lower triangular matrix R_t+1âˆˆ^k Ã— k obtained by the QR decomposition of V_t+1âˆˆ^d Ã— k.

 

We can rewrite V_t+1âˆˆ^d Ã— k as follows

    V_t+1 =    Â VÌ‚_t+1 R^-1
    
          =    Â  (W_*^âŠ¤ U_t-F)R^-1

where the first step follows from  Eq.Â (<ref>) , and the last step follows from Eq.Â (<ref>).

Multiplying both the sides by V_*,âˆˆ^d Ã— (d-k), where V_*,âˆˆ^d Ã— (d-k) is a fixed orthonormal basis of the subspace orthogonal to span(V_*), using ClaimÂ <ref>

    (V_*, )^âŠ¤ V_t+1 = -(V_*, )^âŠ¤ FR^-1


Thus, we get:

    (V_t+1, V_*) =   Â (V_*, )^âŠ¤ V_t+1
    
        =    Â  (V_*, )^âŠ¤ F R^-1
        =    Â  F R^-1
    â‰¤   Â FÂ·R^-1
    â‰¤   Â  0.001 Ïƒ_k^* (U_t, U_*) Â· R^-1
    â‰¤   Â  0.001 Ïƒ_k^* (U_t, U_*) Â· 2 (Ïƒ_k^*)^-1
    â‰¤   Â  0.01 Â·(U_t,U_*)


where the first step follows from definition of  (see DefinitionÂ <ref>), the second step follows from Eq.Â (<ref>), the third step follows from V_*, is an orthonormal basis, 
 and the forth step follows from  FactÂ <ref>, the fifth step follows from Lemma.Â <ref>, the sixth step follows from LemmaÂ <ref> (In order to run this lemma, we need to the condition of Part 1 statement to be holding), the last step follows from simple algebra. 


Proof of Part 2.

Similarly, we can prove this as Part 1.



 



Â§ MEASUREMENTS ARE GOOD OPERATOR


In this section, we provide detailed analysis for our operators. First SectionÂ <ref> we introduce some standard results for truncated Gaussian. In SectionÂ <ref> and SectionÂ <ref> we bound the term Z_i and [Z_iZ_i^âŠ¤] respectively. In SectionÂ <ref> we state our main lemma. In SectionÂ <ref> we show that out initialization is good. In SectionÂ <ref> we show our two operators are good. 
 



 Â§.Â§ Tools for Gaussian



We state a standard tool from literature,

    Let X âˆ¼ğ’³_k^2 be a chi-squared distributed random variable with k degrees of freedom. Each one has zero means and Ïƒ^2 variance. 
    
    Then it holds that
    
    [X - kÏƒ^2 â‰¥ (2âˆš(kt) + 2t) Ïƒ^2]
            â‰¤   Â exp(-t)
    [kÏƒ^2 - X â‰¥ 2âˆš(kt)Ïƒ^2]
            â‰¤   Â exp(-t)

    Further if k â‰¥Î©(Ïµ^-2 t) and t â‰¥Î©(log(1/Î´)), then we have
    
    [ | X - k Ïƒ^2 | â‰¤Ïµ k Ïƒ^2 ] â‰¤Î´.



We state a standard fact for the 4-th moment of Gaussian distribution.

Let x âˆ¼ N(0,Ïƒ^2), then it holds that _x âˆ¼ N(0,Ïƒ^2)[x^4] = 3 Ïƒ^2.



Let x âˆ¼ N(0, Ïƒ^2 I_d) denote a random Gaussian vector. Then we have


  * Part 1 

    [x x^âŠ¤ x x^âŠ¤] = (d+2) Ïƒ^4


  * Part 2

    [ x x^âŠ¤ x x^âŠ¤ ] = (d+2) Ïƒ^4




We define A:=xx^âŠ¤ xx^âŠ¤. Then we have

    A_i,j = x_i âˆ‘_l=1^d x_l x_l x_j

For i=j, we have

    [A_i,i] =    Â [ x_i âˆ‘_l=1^d x_l x_l x_i ] 
    
    =    Â [x_i(âˆ‘_l=1^i-1x_l x_l + x_i x_i + âˆ‘_l=i+1^d x_l x_l) x_i] 
    
    =    Â [x_i^4] + âˆ‘_l âˆˆ [d] \ i[x_l^2 x_i^2] 
    
    =    Â [x_i^4] + âˆ‘_l âˆˆ [d] \ i[x_l^2] [x_i^2] 
    
    =    Â [x_i^4] + (d-1) Ïƒ^4 
    
    =    Â  3 Ïƒ^4 + (d-1) Ïƒ^4 
    
    =    Â  (d + 2) Ïƒ^4


where the third step follows from linearity of expectation (FactÂ <ref>), the forth step follows from x_l and x_i are independent, the fifth step follows _z âˆ¼ N(0,Ïƒ^2)[z^4] =3 Ïƒ^4.
 

For iâ‰  j, we have

    [A_i,j] =    Â [ x_i âˆ‘_l=1^d x_l x_l x_j ] 
    
    =    Â [x_i x_j^3] + [x_i^3 x_j] + âˆ‘_l âˆˆ [d] \ i,j[x_i x_l^2 x_j] 
    
    =    Â  0

where the second step follows from linearity of expectation (FactÂ <ref>).



[Rotation invariance property of Gaussian]

    Let A^âŠ¤âˆˆ^d Ã— k with k < d denote an orthonormal basis (i.e., AA^âŠ¤ = I_k). Then for a Gaussian x âˆ¼(0, Ïƒ^2 I_d), we have
    
    Ax âˆ¼(0, Ïƒ^2 I_k).




    Let y := Ax âˆˆ^k, then
    
    y_i = âˆ‘_j = 1^dA_ijx_j, Â Â âˆ€ i âˆˆ [k].

    By definition of Gaussian distribution
 
    
    y_i âˆ¼(0, Ïƒ^2âˆ‘_j = 1^dA_ij^2).

    Recall that A^âŠ¤ is an orthonormal basis.

    
    We have
    
    A_ij^2 = 1.

    Thus we have
    
    y âˆ¼(0, Ïƒ^2 I_k),





 Â§.Â§ Bounding 




Let x_i denote a random Gaussian vector samples from N(0, Ïƒ^2 I_d). Let y_i denote a random Gaussian vector samples from N(0, Ïƒ^2 I_d).

Let U_*, V_* âˆˆ^d Ã— k.

We define

    Z_i := x_i x_i^âŠ¤ U_* Î£_* V_*^âŠ¤ y_i y_i^âŠ¤, Â Â Â âˆ€ i âˆˆ [m]



  * Part 1. We have

    [  Z_i â‰¤ C^2 k^2 log^2(d/Î´) Ïƒ^4 Â·Ïƒ_1^* ] â‰¥ 1-Î´/(d).


  * Part 2. If k â‰¥Î©(log(d/Î´))  We have

    [  Z_i â‰¤ C^2 k^2 Ïƒ^4 Â·Ïƒ_1^* ] â‰¥ 1-Î´/(d).





Proof of Part 1.

We define 

    a_i := U_*^âŠ¤ x_i âˆˆ^k 
    
    b_i := V_*^âŠ¤ y_i âˆˆ^k

Since U_* and V_* are orthornormal basis, due to rotation invariance property of Gaussian (FactÂ <ref>) 
, we know that a_i âˆ¼ N(0,Ïƒ^2 I_k) and b_i âˆ¼ N(0, Ïƒ^2 I_k).

We also know that 


    x_i = (U_*^âŠ¤)^â€  a_i = U_* a_i 
    
    y_i = (V_*^âŠ¤)^â€  b_i = V_* b_i


Thus, by replacing x_i,y_i with a_i,b_i, we have

    Z_i  
    =    Â  x_i x_i^âŠ¤ U_* Î£_* V_*^âŠ¤ y_i y_i^âŠ¤
    
    =    Â  U_* a_i a_i^âŠ¤ U_*^âŠ¤ U_* Î£_* V_*^âŠ¤ V_* b_i b_i^âŠ¤ V_*^âŠ¤
    
    =    Â  U_* a_i a_i^âŠ¤Î£_* b_i b_i^âŠ¤ V_*^âŠ¤
    â‰¤   Â  U_* Â· a_i a_i^âŠ¤Â·Î£_* Â· b_i b_i^âŠ¤Â· V_*^âŠ¤
    â‰¤   Â Ïƒ_1^* Â· a_i _2^2 Â· b_i _2^2

where the second step follows from replacing x,y by a,b, the third step follows from U_*^âŠ¤ U_* = I and V_*^âŠ¤ V_* = I, the forth step follows from FactÂ <ref>. 

 

Due to property of Gaussian, we know that

    [ |a_i,j| > âˆš(Clog(d/Î´))Ïƒ ] â‰¤Î´/(d)


Taking a union bound over k coordinates, we know that

    [  a_i _2^2 â‰¤ C k log(d/Î´) Ïƒ^2 ] â‰¥ 1-Î´ /(d)

Similarly, we can prove it for b_i _2^2.


Proof of Part 2.
Since k â‰¥Î©(log(d/Î´)), then we can use LemmaÂ <ref> to obtain a better bound.
 







 Â§.Â§ Bounding 


We can show that

    [ Z_i Z_i^âŠ¤] â‰¤ C^2 k^2 Ïƒ^4 (Ïƒ_1^*)^2.







 

Using LemmaÂ <ref>


    _a âˆ¼ N(0, Ïƒ^2 I_k )[ a_i a_i^âŠ¤ a_i a_i^âŠ¤ ] â‰¤ C k Ïƒ^2.

Thus, we have

    [ a_i a_i^âŠ¤ a_i a_i^âŠ¤] â‰¼ Ck Ïƒ^2 Â· I_k


Then, we have

    [Z_i Z_i^âŠ¤] 
             =    Â _x,y[ x_i x_i^âŠ¤ U_* Î£_* V_*^âŠ¤ y_i y_i^âŠ¤ y_i y_i^âŠ¤ V_* Î£_* U_*^âŠ¤ x_i x_i^âŠ¤ ] 
    
             =    Â _a,b[ U_* a_i a_i^âŠ¤ U_*^âŠ¤ U_* Î£_* V_*^âŠ¤ V_* b_i b_i^âŠ¤ V_*^âŠ¤ V_* b_i b_i^âŠ¤ V_*^âŠ¤ V_* Î£_* U_*^âŠ¤ U_* a_i a_i^âŠ¤ U_*^âŠ¤ ]
    
             =    Â _a,b[ U_* a_i a_i^âŠ¤Î£_*  b_i b_i^âŠ¤  V_*^âŠ¤ V_* b_i b_i^âŠ¤Î£_*  a_i a_i^âŠ¤ U_*^âŠ¤ ] 
     
             =    Â _a,b[ U_* a_i a_i^âŠ¤Î£_*  b_i b_i^âŠ¤ b_i b_i^âŠ¤Î£_*  a_i a_i^âŠ¤ U_*^âŠ¤ ] 
    â‰¤   Â _a,b[ a_i a_i^âŠ¤Î£_*  b_i b_i^âŠ¤ b_i b_i^âŠ¤Î£_*  a_i a_i^âŠ¤  ]  
    â‰¤   Â _a[ a_i a_i^âŠ¤Î£_* _b[ b_i b_i^âŠ¤ b_i b_i^âŠ¤ ] Î£_*  a_i a_i^âŠ¤  ]  
    â‰¤   Â  C^2 k^2 Ïƒ^4 (Ïƒ_1^*)^2

     where the first step follows from the definition of Z_i, the second step follows from replacing x_i,y_i with a_i,b_i, the third step follows from U_*,V_* are orthonormal columns, the fourth step follows from V_* are orthonormal columns, the fifth step follows from
    U_* â‰¤ 1
    , the sixth step follows from
    using LemmaÂ <ref> twice.





 Â§.Â§ Main Results



We prove our main result for measurements.  



     Let {A_i,b_i}_iâˆˆ [m] denote measurements be defined as DefinitionÂ <ref>.

     Assuming the following conditions are holding
     
        
  * k = Î©(log(d/Î´))
        
  * m = Î©(Ïµ^-2 (d+k^2) log(d/Î´))
      
     
     Then, 
     
        
  * The property in DefinitionÂ <ref>, initialization is a Ïµ-operator
        
  * The property in DefinitionÂ <ref>, B are Ïµ-operator.
        
  * The property in DefinitionÂ <ref>, G are Ïµ-operator.
     
     holds with probability at least 1-Î´/(d).



Using LemmaÂ <ref> and LemmaÂ <ref>, we complete the proof.





 Â§.Â§ Initialization Is a Good Operator



We define matrix S âˆˆ^d Ã— d as follows

    S: = 1/mâˆ‘_i=1^m b_i A_i.


If the following two condition holds
 

    
  * Condition 1. k = Î©(log(d/Î´)),
    
  * Condition 2. m = Î©( Ïµ^-2 k^2 log(d/Î´) ).


Then we have

    [  S  - W_* â‰¤ÏµÂ· W_*  ] â‰¥ 1-Î´.







    
 
    
    (Initialization in DefinitionÂ <ref>) Now, we have: 

    
    S =    Â 1/mâˆ‘_i=1^m b_i A_i 
    
              =    Â 1/mâˆ‘_i=1^m b_i x_i y_i^âŠ¤
    
              =    Â 1/mâˆ‘_i=1^m  x_i b_i y_i^âŠ¤
    
              =    Â 1/mâˆ‘_i=1^m x_i x_i^âŠ¤ W_* y_i  y_i^âŠ¤
     
              =    Â 1/mâˆ‘_i=1^m x_i x_i^âŠ¤ U_* Î£_* V_*^âŠ¤ y_i y_i^âŠ¤,

     where the first step follows from DefinitionÂ <ref>, the second step follows from A_i = x_i y_i^âŠ¤, the third step follows from b_i is a scalar, the forth step follows from b_i = x_i^âŠ¤ W_* y_i, the fifth step follows from W_* = U_* Î£_* V_*^âŠ¤. 

For each i âˆˆ [m], we define matrix Z_i âˆˆ^d Ã— d as follows:

    Z_i := x_i x_i^âŠ¤ U_* Î£_* V_*^âŠ¤ y_i y_i^âŠ¤,

then we can rewrite S âˆˆ^d Ã— d in the following sense,
     
    S = 1/mâˆ‘_i=1^m Z_i

     
     Note that, we can compute [Z_i] âˆˆ^d Ã— d
     
    _x_i,y_i[Z_i]
            =    Â _x_i, y_i[  x_ix_i^âŠ¤_d Ã— d U_* Î£_* V_*^âŠ¤_ d Ã— d y_i y_i^âŠ¤_d Ã— d ] 
    
            =    Â _x_i[  x_ix_i^âŠ¤_d Ã— d U_* Î£_* V_*^âŠ¤_ d Ã— d ]  Â·_y_i [ y_i y_i^âŠ¤_d Ã— d ] 
    
            =    Â _x_i [x_i x_i^âŠ¤ ] Â· U_* Î£_* V_*^âŠ¤Â·_y_i[ y_i y_i^âŠ¤] 
     
            =    Â  U_* Î£_* V_*^âŠ¤

    where the first step follows definition of Z_i, the second step follows from x_i and y_i are independent and FactÂ <ref>, the third step follows from FactÂ <ref> the forth step follows from [x_ix_i^âŠ¤] = I_d and [y_i y_i^âŠ¤] = I_d.

 

     As S âˆˆ^d Ã— d is a sum of m random matrices, the goal is to apply TheoremÂ <ref>
    to show that S is close to
    
    [S] =    Â   W_* 
    
        =    Â  U_* Î£_* V_*^âŠ¤

    for large enough m. 
    
Using LemmaÂ <ref> (Part 2) with choosing Gaussian variance Ïƒ^2=1, we have
 

    [  Z_i â‰¤ C^2 k^2  Ïƒ_1^*, âˆ€ i âˆˆ [m] ] â‰¥ 1-Î´/(d)




    
     Using LemmaÂ <ref> with choosing Gaussian variance Ïƒ^2= 1, we can bound [Z_i Z_i^âŠ¤] as follows  
     
     
    [Z_i Z_i^âŠ¤] â‰¤   Â  C^2 k^2 (Ïƒ_1^*)^2



    
 Let Z = âˆ‘_i=1^m (Z_i - W_*).
 
 Applying TheoremÂ <ref> we get 
     
    [  Z â‰¥ t ] â‰¤ 2d Â·exp( -t^2/2/[Z] + M t/3 )

     where

 
    Z =    Â  m S - m W_* 
    [Z] =    Â  m Â· C^2 k^2  (Ïƒ_1^*)^2,    Â byÂ Eq.Â (<ref>)
    
     M=    Â  C^2 k^2 Ïƒ_1^*    Â byÂ Eq.Â (<ref>)


Replacing t= ÏµÏƒ_1^* m and Z = mS - mW_* inside [] in Eq.Â (<ref>), we have 


    [  S - W^* â‰¥   Â ÏµÏƒ_1^* ] â‰¤ 2d Â·exp( -t^2 /2/[Z] + M t /3)

Our goal is to choose m sufficiently large such that the above quantity is upper bounded by 2d Â·exp ( - Î©( log(d/Î´) )).

First, we need 


    t^2/[Z]
    =    Â Ïµ^2 m^2 (Ïƒ_1^*)^2 / m Â· C^2 k^2  (Ïƒ_1^*)^2  
    
    =    Â Ïµ^2 m / C^2 k^2  
    â‰¥   Â log(d/Î´)

where the first step follows from choice of t and bound for [Z].

This requires

    m â‰¥ C^2 Ïµ^-2 k^2 log(d/Î´)


Second, we need 

    t^2 /  M t   =    Â Ïµ m Ïƒ_1^* / M 
    
    =    Â Ïµ m Ïƒ_1^* /C^2 k^2   Ïƒ_1^*
    
    =    Â Ïµ m /C^2 k^2  
    â‰¥   Â log(d/Î´)

where the first step follows from choice of t and the second step follows from bound on M.

This requires

    m â‰¥ C^2 Ïµ^-2 k^2 log(d/Î´)

    
  Finally, we should choose
     
    m â‰¥ 10C^2 Ïµ^-2 k^2 log(d/Î´) ,


    Which implies that 
     
    [  S - W_* â‰¤ÏµÂ·Ïƒ_1^* ] â‰¥ 1- Î´/(d).

     
   Taking the union bound with all Z_i are upper bounded, then we complete the proof.


 



 Â§.Â§ Operator  and  is good




If the following two conditions hold

    
  * Condition 1. d = Î©(log(d/Î´))
    
  * Condition 2. m = Î©(Ïµ^-2 d log(d/Î´))

Then operator B (see DefinitionÂ <ref>) is Ïµ good, i.e.,

    [  B_x - I_dâ‰¤Ïµ ] â‰¥   Â  1-Î´/(d)  
    [  B_y - I_d â‰¤Ïµ ] â‰¥   Â  1-Î´/(d)

Similar results hold for operator G (see DefinitionÂ <ref>).






 

 
     
     Recall that
     B_x:=1/mâˆ‘_l=1^m(y_l^âŠ¤ v)^2x_lx_l^âŠ¤.

     Recall that B_y:=1/mâˆ‘_l=1^m(x_l^âŠ¤ u)^2 y_ly_l^âŠ¤.
     
     Now, as x_i,y_i are rotationally invariant random variables 
     , wlog, we can assume u=e_1.
     
     We use x_i,1âˆˆ to denote the first entry of x_i âˆˆ^d. 

     
     Thus,  
     
    (x_i^âŠ¤ u u^âŠ¤ x_i)=x_i,1^2

     Then 
     
    [ (x_i^âŠ¤ u u^âŠ¤ x_i)^2 ] = [x_i,1^4 ] = 3


    We define
    
    Z_i = (x_i^âŠ¤ u)^2 y_i y_i^âŠ¤

    then
    
    [Z_i] = I_d
    
    
   Using similar idea in LemmaÂ <ref>, we have
    
    [  Z_i â‰¤ C d , âˆ€ i âˆˆ [m] ] â‰¥ 1- Î´/(d)

    
    We can bound
    
    [ Z_i Z_i^âŠ¤ ] 
        =    Â _x,y[ (x_i^âŠ¤ u)^2 y_i y_i^âŠ¤  y_i y_i^âŠ¤  (x_i^âŠ¤ u)^2 ] 
    
        =    Â _x[ (x_i^âŠ¤ u)^2 _y[y_i y_i^âŠ¤  y_i y_i^âŠ¤ ]  (x_i^âŠ¤ u)^2 ] 
    
        =    Â  (d+2) Â· | _x[ (x_i^âŠ¤ u)^2  (x_i^âŠ¤ u)^2 ] | 
    
        =    Â  (d+2) Â· 3 
    â‰¤   Â  C d

where the fourth step follows from C â‰¥ 1 is a sufficiently large constant.


Let Z = âˆ‘_i=1^m (Z_i - I_d).

    Applying TheoremÂ <ref> we get
    
    [ Zâ‰¥ t] â‰¤ 2d Â·exp(-t^2/2/[Z] + Mt/3),

where

    Z =    Â  m Â· B - m Â· I 
    [Z] =    Â  C m d 
    
        M =    Â  C d


Using t = m Ïµ and Z = âˆ‘_i=1^m (Z_i - I_d), and B = 1/mâˆ‘_i=1^m Z_i, we have

    [  Z â‰¥ t] 
    =    Â [ âˆ‘_i=1^m (Z_i - I_d) â‰¥ m Ïµ ] 
    
    =    Â [ 1/mâˆ‘_i=1^m Z_i - I_d â‰¥Ïµ ] 
    
    =    Â [  B - I_d â‰¥Ïµ ]

    
By choosing t = m Ïµ and m = Î©(Ïµ^-2 d log(d/Î´)) we have

    
    [  B - I_d â‰¥Ïµ ] â‰¤Î´/(d).

where B can be either B_x or B_y.


    Similarly, we can prove 
    
    [G_xâ‰¤Ïµ] â‰¥ 1 - Î´, 
    [G_yâ‰¤Ïµ] â‰¥ 1 - Î´.









Â§ ONE SHRINKING STEP


In this section, we provide a shirking step for our result. In SectionÂ <ref> we define the matrices B, C, D ,S to be used in analysis. In SectionÂ <ref> we upper bound the norm of BD- C. In SectionÂ <ref> we show the update term V_t + 1 can be written in a different way. In SectionÂ <ref> and SectionÂ <ref> we upper bounded F and R^-1 respectively. 



 Â§.Â§ Definitions of 



 

For each p âˆˆ [k], let u_*,pâˆˆ^n denotes the p-th column of matrix U_* âˆˆ^n Ã— k. 

For each p âˆˆ [k], let u_t,p denote the p-th column of matrix U_t âˆˆ^n Ã— k.

We define block matrices B, C, D, S âˆˆ^kd Ã— kd as follows:
For each (p,q) âˆˆ [k] Ã— [k]

    
  * Let B_p,qâˆˆ^d Ã— d denote the (p,q)-th block of B 
    
    B_p,q= âˆ‘_i=1^m  y_i y_i^âŠ¤_d Ã— d Â matrixÂ· (x_i^âŠ¤ u_t,p)_scalarÂ· (x_i^âŠ¤ u_t,q) _scalar

    
  * Let C_p,qâˆˆ^d Ã— d denote the (p,q)-th block of C, 
    
    C_p,q= âˆ‘_i=1^m  y_i y_i^âŠ¤_ d Ã— d Â matrixÂ· (x_i^âŠ¤ u_t,p ) _scalarÂ· (x_i^âŠ¤ u_*q) _scalar

    
  * Let D_p,qâˆˆ^d Ã— d denote the (p,q)-th block of D, 
    
    D_p,q= u_t,p^âŠ¤ u_*q I

    
  * Let S_p,qâˆˆ^d Ã— d denote the (p,q)-th block of S, 
    
    S_p,q= Ïƒ_p^* I ,    ifÂ  p=q; 
    Â  0,    ifÂ  p  q.

    Here Ïƒ_1^*, â‹¯Ïƒ_k^* are singular values of W_* âˆˆ^d Ã— d.
   
  * We define F âˆˆ^d Ã— k as follows
   
    (F) _d Ã— 1 :=  B^-1_d Ã— d (BD-C) _d Ã— d S _d Ã— dÂ·(V_*) _d Ã— 1.







 Â§.Â§ Upper Bound on 



Let B, C and D be defined as DefinitionÂ <ref>. Then we have

    BD-Câ‰¤ÏµÂ·(U,U_*) Â·  k





Let z_1, â‹¯, z_k âˆˆ^d denote k vectors. Let z = [ z_1;   â‹®; z_k ]. 

We define f(z):=z^âŠ¤ (BD-  C)z

We define f(z,p,q) = z_p^âŠ¤ (BD-C)_p,q z_q.

Then we can rewrite

    z^âŠ¤ (BD - C) z
    =    Â âˆ‘_p=1^k âˆ‘_q=1^k z_p^âŠ¤ (BD-C)_p,q z_q 
    
    =    Â âˆ‘_p=1^k âˆ‘_q=1^k z_p^âŠ¤ ( B_p,: D_:,q - C_p,q ) z_q  
    
    =    Â âˆ‘_p=1^k âˆ‘_q=1^k z_p^âŠ¤ ( âˆ‘_l=1^k B_p,l D_l,q - C_p,q ) z_q

By definition, we know

    B_p,l =    Â âˆ‘_i=1^m y_i y_i^âŠ¤ (x_i^âŠ¤ u_t,p) Â· (  u_t,l^âŠ¤ x_i ) 
    
    D_l,q =    Â  (u_*,q^âŠ¤ u_t,l ) I_d 
    
    C_p,q =    Â âˆ‘_i=1^m y_i y_i^âŠ¤ (x_i^âŠ¤ u_t,p) Â· (  u_*,q^âŠ¤ x_i )


We can rewrite C_p,q as follows

    C_p,q = âˆ‘_i=1^m y_i y_i^âŠ¤Â· (x_i^âŠ¤ u_t,p) Â· (  u_*,q^âŠ¤ I_d x_i )


Let us compute 

    B_p,l D_l,q 
    =    Â âˆ‘_i=1^m y_i y_i^âŠ¤ (x_i^âŠ¤ u_t,p) Â· ( u_t,l^âŠ¤ x_i  ) Â· ( u_*,q^âŠ¤ u_t,l  )  
    
    =    Â âˆ‘_i=1^m y_i y_i^âŠ¤ (x_i^âŠ¤ u_t,p) Â·  ( u_*,q^âŠ¤ u_t,l  ) Â· ( u_t,l^âŠ¤ x_i  )

where the second step follows from a Â· b = b Â· a for any two scalars.

 

Taking the summation over all l âˆˆ [k], we have

    âˆ‘_l=1^k B_p,l D_l,q 
    =    Â âˆ‘_l=1^k âˆ‘_i=1^m y_i y_i^âŠ¤ (x_i^âŠ¤ u_t,p) Â·  ( u_*,q^âŠ¤ u_t,l  ) Â· ( u_t,l^âŠ¤ x_i  ) 
    
    =    Â âˆ‘_i=1^m y_i y_i^âŠ¤ (x_i^âŠ¤ u_t,p) Â·   u_*,q^âŠ¤âˆ‘_l=1^k (u_t,lÂ· u_t,l^âŠ¤ ) x_i   
    
    =    Â âˆ‘_i=1^m  y_i y_i^âŠ¤_matrixÂ· (x_i^âŠ¤ u_t,p) _scalarÂ·  u_*,q^âŠ¤  U_t U_t^âŠ¤ x_i _scalar

where first step follows from definition of B and D.

Then, we have

    âˆ‘_l=1^k B_p,l D_l,q - C_p,q
    =    Â  (âˆ‘_i=1^m  y_i y_i^âŠ¤_matrixÂ· (x_i^âŠ¤ u_t,p) _scalarÂ·  u_*,q^âŠ¤  U_t U_t^âŠ¤ x_i _scalar) - C_p,q
    
    =    Â  (âˆ‘_i=1^m  y_i y_i^âŠ¤_matrixÂ· (x_i^âŠ¤ u_t,p) _scalarÂ·  u_*,q^âŠ¤  U_t U_t^âŠ¤ x_i _scalar) - (âˆ‘_i=1^m y_i y_i^âŠ¤Â· (x_i^âŠ¤ u_t,p) Â· (  u_*,q^âŠ¤ I_d x_i )) 
    
    =    Â âˆ‘_i=1^m  y_i y_i^âŠ¤_matrixÂ· (x_i^âŠ¤ u_t,p) _scalarÂ·  u_*,q^âŠ¤ ( U_t U_t^âŠ¤ - I_d) x_i _scalar

where the first step follows from Eq.Â (<ref>), the second step follows from Eq.Â (<ref>), the last step follows from merging the terms to obtain (U_t U_t^âŠ¤ - I_d).


Thus,

    f(z,p,q)
    =    Â z_p^âŠ¤ ( âˆ‘_l=1^k B_p,l D_l,q - C_p,q ) z_q 
    
    =    Â âˆ‘_i=1^m  ( z_p^âŠ¤ y_i ) _scalar ( y_i^âŠ¤ z_q ) _scalarÂ· (x_i^âŠ¤ u_t,p) _scalarÂ·   u_*,q^âŠ¤  ( U_t U_t^âŠ¤ - I_d) x_i _scalar


 
For easy of analysis, we define v_t:= u_*,q^âŠ¤  ( U_t U_t^âŠ¤ - I_d). This means v_t lies in the complement of span of U_t. 

Then 

    v_t _2 
    =    Â  u_*,q^âŠ¤  ( U_t U_t^âŠ¤ - I_d) _2 
    
    =    Â  e_q^âŠ¤ U_*^âŠ¤ (U_t U_t^âŠ¤ - I_d) 
    â‰¤   Â  U_*^âŠ¤ (U_t U_t^âŠ¤ - I_d) 
    
    =    Â (U_*,U_t).

where the second step follows from u_*,q^âŠ¤ = e_q^âŠ¤ U_*^âŠ¤ (e_q âˆˆ^k is the vector q-th location is 1 and all other locations are 0s), 
third step follows from FactÂ <ref>.

We want to apply DefinitionÂ <ref>, but the issue is z_p, z_q and v_t are not unit vectors. So normalize them. Let z_p = z_p / z_p _2 , z_q = z_q / z_q _2 and v_t = v_t/  v_t _2.

In order to apply for DefinitionÂ <ref>, we also need v_t^âŠ¤ u_t,p=0. 

This is obvious true, since v_t lies in the complement of span of U_t and u_t,p in the span of U_t. 

We define 

    G := âˆ‘_i=1^m  (x_i^âŠ¤ u_t,p) _scalarÂ· (x_i^âŠ¤v_t) _scalarÂ· y_i y_i^âŠ¤_matrix


By  DefinitionÂ <ref>, we know that 

    G â‰¤Ïµ.

By definition of spectral norm, we have for any unit vector z_p and z_q, we know that

    |z_p^âŠ¤ G z_q | â‰¤ G â‰¤Ïµ.

where the first step follows from definition of spectral norm (FactÂ <ref>), and the last step follows from DefinitionÂ <ref>.
 

Note that

    f(p,q,z) =    Â âˆ‘_i=1^m  (x_i^âŠ¤ u_t,p) Â· (x_i^âŠ¤v_t) _scalarÂ· (z_p^âŠ¤ y_i) Â· (y_i^âŠ¤z_q) _scalarÂ· z_p _2 Â· z_q _2 Â· v_t _2 _scalar
    
    =    Â z_p^âŠ¤_ 1 Ã— dÂ·( âˆ‘_i=1^m  (x_i^âŠ¤ u_t,p) Â· (x_i^âŠ¤v_t) _scalarÂ· y_i y_i^âŠ¤_ d Ã— d ) Â·z_q _d Ã— 1Â· z_p _2 Â· z_q _2 Â· v_t _2 _scalar
    
    =    Â z_p^âŠ¤_1 Ã— dÂ· G _d Ã— dÂ·z_q _d Ã— 1Â· z_p _2 Â· z_q _2 Â· v_t _2 _scalar

where the second step follows from rewrite the second scalar (z_p^âŠ¤ y_i) (y_i^âŠ¤z_q) = z_p^âŠ¤ (y_i y_i^âŠ¤) z_q, the last step follows from definition of G.

Then,

    |f(z,p,q)|
    =    Â  | âˆ‘_i=1^m z_p^âŠ¤ G z_q | Â· z_p _2  z_q _2  v_t _2 
    â‰¤   Â Ïµ z_p _2  z_q _2 Â· v_t _2 
    â‰¤   Â Ïµ z_p _2  z_q _2 Â·(U_t,U_*)

where the last step follows from Eq.Â (<ref>).



Finally, we have

    BD-C
        =    Â max_z,z_2=1|z^âŠ¤(BD-C)z| 
    
        =    Â max_z,z_2=1|âˆ‘_ p âˆˆ [ k ],q âˆˆ [k] f(z,p,q)| 
    â‰¤   Â max_z,z_2=1âˆ‘_ p âˆˆ [ k ],q âˆˆ [k] | f(z,p,q)| 
    â‰¤   Â ÏµÂ·(U_t,U_*) max_z,z_2=1âˆ‘_pâˆˆ [k] , q âˆˆ [k] z_p_2z_q_2 
    â‰¤   Â ÏµÂ·(U,U_*) Â·  k

where the first step follows from FactÂ <ref>, the last step step follows from âˆ‘_p=1^k  z_p _2 â‰¤âˆš(k) (âˆ‘_p=1^k  z_p _2^2)^1/2 = âˆš(k).

  





 Â§.Â§ Rewrite 




If 

    V_t+1 = (W_*^âŠ¤ U_t-F)R^-1

then, 

    (V_*, )^âŠ¤ V_t+1 = -(V_*, )^âŠ¤ FR^-1





 
    Multiplying both sides by V_*,âˆˆ^d Ã— (d-k):
    
    V_t+1=   Â  (W_*^âŠ¤ U_t-F)R^-1
    
            (V_*, )^âŠ¤ V_t+1=   Â (V_*, )^âŠ¤(W_*^âŠ¤ U_t-F)R^-1
    
            (V_*, )^âŠ¤ V_t+1=   Â (V_*, )^âŠ¤ W_*^âŠ¤ R^-1-(V_*, )^âŠ¤ FR^-1

    We just need to show (V_*, )^âŠ¤ W_*^âŠ¤ R^-1=0.

By definition of V_*,, we know:

    V_*,^âŠ¤ V_*= 0_k Ã— (n-k)


Thus, we have:

    (V_*, )^âŠ¤ W_*^âŠ¤ =   Â  V_*,^âŠ¤ V_* Î£_* U_*^âŠ¤
    
        =   Â  0

 






 Â§.Â§ Upper bound on 



Let ğ’œ be a rank-one measurement operator where A_i = x_i u_i^âŠ¤. Let Îº be defined as DefinitionÂ <ref>.  

 
 Then, we have
 
    F â‰¤ 2 Ïµ k^1.5Â·Ïƒ_1^* Â·(U_t,U_*)


 Further, if Ïµâ‰¤ 0.001 / ( k^1.5Îº )

    F â‰¤ 0.01 Â·Ïƒ_k^* Â·(U_t,U_*).



Recall that 

    (F) = B^-1(BD-C)S Â·(V_*).


Here, we can upper bound F as follows

    Fâ‰¤   Â F_F 
    
        =    Â (F) _2 
    â‰¤   Â B^-1Â·BD-CÂ·SÂ·(V_*)_2 
    
        =    Â B^-1Â·(BD-C)Â· S Â·âˆš(k)
    â‰¤   Â B^-1Â·(BD-C)Â·Ïƒ_1^* Â·âˆš(k)

where the first step follows from Â·â‰¤Â·_F (FactÂ <ref>), the second step follows vectorization of F is a vector, the third step follows from A x _2 â‰¤A Â· x _2, the forth step follows from (V_*) _2 =  V_* _F â‰¤âˆš(k)  
(FactÂ <ref>) and the last step follows from S â‰¤Ïƒ_1^* (see DefinitionÂ <ref>). 

Now, we first bound B^-1=1/(Ïƒ_min(B)). 

Also, let Z=[ z_1 z_2   â‹¯ z_k ] and let z=(Z). 


Note that B_p,q denotes the (p,q)-th block of B.

We define 

    B := { x âˆˆ^kdÂ |Â  x _2 = 1 }.


Then  



    Ïƒ_min(B)
        =    Â min_z âˆˆ Bz^âŠ¤ B z 
    
        =    Â min_z âˆˆ Bâˆ‘_ p âˆˆ [k], q âˆˆ [k] z_p^âŠ¤ B_pqz_q 
    
        =    Â min_z âˆˆ Bâˆ‘_p=1^k z_p^âŠ¤ B_p,pz_p+âˆ‘_pâ‰  qz_p^âŠ¤ B_p,qz_q.

where the first step follows from FactÂ <ref>, 
the second step follows from simple algebra, the last step follows from  
(FactÂ <ref>).



 

We can lower bound z_p^âŠ¤ B_p,pz_p as follows

    z_p^âŠ¤ B_p,p z_p
        â‰¥   Â Ïƒ_min(B_p,p) Â· z_p _2^2 
    â‰¥   Â  (1-Ïµ) Â· z_p _2^2

where the first step follows from FactÂ <ref>  
, the last step follows from DefinitionÂ <ref> .


We can upper bound | z^âŠ¤ B_p,q z_q | as follows,

    |z_p^âŠ¤ B_p,q z_q|
        â‰¤   Â  z_p _2 Â· B_p,qÂ· z_q _2 
    â‰¤   Â ÏµÂ· z_p _2 Â· z_q _2

where the first step follows from FactÂ <ref>, the last step follows from DefinitionÂ <ref> . 

We have

    Ïƒ_min(B)
        =    Â min_z,z_2=1âˆ‘_p=1^k z_p^âŠ¤ B_p,pz_p+âˆ‘_pâ‰  qz_p^âŠ¤ B_p,qz_q 
    â‰¥   Â min_z,z_2=1 (1-Ïµ)âˆ‘_p=1^k  z_p _2^2 +âˆ‘_pâ‰  qz_p^âŠ¤ B_p,qz_q 
    â‰¥   Â min_z,z_2=1(1-Ïµ)âˆ‘_p=1^kz_p_2^2-Ïµâˆ‘_p â‰  qz_p_2z_q_2 
    
        =    Â min_z,z_2=1 (1-Ïµ) -Ïµâˆ‘_p â‰  qz_p_2z_q_2  
    
        =    Â min_z,z_2=1 (1-Ïµ) - k Ïµ
    â‰¥   Â  1- 2 kÏµ
    â‰¥   Â  1/2



where the first step follows from Eq.Â (<ref>),
the second step follows from Eq.Â (<ref>), the third step follows from Eq.Â (<ref>), the forth step follows from âˆ‘_p=1^k  z_p _2^2 = 1(which derived from the z_2=1 constraint and the definition of z_2), the fifth step follows from âˆ‘_p â‰  q z_p _2  z_q _2 â‰¤ k, 
and the last step follows from Ïµâ‰¤ 0.1/k. 

 We can show that
 
    B^-1 = Ïƒ_min(B) â‰¤ 2.

 where the first step follows from FactÂ <ref>, the second step follows from Eq.Â (<ref>).

Now, consider BD-C, using ClaimÂ <ref>, we have

    BD-Câ‰¤  kÂ·ÏµÂ·(U_t,U_*)



Now, we have

    F â‰¤   Â  B^-1Â· (BD - C) Â·Ïƒ_1^* Â·âˆš(k)
    â‰¤   Â  2 Â· (BD - C) Â·Ïƒ_1^* Â·âˆš(k)
    â‰¤   Â  2 Â· k Â·ÏµÂ·(U_t,U_*) Â·Ïƒ_1^* Â·âˆš(k)
 
where the first step follows from EqÂ .(<ref>), the second step follows from Eq.Â (<ref>),  
and the third step follows from  Eq.Â (<ref>).




 Â§.Â§ Upper bound on 



Let ğ’œ be a rank-one measurement operator matrix where A_i=x_i y_i^âŠ¤. Also, let ğ’œ satisfy three properties mentioned in Theorem <ref>.

If the following condition holds

    
  * (U_t, U_*) â‰¤1/4â‰¤Ïµ_d = 1/10 (The condition of Part 1 of LemmaÂ <ref>)



Then, 

    R^-1â‰¤   Â  10 /Ïƒ_k^*



For simplicity, in the following proof, we use V to denote V_t+1. We use U to denote U_t.

Using FactÂ <ref>

    R^-1 = Ïƒ_min(R)^-1


We can lower bound Ïƒ_min(R) as follows:

    Ïƒ_min(R)
        =    Â min_z,z_2=1Rz_2 
    
        =    Â min_z,z_2=1VRz_2 
    
        =    Â min_z,z_2=1V_*Î£_*U_*^âŠ¤ Uz-Fz_2 
    â‰¥   Â min_z,z_2=1V_*Î£_*U_*^âŠ¤ Uz_2-Fz_2 
    â‰¥   Â min_z,z_2=1V_*Î£_*U_*^âŠ¤ Uz_2-F

where the first step follows from definition of Ïƒ_min,
the second step follows from FactÂ <ref>,  
the third step follows from V = (W_*^âŠ¤ U-F)R^-1 =  (V_* Î£_* U_*^âŠ¤ U - F) R^-1 (due to Eq.Â (<ref>) and DefinitionÂ <ref>)  
, the forth step follows from triangle inequality,  
the fifth step follows from A x _2 â‰¤ A for all x _2=1.

Next, we can show that

    min_z,z_2=1V_*Î£_*U_*^âŠ¤ Uz_2
    =    Â min_z,z_2=1Î£_*U_*^âŠ¤ Uz_2 
    â‰¥   Â min_z,z_2=1Ïƒ_k^* Â· U_*^âŠ¤ Uz_2 
    
    =    Â Ïƒ_k^* Â·Ïƒ_min(U^âŠ¤ U_*)
 
where the first step follows from FactÂ <ref>,  
 the second step follows from FactÂ <ref>, the third step follows from definition of Ïƒ_min,  
 
Next, we have

    Ïƒ_min(U^âŠ¤ U_*) 
    =    Â cosÎ¸(U_*, U) 
    
    =    Â âˆš(1-sin^2 Î¸(U_*,U))
    â‰¥   Â âˆš(1- (U_*,U)^2)
 
where the first step follows definition of cos, the second step follows from sin^2 Î¸ + cos^2 Î¸  =1 (LemmaÂ <ref>), the third step follows from sinâ‰¤ (see DefinitionÂ <ref>).


Putting it all together, we have

    Ïƒ_min(R) â‰¥   Â Ïƒ_k^* âˆš(1-(U_*,U)^2) -  F 
    â‰¥   Â Ïƒ_k^* âˆš(1-(U_*,U)^2) - 0.001 Ïƒ_k^* (U_*,U) 
    
    =    Â Ïƒ_k^* ( âˆš(1-(U_*,U)^2) - 0.001 (U_*,U)  ) 
    â‰¥   Â  0.2 Ïƒ_k^*

where the second step follows from LemmaÂ <ref>, the last step follows from (U_*,U) < 1/10.
 





Â§ MATRIX SENSING REGRESSION



Our algorithm has O(log(1/Ïµ_0)) iterations, in previous section we have proved why is that number of iterations sufficient. In order to show the final running time, we still need to provide a bound for the time we spend in each iteration. In this section, we prove a bound for cost per iteration. 
In SectionÂ <ref> we provide a basic claim that, our sensing problem is equivalent to some regression problem. In SectionÂ <ref> we show the different running time of the two implementation of each iteration. In SectionÂ <ref> we provide the time analysis for each of the iteration of our solver. In SectionÂ <ref> shows the complexity for the straightforward solver. Finally in SectionÂ <ref> we show the bound for the condition number. 



 Â§.Â§ Definition and Equivalence


In matrix sensing, we need to solve the following problem per iteration:


Let A_1,â€¦,A_m âˆˆ^dÃ— d, Uâˆˆ^dÃ— k and bâˆˆ^m be given. The goal is to solve the following minimization problem

    min_Vâˆˆ^dÃ— kâˆ‘_i=1^m ([A_i^âŠ¤ U V^âŠ¤]-b_i)^2,



We define another regression problem 

Let A_1,â€¦,A_m âˆˆ^dÃ— d, Uâˆˆ^dÃ— k and bâˆˆ^m be given.

We define matrix Mâˆˆ^mÃ— dk as follows

    M_i,* :=    Â (U^âŠ¤ A_i), Â Â Â âˆ€ i âˆˆ [m].


The goal is to solve the following minimization problem.

    min_vâˆˆ^d kMv-b _2^2,



We can prove the following equivalence result


Let A_1,â€¦,A_m âˆˆ^dÃ— d, Uâˆˆ^dÃ— k and bâˆˆ^m be given.

If the following conditions hold

    
  * M_i,* :=  (U^âŠ¤ A_i), Â Â Â âˆ€ i âˆˆ [m].

  * The solution matrix V âˆˆ^d Ã— k can be reshaped through vector v âˆˆ^dk, i.e., v = (V^âŠ¤).



Then, the problem (defined in DefinitionÂ <ref>) is equivalent to problem (defined in DefinitionÂ <ref>) .
 



Let X, Yâˆˆ^dÃ— d, we want to show that 

    [X^âŠ¤ Y] =    Â (X)^âŠ¤(Y).

Note that the RHS is essentially
âˆ‘_i âˆˆ [d]âˆ‘_j âˆˆ [d] X_i,jY_i,j, for the LHS, note that 

    (X^âŠ¤ Y)_j,j =    Â âˆ‘_iâˆˆ [d] X_i,j Y_i,j,

the trace is then sum over j. 

Thus, we have Eq.Â (<ref>). This means that for each iâˆˆ [d], 

    [A_i^âŠ¤ UV^âŠ¤]=(U^âŠ¤ A_i)^âŠ¤(V^âŠ¤).



Set Mâˆˆ^m Ã— dk be the matrix where each row is (U^âŠ¤ A_i), we see DefinitionÂ <ref> is equivalent to solve the regression problem as in the statement. This completes the proof.




 Â§.Â§ From Sensing Matrix to Regression Matrix




Let A_1,â€¦,A_m âˆˆ^dÃ— d, Uâˆˆ^dÃ— k . 
 We define matrix Mâˆˆ^mÃ— dk as follows

    M_i,* :=    Â (U^âŠ¤ A_i), Â Â Â âˆ€ i âˆˆ [m].



The naive implementation of computing M âˆˆ^m Ã— dk
takes m Â·(k,d,d) time.
Without using fast matrix multiplication, it is O(md^2k) time.


For each i âˆˆ [m], computing matrix U^âŠ¤âˆˆ^k Ã— d times A_i âˆˆ^d Ã— d takes (k,d,d) time. Thus, we complete the proof.



The batch implementation takes (k,dm,d) time. 
Without using fast matrix multiplication, it takes O(md^2 k) time.


We can stack all the A_i together, then we matrix multiplication. For example, we construct matrix A âˆˆ^d Ã— dm. Then computing U^âŠ¤ A takes (k,d,dm) time.

The above two approach only has difference when we use fast matrix multiplication.



 Â§.Â§ Our Fast Regression Solver

In this section, we provide the results of our fast regression solver. Our approach is basically as in <cit.>. For detailed analysis, we refer the readers to the SectionÂ 5 in <cit.>. 


Assume m = Î©(dk).  
There is an algorithm that runs in time

    O( m d^2 k + d^3 k^3 )

and outputs a v' such that 

    M v' - b _2 â‰¤ (1+Ïµ) min_v âˆˆ^dk M v - b_2




From ClaimÂ <ref>, writing down M âˆˆ^m Ã— dk takes O(md^2 k) time.


 

Using Fast regression resolver as <cit.>, the fast regression solver takes
 

    O( ( mÂ· dk  + (dk)^3 ) Â·log(Îº(M)/Ïµ) Â·log^2(n/Î´) )




In each iteration, our requires takes O( m d^2 k) time.


Finally, in order to run LemmaÂ <ref>, we need to argue that Îº(M) â‰¤(k,d,Îº(W_*)).


 This is true because Îº(U) â‰¤ O(Îº(W_*)) and condition number of random Gaussian matrices is bounded by (k,d).



Then applying LemmaÂ <ref>, we can bound Îº(M) in each iteration.



Eventually, we just run standard error analysis in <cit.>. Thus, we should get the desired speedup.

The reason we can drop the (dk)^3 is m â‰¥ dk^2.





 Â§.Â§ Straightforward Solver


Note that from sample complexity analysis, we know that m = Î©(dk).

Assume m = Î©(dk). 
The straightforward implementation of the regression problem (DefintionÂ <ref>) takes 

    O(md^2 k^2)

time.


The algorithm has two steps. From ClaimÂ <ref>, writing down M âˆˆ^m Ã— dk takes O(md^2 k) time.

The first step is writing down the matrix M âˆˆ^m Ã— dk.

The second step is solving regression, it needs to compute M^â€  b (where M^â€ âˆˆ^d k Ã— m )

    M^â€  b = ( M^âŠ¤ M )^-1 M b


this will take time

    (dk,m,dk) + (dk,dk,dk) 
    =    Â  m d^2k^2 + (dk)^3 
    
    =    Â  m d^2 k^2

the second step follows from m =Î©(dk)
.

Thus, the total time is

    m d^2 k + md^2 k^2 = O(m d^2k^2)






 Â§.Â§ Condition Number





We define B âˆˆ^m Ã— k as follows B := X U and X âˆˆ^m Ã— d and U âˆˆ^d Ã— k.

Then, we can rewrite M âˆˆ^m Ã— dk
 

    M_m Ã— dk = B_m Ã— kâŠ—Y_m Ã— d


Then, we know that Îº(M) = Îº(B) Â·Îº(Y) â‰¤Îº(U) Îº(X) Îº(Y).
 




Recall U âˆˆ^d Ã— k. Then we define b_i = U^âŠ¤ x_i for each i âˆˆ [m].

Then we have

    M_i,* = ( U^âŠ¤ x_i y_i^âŠ¤ ) = (b_i y_i^âŠ¤ ).


Thus, it implies 

    M = B âŠ— Y
 

 





alpha



alpha



































