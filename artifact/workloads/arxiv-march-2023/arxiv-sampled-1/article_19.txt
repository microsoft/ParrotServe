
	
	
	
	
   



    


Provable Convergence of Tensor Decomposition-Based Neural Network Training
    Chenyang Li, Bo Shen*

Department of Mechanical and Industrial Engineering, New Jersey Institute of Technology
Corresponding Author: mailto:bo.shen@njit.edubo.shen@njit.edu  
    

==================================================================================================================================================================================

firstpage



    Abstract


Advanced tensor decomposition, such as tensor train (TT), has been widely studied for tensor decomposition-based neural network (NN) training, which is one of the most common model compression methods. However, training NN with tensor decomposition always suffers significant accuracy loss and convergence issues. In this paper, a holistic framework is proposed for tensor decomposition-based NN training by formulating TT decomposition-based NN training as a nonconvex optimization problem. This problem can be solved by the proposed tensor block coordinate descent (tenBCD) method, which is a gradient-free algorithm. The global convergence of tenBCD to a critical point at a rate of ğ’ª(1/k) is established with the Kurdyka Åojasiewicz (KÅ) property, where k is the number of iterations. The theoretical results can be extended to the popular residual neural networks (ResNets). The effectiveness and efficiency of our proposed framework are verified through an image classification dataset, where our proposed method can converge efficiently in training and prevent overfitting. 
 



Â§ KEYWORDS
Model Compression, Tensor Train Decomposition, Global Convergence, Gradient-free Training.





Â§ INTRODUCTION
 

Neural network (NN) has revolutionized many facets of our modern society, such as image classificationÂ <cit.>, object detectionÂ <cit.>, speech recognitionÂ <cit.>, etc. These advances have become possible because of algorithmic advances, large amounts of available data, and modern hardware. Despite their widespread success and popularity, there still remains a significant challenge in executing NNs with many parameters on edge devices. For most embedded and Internet-of-Things (IoT) systems, the sizes of many state-of-the-art NN models are too large, thereby causing high storage and computational demands and severely hindering the practical deployment of NNs. For example, wearable robotsÂ <cit.>, such as exoskeletons, typically have limited processing power, memory, storage, and energy supply due to their small size and portability. In addition, these wearable devices rely on wireless communication with remote servers, as larger models would require more bandwidth and higher latency, leading to slower and less reliable performance. 

To address this issue, numerous model compression techniques are proposed in the literature, which can be summarized into the following categories. (1) PruningÂ <cit.>: this technique involves removing unnecessary connections or neurons from a pre-trained model.  This can result in a smaller network with similar performance. (2) QuantizationÂ <cit.>: this involves reducing the number of bits required to represent the weights and activations in a neural network. For example, weights and activations may be represented using 8-bit integers instead of 32-bit floating-point numbers. (3) Structured sparsity <cit.>: this involves imposing a structured sparsity pattern on the weights of a model, such as by sparsifying entire rows or columns of weight matrices. (4) Knowledge distillationÂ <cit.>: this involves training a smaller model to mimic the behavior of a larger, more complex model, using the outputs of the larger model as labels. (5) Low-rank approximationÂ <cit.>: this technique involves approximating the weight matrices/tensors of a deep learning model with low-rank matrices/tensors.

Among all model compression methods, low-rank approximation, especially tensor decompositionÂ <cit.>, is an extremely attractive NN model compression technique since it can reduce the number of parameters in a model while maintaining a high level of accuracy. Specifically, tensor decomposition is a mathematical tool that explores the low tensor rank characteristics of large-scale tensor data, which stands out by offering an ultra-high compression ratio. By utilizing advanced tensor decomposition techniques like tensor train (TT)Â <cit.>, it is possible to achieve more than a 1,000Ã— reduction in parameters for the input-to-hidden layers of neural network modelsÂ <cit.>. Moreover, these compression methods can also enhance the classification accuracy in video recognition tasks significantly. Given such impressive compression performance, there has been a surge of interest in exploring the potential of tensor decomposition-based neural network models in prior research effortsÂ <cit.>. Due to the benefits brought by the TT-based NN models, several TT-based NN hardware accelerators have been developed and implemented in different chip formats including digital CMOS ASICÂ <cit.>, memristor ASICÂ <cit.> and IoT boardÂ <cit.>.

Although tensor decomposition shows strong compression performance, the training of tensor decomposition-based NN is a quite challenging taskÂ <cit.> because it involves tensor decomposition in NN training. In general, there are two ways to use tensor decomposition to obtain a compressed model: (1) Train from scratch in the decomposed format, and (2) Decompose a pre-trained uncompressed model and then retrain. In the first case, when the required tensor decomposition-based, e.g. TT-format model, is directly trained from scratch because the structure of the models is already pre-set to low tensor rank format before the training, the corresponding model capacity is typically limited as compared to the full-rank structure, thereby causing the training process being very sensitive to initialization and more challenging to achieve high accuracy. In the latter scenario, though the pre-trained uncompressed model provides a good initialization position, the straightforwardly decomposing full-rank uncompressed model into low tensor rank format causes inevitable and non-negligible approximation error, which is still very difficult to be recovered even after a long-time re-training period. 

No matter which training strategy with tensor decomposition is adopted, the training of NN heavily relies on gradient-based methods, which make use of backpropagationÂ <cit.> to compute gradients of network parameters. These gradient-based methods are based on the Stochastic Gradient Descent (SGD) methodÂ <cit.>. In recent years, a considerable amount of research has been dedicated to developing adaptive versions of the vanilla SGD algorithm. These adaptive variants include AdaGradÂ <cit.>, RMSPropÂ <cit.>, AdamÂ <cit.>, and AMSGradÂ <cit.>.  Despite the great success of these gradient-based methods, tensor decomposition always brings a linear increase in network depth, which implies training the tensor decomposition format NNs are typically more prone to the gradient vanishing problemÂ <cit.> and hence being difficult to be trained well. 


This paper aims to address the current limitations and fully unlock the potential of tensor decomposition-based NN training. To achieve this objective, a holistic framework for tensor decomposition-based NN training is proposed, which formulates tensor train decomposition-based NN training as a nonconvex optimization problem. This problem can be solved by the proposed tensor block coordinate descent (tenBCD) methods.   BCD is a gradient-free method that has been recently adapted to NN trainingÂ <cit.>. The main reasons for the surge of attention of BCD algorithms are twofold. One reason is that they are gradient-free, and thus are able to deal with non-differentiable nonlinearities and potentially avoid the vanishing gradient issue. The other reason is that BCD can be easily implemented in a distributed and parallel manner, therefore in favor of distributed/federated scenarios. To summarize, the contributions of this paper are as follows:
 

    
  * A holistic framework is proposed for  tensor decomposition-based NN training, which involves a highly nonconvex optimization problem.
    
  * An efficient tensor BCD (tenBCD) algorithm is implemented to solve the proposed optimization problem;
    
  * Convergence of the iterative sequence generated by the tenBCD algorithm is analyzed, which is proved to be globally convergent to a critical point at a rate of ğ’ª(1/k).




Â§ BACKGROUND AND PRELIMINARIES


In SectionÂ <ref>,   the notation and basics of multi-linear/tensor algebra used in this paper are reviewed. Then, tensor train decompositionÂ <cit.> is reviewed briefly in SectionÂ <ref>. Afterward, the tensor train fully-connected layerÂ <cit.> is reviewed in SectionÂ <ref>.


 Â§.Â§ Notation and Tensor Basis
 

Throughout this paper, scalars are denoted by lowercase letters, e.g., x; vectors are denoted by lowercase boldface letters, e.g., x; matrices are denoted by uppercase boldface, e.g., X; and tensors are denoted by calligraphic letters, e.g., X. The order of a tensor is the number of its modes or dimensions. A real-valued tensor of order-d is denoted by ğ’³âˆˆâ„^n_1Ã— n_2Ã—â‹¯Ã— n_d and its entries by  ğ’³(i_1, â‹¯, i_d). The inner product of two same-sized tensors ğ’³ and ğ’´ is the sum of the products of their entries, namely,  âŸ¨ğ’³,ğ’´âŸ© =âˆ‘_i_1 â‹¯âˆ‘_i_d X (i_1,â€¦ ,i_d) Â·ğ’´(i_1,â€¦ ,i_d). Following  the definition of inner product, the Frobenius norm of a tensor ğ’³ is defined as ğ’³_F=âˆš(âŸ¨ğ’³,ğ’³âŸ©). 
 



 Â§.Â§ Tensor Train (TT) Decomposition
 

 Given a tensor ğ’œâˆˆâ„^n_1Ã— n_2Ã—â‹¯Ã— n_d, it can be decomposed to a sort of 3-order tensors via Tensor Train Decomposition (TTD)Â <cit.> as follows:

    ğ’œ(i_1, i_2, â‹¯, i_d)     = ğ’¢_1(:, i_1,:) ğ’¢_2(:, i_2,:) â‹¯ğ’¢_d(:, i_d,:)  
        =âˆ‘_Î±_0, Î±_1â‹¯Î±_d^r_0, r_1, â‹¯ r_dğ’¢_1(Î±_0, i_1, Î±_1) ğ’¢_2(Î±_1, i_2, Î±_2) â‹¯ğ’¢_d(Î±_d-1, i_d, Î±_d),
 where ğ’¢_kâˆˆâ„^r_k-1Ã— n_kÃ— r_k are called TT-cores for k= 1,2, â‹¯, d, and r=[r_0, r_1, â‹¯, r_d], r_0=r_d=1 are called TT-ranks, which determine the storage complexity of TT-format tensor.  The representation of  ğ’œ via the explicit enumeration of all its entries requires storing Î _k=1^d n_k numbers compared with âˆ‘_k=1^d n_k r_k-1 r_k numbers if the tensor is stored in TT-format. 



 Â§.Â§ Tensor Train Fully-Connected Layer


Consider a simple fully-connected layer with weight matrix Wâˆˆâ„^M Ã— N and input xâˆˆâ„^N, where M=âˆ_k=1^d m_k and N=âˆ_k=1^d n_k, the output yâˆˆâ„^M is obtained by y=Wx. In order to transform this standard layer to TT fully-connected (TT-FC) layer,  the weight matrix W is first  tensorized to a d-order weight tensor ğ’²âˆˆâ„^(m_1Ã— n_1) Ã—â‹¯Ã—(m_dÃ— n_d) by reshaping and order transposing. Then ğ’² can be decomposed to TT-format:

    ğ’²((i_1, j_1), â‹¯,(i_d, j_d))=ğ’¢_1(:, i_1, j_1,:) â‹¯ğ’¢_d(:, i_d, j_d,:)
  
Here, each TT-core ğ’¢_kâˆˆâ„^r_k-1Ã— m_kÃ— n_kÃ— r_k is a 4-order tensor, which is one dimension more than the standard oneÂ (<ref>) since the output and input dimensions of W are divided separately. Hence, the forward propagation on the TT-FC layer can be expressed in the tensor format as follows (the bias term is ignored here):

    ğ’´(i_1, â‹¯, i_d)=âˆ‘_j_1, â‹¯, j_dğ’¢_1(:, i_1, j_1,:) â‹¯ğ’¢_d(:, i_d, j_d,:) ğ’³(j_1, â‹¯, j_d)

where ğ’³âˆˆâ„^m_1Ã—â‹¯Ã— m_d and ğ’´âˆˆâ„^n_1Ã—â‹¯Ã— n_d are the tensorized input and output corresponding to x and y, respectively. The details about the TT-FC layer are introduced inÂ <cit.>. As the TT-FC layer and the corresponding forward propagation schemes are formulated, standard stochastic gradient descent (SGD) algorithm can be used to update the TT-cores with the rank set r, which determines the target compression ratio. The initialization of the TT-cores can be either
randomly set or obtained from directly TT-decomposing a
pre-trained uncompressed model.




Â§ PROPOSED METHODOLOGY
 

Consider N-layer feedforward neural networks with N-1 hidden layers of the neural networks. Particularly, let n_i âˆˆâ„• be the number of hidden units in the i-th hidden layer for i=1, â€¦, N-1. Let n_0 and n_N be the number of units of input and output layers, respectively. Let W_i âˆˆâ„^n_i Ã— n_i-1 be the weight matrix between the (i-1)-th layer and the i-th layer for any i= 1, â€¦, N. Let ğ’µ:={(x_j, y_j)}_j=1^n âŠ‚â„^n_0Ã—â„^n_N be n samples, where y_j's are the one-hot vectors of labels. Denote ğ’²{W_i}_i=1^N, X:=(x_1, x_2, â€¦, x_n) âˆˆâ„^n_0 Ã— n and Y:=(y_1, y_2, â€¦, y_n) âˆˆâ„^n_N Ã— n. 




 Â§.Â§ Problem Formulation
 

As shown in FigureÂ <ref>, the weight in i-th layer, namely, W_i, can be transformed into a tensor ğ’²_i. The tensor can be further decomposed into TT-format.   Therefore, tensor train decomposition-based NN training problem can be formulated as the following empirical risk (i.e., training loss) minimization:

    min_ğ’²â„›_n(Î¦(X ; ğ’²), Y),   subject to ğ’²_i = TTD(r_i)   i=1, â€¦, N

where â„›_n(Î¦(X ; ğ’²), Y):=1/nâˆ‘_j=1^n â„“(Î¦(x_j ; ğ’²), y_j) with loss function â„“: â„^n_NÃ—â„^n_Nâ†’â„_+âˆª{0}, Î¦(x_j ; ğ’²)=Ïƒ_N(W_N Ïƒ_N-1(W_N-1â‹¯W_2 Ïƒ_1(W_1 x_j))) is the neural network model with N layers. TTD(r_i) is the tensor train decomposition with rank r_i inÂ (<ref>) for weight tensor ğ’²_i  and Ïƒ_i is the activation function of the i-th layer (generally, Ïƒ_Nâ‰¡Id is the identity function). 



Note that the NN training modelÂ (<ref>) is highly nonconvex as the variables are coupled via the NN architecture, which brings many challenges for the design of efficient training algorithms and also its theoretical analysis. To make ProblemÂ (<ref>) more computationally tractable, variable splitting is one of the most commonly used waysÂ <cit.>. The main idea of variable splitting is to transform a complicated problem (where the variables are coupled nonlinearly) into a relatively simpler one (where the variables are coupled much looser) by introducing some additional variables.








Considering general NN architectures, the regularized NN training model is applied here, which can reduce the original NN training modelÂ (<ref>). Specifically, the variable splitting model is: 
    min _ğ’², ğ’±â„’_0(ğ’², ğ’±)     :=â„›_n(V_N ; Y)+âˆ‘_i=1^NÏ„_i(W_i)+âˆ‘_i=1^N s_i(V_i) 
     subject to   U_i   =W_iV_i-1,V_i =Ïƒ_i(U_i), ğ’²_i = TTD(r_i)   i=1, â€¦, N,

where â„›_n(V_N ; Y):=1/nâˆ‘_j=1^nâ„“((V_N)_: j, y_j) denotes the empirical risk, ğ’±:={V_i}_i=1^N,(V_N)_: j is the j-th column of V_N. In addition, Ï„_i and s_i are extended-real-valued, nonnegative functions revealing the priors of the weight variable W_i and the state variable V_i (or the constraints on W_i and V_i ) for each i=1, â€¦ N, and define V_0:=X. To solve the formulation inÂ (<ref>), the following alternative minimization problem was considered:

    min_ğ’², ğ’±,ğ’°,ğ’¢   â„’(ğ’², ğ’±,ğ’°,ğ’¢):=  â„’_0(ğ’², ğ’±)+Î³/2âˆ‘_i=1^NV_i-Ïƒ_i(U_i)_F^2
       +Ï/2âˆ‘_i=1^NU_i-W_iV_i-1_F^2+ Ï„/2âˆ‘_i=1^Nğ’²_i - TTD(r_i)_F^2
 
where Î³,Ï,Ï„>0 are hyperparameters for different regularization terms, ğ’°:={U_i}_i=1^N, and ğ’¢:={ğ’¢_i}_i=1^N is the set of TT-cores ğ’¢_i from i-th layer. The NN training modelÂ (<ref>) can be very general, where: (a) â„“ can be the squared, logistic, hinge, cross-entropy or other commonly used loss functions; (b) Ïƒ_i can be ReLU, leaky ReLU, sigmoid, linear, polynomial, softplus or other commonly used activation functions; (c) Ï„_i can be the squared â„“_2 norm, the â„“_1 norm, the elastic net, the indicator function of some nonempty closed convex set (such as the nonnegative closed half-space or a closed interval [0,1]); (d) s_i can be the â„“_1 norm, the indicator function of some convex set with simple projection. Particularly, if there is no regularizer or constraint on W_i (or V_i), then Ï„_i (or s_i) can be zero. The network architectures considered in this paper exhibit generality to various types of NNs, including but not limited to the fully (or sparse) connected MLPs, convolutional neural networks (CNN) and residual neural networks (ResNets)Â <cit.>. 

 As mentioned before, an existing TT-format NN is either 1) trained from randomly initialized tensor cores; or 2) trained from a direct decomposition of a pre-trained model. For the first strategy, it does not utilize any information related to the high-accuracy uncompressed model; while other model compression methods, e.g. pruning and knowledge distillation, have shown that proper utilization of the pre-trained models is very critical for NN compression. For the second strategy, though the knowledge of the pre-trained model is indeed utilized, because the pre-trained model generally lacks low TT-rank property, after direct low-rank tensor decomposition the approximation error is too significant to be properly recovered even using long-time re-training. Such inherent limitations of the existing training strategies, consequently, cause significant accuracy loss for the compressed TT-format NN models. To overcome these limitations, it is to maximally retain the knowledge contained in the uncompressed model, or in other words, minimize the approximation error after tensor decomposition with given target tensor ranks. In our formulationÂ (<ref>), â„’_0(ğ’², ğ’±) is the loss function of the uncompressed model while the regularization term ğ’²_i - TTD(r_i)_F^2 can encourage the uncompressed DNN models to gradually exhibit low tensor rank properties.   



















      



     







 Â§.Â§ Tensor BCD Algorithms
 

Note that (<ref>) is a nonconvex optimization problem with multi-block variables. BCD  is a Gauss-Seidel type method for a minimization problem with multi-block variables to update all the variables cyclically while fixing the remaining blocks at their last updated valuesÂ <cit.>.  A tensor BCD (tenBCD) algorithm is developed for solving (<ref>). In this paper, proximal terms are added to some sub-problems arising from the tenBCD algorithm for two major reasons: (1) To practically stabilize the training process; (2) To yield the desired â€œsufficient descreaseâ€ property for theoretical justification. At each iteration k, the tenBCD method with the backward order is considered for the updates of variables, i.e., the variables are updated from the output layer (layer N) to the input layer (layer 1). For each layer, the variables {V_i, U_i, W_i,ğ’¢_i} are updated cyclically for ProblemÂ (<ref>). Since Ïƒ_Nâ‰¡Id, the output layer is paid special attention.  The tenBCD algorithms for (<ref>) can be summarized in AlgorithmÂ <ref>. 



  Â§.Â§.Â§ Optimization over V_i

At iteration k, V_N can be updated through the following optimization problem 

    V_N^k=argmin_V_N{s_N(V_N)+â„›_n(V_N ; Y)+Î³/2V_N-U_N^k-1_F^2+Î±/2V_N-V_N^k-1_F^2},

where s_N(V_N)+â„›_n(V_N ; Y) is regarded as a new proximal function sÌƒ_N(V_N).  When i<N, V_i can be updated through the following optimization problem 

    V_i^k=argmin_V_i{s_i(V_i)+Î³/2V_i-Ïƒ_i(U_i^k-1)_F^2+Ï/2U_i+1^k-W_i+1^kV_i_F^2 }.

For subproblemÂ (<ref>), Î±/2V_N-V_N^k-1_F^2 is the proximal term, where Î±>0 is the positive coefficient. 

The above two problemsÂ (<ref>) andÂ (<ref>) are simple proximal updatesÂ <cit.> (or just least squares problems), which usually have closed-form solutions to many commonly used NNs. For V_N^k-update,  s_N(V_N)+â„›_n(V_N ; Y) is regarded as a new proximal function sÌƒ_N(V_N). Some typical examples leading to the closed-form solutions include: (a) s_i are 0 (i.e., no regularization), or the squared â„“_2 norm, or the indicator function of a nonempty closed convex set with a simple projection like the nonnegative closed half-space and the closed interval [0,1]; (b) the loss function â„“ is the squared loss or hinge loss.[The V_N-update with hinge loss and other smooth losses is provided in AppendixÂ <ref>.]



  Â§.Â§.Â§ Optimization over U_i

At iteration k, U_N can be updated through the following optimization problem 

    U_N^k=argmin_U_N{Î³/2V_N^k-U_N_F^2+Ï/2U_N-W_N^k-1V_N-1^k-1_F^2 }

U_i,i<N can be updated through the following optimization problem 

    U_i^k=argmin_U_i{Î³/2V_i^k-Ïƒ_i(U_i)_F^2+Ï/2U_i-W_i^k-1V_i-1^k-1_F^2  +Î±/2U_i-U_i^k-1_F^2}

For subproblemÂ (<ref>), Î±/2U_i-U_i^k-1_F^2 is the proximal term. The subproblemÂ (<ref>) is a least-square optimization where the closed-form solution can be derived. The subproblemÂ (<ref>)  is a nonlinear and nonsmooth optimization where  Ïƒ_i is ReLU or leaky ReLU. Accordingly,   the closed-form solution to solve the subproblemÂ (<ref>) is provided in AppendixÂ <ref>.



  Â§.Â§.Â§ Optimization over W_i

At iteration k, W_i,i=1,â€¦,N can be updated through the following optimization problem 

    W_i^k=argmin_W_i{Ï„_i(W_i)+Ï/2U_i^k-W_i V_i-1^k-1_F^2+Ï„/2ğ’²_i - TTD(r_i)_F^2},

 The closed-form solution to solve the above optimization problem can be obtained  when Ï„_i is 0 (i.e., no regularization), or the squared â„“_2 norm (i.e., weight decay), or the indicator function of a nonempty closed convex set with a simple projection like the nonnegative closed half-space and the closed interval [0,1].



  Â§.Â§.Â§ Optimization over ğ’¢_i

At iteration k, ğ’¢_i,i=1,â€¦,N can be updated through the following optimization problem

    ğ’¢_i^k = argmin_ğ’¢_i{Ï„/2ğ’²_i^k - TTD(r_i)_F^2 +Î±/2ğ’¢_i-ğ’¢_i^k-1_F^2}
  where Î±/2ğ’¢_i-ğ’¢_i^k-1_F^2 is the proximal terms. 
This subproblem is implemented in TensorLy packageÂ <cit.>.





















      



     










 Â§.Â§ Global Convergence Analysis of tenBCD


In this section, the global convergence of AlgorithmÂ <ref> for ProblemÂ (<ref>) is established. Firstly, let h: â„^pâ†’â„âˆª{+âˆ} be an extended-real-valued function, its graph is defined by
Graph(h) :={(x, y) âˆˆâ„^pÃ—â„: y=h(x)}, and its domain by dom(h):={xâˆˆâ„^p: h(x)<+âˆ}. The subdifferential of a function is defined as follows. 



Â Assume that f:Â â„^p â†’ (-âˆ,+âˆ) is a proper and lower semicontinuous function. 
  

	
  * The domain of f is defined and denoted by domf{xâˆˆâ„^p:f(x)<+âˆ}
	
  * For a given xâˆˆdomf, the FrÃ©chet subdifferential of f at x, written âˆ‚Ì‚f(x), is the set of all vectors uâˆˆâ„^p that satisfy

    lim_yâ‰ xinf_yâ†’xf(y)-f(x) - âŸ¨u , y-xâŸ©/y-xâ‰¥ 0.


  * The limiting-subdifferential, or simply the subdifferential, of f at x, written âˆ‚ f(x) is defined through the following closure process

    âˆ‚ f(x):={uâˆˆâ„^p: âˆƒx^k â†’x,f(x^k) â†’ f(x)  and u^k âˆˆâˆ‚Ì‚f(x^k) â†’u as  kâ†’âˆ}.

	

Now, our first main lemma about the sufficient decrease property of the iterative sequence  {ğ’«^k:=({W_i^k}_i=1^N,{V_i^k}_i=1^N,{U_i^k}_i=1^N),{ğ’¢_i^k}_i=1^N}_k âˆˆâ„• from AlgorithmÂ <ref> is ready to be introduced.

 
Given that Î±,Î³,Ï,Ï„>0, {ğ’«^k}_k âˆˆâ„• is the sequence generated by the tenBCD algorithmÂ <ref>, then the sequence satisfies

    â„’(ğ’«^k) â‰¤â„’(ğ’«^k-1)-Î»ğ’«^k-ğ’«^k-1_F^2.

For the case that V_N is updated via the proximal strategy,  Î»:=min{Î±/2, Î³+Ï/2,Ï„/2}. For the case that V_N is update via the prox-linear strategy, Î»:=min{Î±/2, Î³+Ï/2,Ï„/2, Î±+Î³-L_R/2}, where âˆ‡â„›_n is Lipschitz continuous with a Lipschitz constant L_R and Î±>max{0, L_R-Î³/2}.




The inequalityÂ (<ref>) can be developed by considering the descent quantity along the update of each block variable, i.e., {V_i}_i=1^N, {U_i}_i=1^N, {W_i}_i=1^N, and {ğ’¢_i}_i=1^N. To begin with, the following notations are introduced. Specifically, 
W_<i:=(W_1, W_2, â€¦, W_i-1), W_>i:=(W_i+1, W_i+1, â€¦, W_N), and V_<i, V_>i, U_<i, U_>i,ğ’¢_<i,ğ’¢_>i are defined similarly. We will consider each case separately. 



  Â§.Â§.Â§ Optimization over V_i

V_N^k-block: at iteration k,  there are two ways to update the variable: (1) proximal update with closed-form solution: the following inequality can be derived

    â„’({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k,{U_i^k-1}_i=1^N, {ğ’¢_i^k-1}_i=1^N)
    â‰¤   â„’({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k-1,{U_i^k-1}_i=1^N, {ğ’¢_i^k-1}_i=1^N)-Î±/2V_N^k-V_N^k-1_F^2.
 
The above inequalityÂ (<ref>) is due to the fact that V_N^k is the optimal solution for subproblemÂ (<ref>). (2) proximal-linear case: let h^k(V_N):=s_N(V_N)+â„›_n(V_N ; Y)+Î³/2V_N-U_N^k-1_F^2 and hÌ…^k(V_N):=s_N(V_N)+â„›_n(V_N^k-1 ; Y)+âŸ¨âˆ‡â„›_n(V_N^k-1 ; Y), V_N-V_N^k-1âŸ©+Î±/2V_N-V_N^k-1_F^2 +Î³/2V_N-U_N^k-1_F^2. By the optimality of V_N^k and the strong convexity[The function h is called a strongly convex function with parameter Î³>0 if h(u) â‰¥ h(v)+âŸ¨âˆ‡ h(v), u-vâŸ©+Î³/2u-v^2.] of hÌ…^k(V_N) with modulus at least Î± +Î³, the following holds

    hÌ…^k(V_N^k) â‰¤hÌ…^k(V_N^k-1) -Î±+Î³/2V_N^k-V_N^k-1_F^2,

which implies 


    h^k(V_N^k)    â‰¤  h^k(V_N^k-1) + â„›_n(V_N^k ; Y)-â„›_n(V_N^k-1 ; Y)-âŸ¨âˆ‡â„›_n(V_N^k-1 ; Y), V_N^k-V_N^k-1âŸ©
       -(Î±+Î³/2)V_N^k-V_N^k-1_F^2
       â‰¤ h^k(V_N^k-1)-(Î±+Î³-L_R/2)V_N^k-V_N^k-1_F^2,


where inequalityÂ (<ref>) is due to the inequalityÂ (<ref>),  the relationship between h^k(V_N^k-1) and hÌ…^k(V_N^k-1),  and the relationship between h^k(V_N^k) and hÌ…^k(V_N^k). The inequalityÂ (<ref>) holds for the L_R-Lipschitz continuity of âˆ‡â„›_n, i.e., the following inequality byÂ <cit.>

    â„›_n(V_N^k ; Y) â‰¤â„›_n(V_N^k-1 ; Y)+âŸ¨âˆ‡â„›_n(V_N^k-1 ; Y), V_N^k-V_N^k-1âŸ©+L_R/2V_N^k-V_N^k-1_F^2.

According to  the relationship between h^k(V_N) and â„’({W_i^k-1}_i=1^N, V_i<N^k-1, V_N,{U_i^k-1}_i=1^N, {ğ’¢_i^k-1}_i=1^N), and the inequalityÂ (<ref>),

    â„’({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k,{U_i^k-1}_i=1^N, {ğ’¢_i^k-1}_i=1^N)
    â‰¤   â„’({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k-1,{U_i^k-1}_i=1^N, {ğ’¢_i^k-1}_i=1^N)-(Î±+Î³-L_R/2)V_N^k-V_N^k-1_F^2.
 


V_i^k-block (i<N): V_i^k is updated according to the following

    V_i^kâ†V_iargmin{s_i(V_i)+Î³/2V_i-Ïƒ_i(U_i^k-1)_F^2+Ï/2U_i+1^k-W_i+1^kV_i_F^2}.

Let h^k(V_i)=s_i(V_i)+Î³/2V_i-Ïƒ_i(U_i^k-1)_F^2+Ï/2U_i+1^k-W_i+1^kV_i_F^2. By the convexity of s_i, the function h^k(V_i) is a strongly convex function with modulus no less than Î³. By the optimality of V_i^k, the following holds

    h^k(V_i^k) â‰¤ h^k(V_i^k-1) - Î³/2V_i^k-V_i^k-1_F^2.

Based on the inequalityÂ (<ref>), it yields for 

    â„’(W_â‰¤ i^k-1, W_>i^k, V_<i^k-1, V_i^k, V_>i^k, U_â‰¤ i^k-1, U_>i^k, ğ’¢_â‰¤ i^k-1, ğ’¢_>i^k)
    â‰¤   â„’(W_â‰¤ i^k-1, W_>i^k, V_<i^k-1, V_i^k-1, V_>i^k, U_â‰¤ i^k-1, U_>i^k, ğ’¢_â‰¤ i^k-1, ğ’¢_>i^k) - Î³/2V_i^k-V_i^k-1_F^2

for i=1, â€¦, N-1, where 

    h^k(V_i^k) -  h^k(V_i^k-1)     = â„’(W_â‰¤ i^k-1, W_>i^k, V_<i^k-1, V_i^k, V_>i^k, U_â‰¤ i^k-1, U_>i^k, ğ’¢_â‰¤ i^k-1, ğ’¢_>i^k) 
        - â„’(W_â‰¤ i^k-1, W_>i^k, V_<i^k-1, V_i^k-1, V_>i^k, U_â‰¤ i^k-1, U_>i^k, ğ’¢_â‰¤ i^k-1, ğ’¢_>i^k).




  Â§.Â§.Â§ Optimization over U_i

U_N^k-block: similar to the inequalityÂ (<ref>), the  descent quantity is established as follows

    â„’(W_â‰¤ N^k-1, V_<N^k-1, V_N^k, U_<N^k-1, U_N^k, ğ’¢_â‰¤ N^k-1) 
    â‰¤   â„’(W_â‰¤ N^k-1, V_<N^k-1, V_N^k, U_<N^k-1, U_N^k-1, ğ’¢_â‰¤ N^k-1) - Î³+Ï/2U_N^k-U_N^k-1_F^2,

where the above inequality is because the objective function in subproblemÂ (<ref>) is a strongly convex function with modulus at least Î³+Ï.

U_i^k-block (i<N): the following can be obtained

    â„’(W_â‰¤ i^k-1,  W_>i^k, V_<i^k-1, V_â‰¥ i^k, U_<i^k-1, U_i^k, U_>i^k,ğ’¢_â‰¤ i^k-1,  ğ’¢_>i^k) 
    â‰¤   â„’(W_â‰¤ i^k-1,  W_>i^k, V_<i^k-1, V_â‰¥ i^k, U_<i^k-1, U_i^k-1, U_>i^k,ğ’¢_â‰¤ i^k-1,  ğ’¢_>i^k) - Î±/2U_i^k-U_i^k-1_F^2

for i=1, â€¦, N-1 since U_i^k is the optimal solution for subproblemÂ (<ref>).






  Â§.Â§.Â§ Optimization over W_i

W_i^k-block (iâ‰¤ N): W_i^k is updated according to the following

    W_i^kâ†’W_iargmin{r_i(W_i)+Ï/2U_i^k-W_iV_i-1^k-1_F^2+Ï„/2ğ’²_i - TTD(r_i)_F^2},

where h^k(W_i)=r_i(W_i)+Ï/2U_i^k-W_iV_i-1^k-1_F^2+Ï„/2ğ’²_i - TTD(r_i)_F^2 is  a strongly convex function with modulus at least Ï„. Accordingly, the following holds

    â„’(W_<i^k-1, W_i^k, W_>i^k, V_<i^k-1,  V_â‰¥ i^k, U_<i^k-1,  U_â‰¥ i^k, ğ’¢_â‰¤ i^k-1,  ğ’¢_> i^k) 
    â‰¤   â„’(W_<i^k-1, W_i^k-1, W_>i^k, V_<i^k-1,  V_â‰¥ i^k, U_<i^k-1,  U_â‰¥ i^k, ğ’¢_â‰¤ i^k-1,  ğ’¢_> i^k)-Ï„/2W_i^k-W_i^k-1_F^2,

which is due to the relationship between h^k(W_i) and â„’(W_<i^k-1, W_i, W_>i^k, V_<i^k-1,  V_â‰¥ i^k, U_<i^k-1,  U_â‰¥ i^k, ğ’¢_â‰¤ i^k-1,  ğ’¢_> i^k).





































  Â§.Â§.Â§ Optimization over ğ’¢_i

ğ’¢_i-block (iâ‰¤ N): the descent quantity for ğ’¢_i can be derived as follows

    â„’(W_<i^k-1, W_â‰¥ i^k, V_<i^k-1,  V_â‰¥ i^k, U_<i^k-1,  U_â‰¥ i^k, ğ’¢_< i^k-1,  ğ’¢_i^k, ğ’¢_> i^k) 
    â‰¤   â„’(W_<i^k-1, W_â‰¥ i^k, V_<i^k-1,  V_â‰¥ i^k, U_<i^k-1,  U_â‰¥ i^k, ğ’¢_< i^k-1,  ğ’¢_i^k-1, ğ’¢_> i^k)-Î±/2ğ’¢_i^k-ğ’¢_i^k-1_F^2,

where the above inequalityÂ (<ref>) is due to the fact that ğ’¢_i^k is the optimal solution for subproblemÂ (<ref>).

By summing up inequalitiesÂ (<ref>) (orÂ (<ref>)), (<ref>), (<ref>), (<ref>), andÂ (<ref>), it yields the

    â„’(ğ’«^k) â‰¤â„’(ğ’«^k-1)-Î»ğ’«^k-ğ’«^k-1_F^2,

where Î»:=min{Î±/2, Î³+Ï/2,Ï„/2} (or Î»:=min{Î±/2, Î³+Ï/2,Ï„/2, Î±+Î³-L_R/2}). 










From LemmaÂ <ref>, the Lagrangian sequence {â„’(ğ’«^k)}__k âˆˆâ„• is monotonically decreasing, and the descent quantity of each iterate can be lower bounded by the discrepancy between the current iterate and its previous iterate. This lemma is crucial for the global convergence of a nonconvex algorithm. It tells at least the following four important items: (i) {â„’(ğ’«^k)}_k âˆˆâ„• is convergent if â„’ is lower bounded; (ii) {ğ’«^k}_k âˆˆâ„• itself is bounded if â„’ is coercive and ğ’«^0 is finite; (iii) {ğ’«^k}_k âˆˆâ„• is square summable, i.e., âˆ‘_k=1^âˆğ’«^k-ğ’«^k-1_F^2<âˆ, implying its asymptotic regularity, i.e., ğ’«^k-ğ’«^k-1_Fâ†’ 0 as k â†’âˆ; and (iv) 1/Kâˆ‘_k=1^Kğ’«^k-ğ’«^k-1_F^2â†’ 0 at a rate of ğ’ª(1 / K). Leveraging LemmaÂ <ref>, we can establish the global convergence (i.e., the whole sequence convergence) of tenBCD algorithmÂ <ref> in NN training settings. In contrast, <cit.> only establish the subsequence convergence of SGD in NN training settings. Such a gap between the subsequence convergence of SGD in <cit.>  and the whole sequence convergence of tenBCD algorithmÂ <ref> in this paper exists mainly because SGD can only achieve the descent property but not the sufficient descent property.

It can be noted from LemmaÂ <ref> that neither multiconvexity and differentiability nor Lipschitz differentiability assumptions are imposed on the NN training models to yield this lemma, as required in the literature <cit.>. Instead, we mainly exploit the proximal strategy for all nonstrongly convex subproblems in AlgorithmÂ <ref> to establish this lemma.

 Our second main lemma is about the subgradient lower bound.  
 
    Under the same conditions of LemmaÂ <ref>, let â„¬ be an upper bound of ğ’«^k-1 and ğ’«^k for any positive integer k, L_â„¬ be a uniform Lipschitz constant of Ïƒ_i on the bounded set {ğ’«:ğ’«_Fâ‰¤â„¬}, and

    Î´:=max{Î³, Î±+Ïâ„¬, Î±+Î³ L_â„¬, 2 Ïâ„¬+ 2Ïâ„¬^2, Î± + âˆš(N)Ï„â„¬^N-1}

(or, for the prox-linear case, Î´:=max{Î³, L_R+Î±+Ïâ„¬, Î±+Î³ L_â„¬, 2 Ïâ„¬+ 2 Ïâ„¬^2, Î± + âˆš(N)Ï„â„¬^N-1}), then for any positive integer k, there holds,

    dist(0, âˆ‚â„’ (ğ’«^k))    â‰¤Î´âˆ‘_i=1^N[W_i^k-W_i^k-1_F+V_i^k-V_i^k-1_F+U_i^k-U_i^k-1_F+ğ’¢_i^k-ğ’¢_i^k-1_F] 
       â‰¤Î´Ì…ğ’«^k-ğ’«^k-1_F

where Î´Ì…:= Î´âˆš(4 N), dist(0, ğ’®):=inf _sâˆˆğ’®s_F for a set ğ’®, and

    âˆ‚â„’(ğ’«^k):=({âˆ‚_W_iâ„’}_i=1^N,{âˆ‚_V_iâ„’}_i=1^N,{âˆ‚_U_iâ„’}_i=1^N,{âˆ‚_ğ’¢_iâ„’}_i=1^N)(ğ’«^k).

 

    The inequalityÂ (<ref>) is established via bounding each term of âˆ‚â„’(ğ’«^k). Specifically, the following holds


    0âˆˆâˆ‚ s_N(V_N^k)+âˆ‚â„›_n(V_N^k ; Y)+Î³(V_N^k-U_N^k-1)+Î±(V_N^k-V_N^k-1), 
       0âˆˆâˆ‚ s_N(V_N^k)+âˆ‡â„›_n(V_N^k-1 ; Y)+Î³(V_N^k-U_N^k-1)+Î±(V_N^k-V_N^k-1),  (proximal-linear)
       0=Î³(U_N^k-V_N^k)+Ï(U_N^k-W_N^k-1V_N-1^k-1), 
       0âˆˆâˆ‚Ï„_N(W_N^k)+Ï(W_N^kV_N-1^k-1-U_N^k) V_N-1^k-1^âŠ¤+Ï„(W_N^k-TTD^k-1(r_N)), 
       0âˆˆâˆ‚(Ï„/2ğ’²_N^k - TTD^k(r_N)_F^2) +Î±(ğ’¢_N^k-ğ’¢_N^k-1),
 

where (<ref>), (<ref>), (<ref>),  (<ref>), and  (<ref>) are  due to the optimality conditions of all updates in (<ref>), (<ref>), (<ref>), (<ref>), and (<ref>), respectively.

For i=N-1, â€¦, 1, the following holds


    0âˆˆâˆ‚ s_i(V_i^k)+Î³(V_i^k-Ïƒ_i(U_i^k-1))+ÏW_i+1^k^âŠ¤(W_i+1^kV_i^k-U_i+1^k), 
       0âˆˆÎ³[(Ïƒ_i(U_i^k)-V_i^k) âŠ™âˆ‚Ïƒ_i(U_i^k)]+Ï(U_i^k-W_i^k-1V_i-1^k-1)+Î±(U_i^k-U_i^k-1), 
       0âˆˆâˆ‚Ï„_i(W_i^k)+Ï(W_i^kV_i-1^k-1-U_i^k) V_i-1^k-1^âŠ¤+Ï„(W_i^k-TTD^k-1(r_i)), 
       0âˆˆâˆ‚(Ï„/2ğ’²_i^k - TTD^k(r_i)_F^2) +Î±(ğ’¢_i^k-ğ’¢_i^k-1),
 

where (<ref>), (<ref>), (<ref>), and (<ref>) are due to the optimality conditions of all updates in (<ref>), (<ref>), (<ref>), and (<ref>), respectively. V_0^kâ‰¡V_0=X for all k, and âŠ™ is the Hadamard product. Through the above relationshipÂ (<ref>), we have

    -Î±(V_N^k-V_N^k-1)-Î³(U_N^k-U_N^k-1) âˆˆâˆ‚ s_N(V_N^k)+âˆ‚â„›_n(V_N^k ; Y)+Î³(V_N^k-U_N^k)=âˆ‚_V_Nâ„’(ğ’«^k), 
       (âˆ‡â„›_n(V_N^k ; Y)-âˆ‡â„›_n(V_N^k-1 ; Y))-Î±(V_N^k-V_N^k-1)-Î³(U_N^k-U_N^k-1) âˆˆâˆ‚_V_Nâ„’(ğ’«^k),  (proximal-linear)
        -Ï(W_N^k-W_N^k-1) V_N-1^k-ÏW_N^k-1(V_N-1^k-V_N-1^k-1)=Î³(U_N^k-V_N^k)+Ï(U_N^k-W_N^k V_N-1^k)=âˆ‚_U_Nâ„’(ğ’«^k), 
       ÏW_N^k[V_N-1^k(V_N-1^k-V_N-1^k-1)^âŠ¤+(V_N-1^k-V_N-1^k-1) V_N-1^k-1^âŠ¤]-ÏU_N^k(V_N^k-V_N^k-1)^âŠ¤+Ï„(TTD^k(r_N)-TTD^k-1(r_N)) 
       âˆˆâˆ‚ r_N(W_N^k)+Ï(W_N^k V_N-1^k-U_N^k) V_N-1^k^âŠ¤+Ï„(W_N^k-TTD^k(r_i))=âˆ‚_W_Nâ„’(ğ’«^k), 
        -Î±(ğ’¢_N^k-ğ’¢_N^k-1) âˆˆâˆ‚_ğ’¢_Nâ„’(ğ’«^k).

For i=N-1, â€¦, 1, the relationshipÂ (<ref>) implies

    -Î³(Ïƒ_i(U_i^k)-Ïƒ_i(U_i^k-1)) âˆˆâˆ‚ s_i(V_i^k)+Ï(V_i^k-Ïƒ_i(U_i^k))+Î³W_i+1^k^âŠ¤(W_i+1^k V_i^k-U_i+1^k)=âˆ‚_V_iâ„’(ğ’«^k), 
        -ÏW_i^k-1(V_i-1^k-V_i-1^k-1)-Ï(W_i^k-W_i^k-1) V_i-1^k-Î±(U_i^k-U_i^k-1) 
       âˆˆÎ³[(Ïƒ_i(U_i^k)-V_i^k) âŠ™âˆ‚Ïƒ_i(U_i^k)]+Ï(U_i^k-W_i^k V_i-1^k)=âˆ‚_U_iâ„’(ğ’«^k) , 
       ÏW_i^k[V_i-1^k(V_i-1^k-V_i-1^k-1)^âŠ¤+(V_i-1^k-V_i-1^k-1) V_i-1^k-1]-ÏU_i^k(V_i-1^k-V_i-1^k-1)^âŠ¤+Ï„(TTD^k(r_i)-TTD^k-1(r_i)) 
       âˆˆâˆ‚ r_i(W_i^k)+Ï(W_i^k V_i-1^k-U_i^k) V_i-1^k^âŠ¤=âˆ‚_W_iâ„’(ğ’«^k),
        -Î±(ğ’¢_i^k-ğ’¢_i^k-1) âˆˆâˆ‚_ğ’¢_iâ„’(ğ’«^k).

Based on the above relationships, and by the Lipschitz continuity of the activation function on the bounded set {ğ’«:ğ’«_Fâ‰¤â„¬} and the bounded assumption of both ğ’«^k-1 and ğ’«^k, we have

    [                            Î¾_V_N^k_Fâ‰¤Î±V_N^k-V_N^k-1_F+Î³U_N^k-U_N^k-1_F,                                                    Î¾_V_N^kâˆˆâˆ‚_V_Nâ„’(ğ’«^k),; (or Î¾_V_N^k_Fâ‰¤(L_R+Î±)V_N^k-V_N^k-1_F+Î³U_N^k-U_N^k-1_F)  proximal-linear                                                                        ;                      Î¾_U_N^k_Fâ‰¤Ïâ„¬W_N^k-W_N^k-1_F+Ïâ„¬V_N-1^k-V_N-1^k-1_F,                                                    Î¾_U_N^kâˆˆâˆ‚_U_Nâ„’(ğ’«^k),;                  Î¾_W_N^k_Fâ‰¤ 2 Ïâ„¬^2V_N-1^k-V_N-1^k-1_F+Ïâ„¬V_N^k-V_N^k-1_F                                                                        ;                                            +Ï„TTD^k(r_N)-TTD^k-1(r_N)_F,                                                   Î¾_W_N^k âˆˆâˆ‚_W_Nâ„’(ğ’«^k),;                                             Î¾_ğ’¢_N^k_Fâ‰¤Î±ğ’¢_N^k-ğ’¢_N^k-1_F,                                                   Î¾_ğ’¢_N^k âˆˆâˆ‚_ğ’¢_Nâ„’(ğ’«^k), ]

and for i=N-1, â€¦, 1,

    [                                     Î¾_V_i^k_Fâ‰¤Î³ L_â„¬U_i^k-U_i^k-1_F,                                                Î¾_V_i^kâˆˆâˆ‚_V_iâ„’(ğ’«^k),; Î¾_U_i^k_Fâ‰¤Ïâ„¬V_i-1^k-V_i-1^k-1_F+Ïâ„¬W_i^k-W_i^k-1_F+Î±U_i^k-U_i^k-1_F,                                                Î¾_U_i^kâˆˆâˆ‚_U_iâ„’(ğ’«^k),; Î¾_W_i^k_Fâ‰¤(2Ïâ„¬^2+Ïâ„¬)V_i-1^k-V_i-1^k-1_F+Ï„TTD^k(r_i)-TTD^k-1(r_i)_F,                                                ğ’¢_W_i^kâˆˆâˆ‚_W_iâ„’(ğ’«^k),;                                         Î¾_ğ’¢_i^k_Fâ‰¤Î±ğ’¢_i^k-ğ’¢_i^k-1_F,                                               Î¾_ğ’¢_i^k âˆˆâˆ‚_ğ’¢_iâ„’(ğ’«^k). ]

In addition, we have the following bound

    TTD^k(r_i)-TTD^k-1(r_i)_Fâ‰¤âˆš(N)â„¬^N-1ğ’¢_i^k-ğ’¢_i^k-1_F.

Summing the above inequalitiesÂ (<ref>),(<ref>), andÂ (<ref>), the subgradient lower boundÂ (<ref>) can be obtained for any positive integer k

    dist(0, âˆ‚â„’ (ğ’«^k))    â‰¤Î´âˆ‘_i=1^N[W_i^k-W_i^k-1_F+V_i^k-V_i^k-1_F+U_i^k-U_i^k-1_F+ğ’¢_i^k-ğ’¢_i^k-1_F] 
       â‰¤Î´Ì…ğ’«^k-ğ’«^k-1_F,

where

    Î´:=max{Î³, Î±+Ïâ„¬, Î±+Î³ L_â„¬, 2 Ïâ„¬+ 2Ïâ„¬^2, Î± + âˆš(N)Ï„â„¬^N-1},

(or, for the prox-linear case, Î´:=max{Î³, L_R+Î±+Ïâ„¬, Î±+Î³ L_â„¬, 2 Ïâ„¬+ 2 Ïâ„¬^2, Î± + âˆš(N)Ï„â„¬^N-1}). 


 Â A necessary condition for x to be a minimizer of a proper and lower semicontinuous (PLSC) function  f is that
 
    0âˆˆâˆ‚ f(x).

 A point that satisfies (<ref>) is called limiting-critical or simply critical.


 
Any iterative algorithm for solving an optimization problem over a set X, is said to be globally convergent if for any starting point x_0 âˆˆ X, the sequence generated by the algorithm always has an accumulation critical point.


To build the global convergence of our iterative sequence  {ğ’«^k}_k âˆˆâ„• from AlgorithmÂ <ref>, the function â„’(ğ’², ğ’±,ğ’°,ğ’¢) needs to have the Kurdyka Åojasiewicz (KÅ)  property as follows

A real function f: â„^p â†’ (-âˆ,+âˆ] has the  Kurdyka Åojasiewicz (KÅ) property, namely, for any point uÌ…âˆˆâ„^p, in a neighborhood N(uÌ…,Ïƒ), there exists a desingularizing function Ï•(s)=cs^1-Î¸ for some c>0 and Î¸âˆˆ [0,1) such that 

    Ï•'(|f(u)-f(uÌ…)|)d(0,âˆ‚ f(u))â‰¥ 1

for any  uâˆˆ N(uÌ…,Ïƒ) and  f(u)â‰  f(uÌ…).
    
The real analytic and semi-algebraic functions, which are related to KÅ  property, are introduced below.

    A function h with domain an open set U âŠ‚â„ and range the set of either all real or complex numbers, is said to be real analytic at u if the function h may be represented by a convergent power series on some interval of positive radius centered at u, i.e., h(x)= âˆ‘_j=0^âˆÎ±_j(x-u)^j, for some {Î±_j}âŠ‚â„. The function is said to be real analytic on V âŠ‚ U if it is real analytic at each u âˆˆ V <cit.>. The real analytic function f over â„^p for some positive integer p>1 can be defined similarly.



   
A subset S of â„^p is a real semi-algebraic set if there exists  a finite number of real polynomial functions g_ij,h_ij:  â„^p â†’â„ such that S=âˆª_j=1^qâˆ©_i=1^m{uâˆˆâ„^p:g_ij(u)=0  and h_ij(u)<0 }. In addition, a function h:â„^p+1â†’â„âˆª+âˆ is called 	semi-algebraic if its graph {(u, t)âˆˆâ„^p+1: h(u)=t } is a real semi-algebraic set.
 
 
Based on the above definitions, the following lemma can be obtained.

Most of the commonly used NN training modelsÂ (<ref>) can be verified to satisfy the following 
  

    
  * the loss function â„“ is a proper lower semicontinuous and nonnegative function. For example,  the squared, logistic, hinge, or cross-entropy losses.
    
  * the activation functions Ïƒ_i(i=1 â€¦, N-1) are Lipschitz continuous on any bounded set. For example, ReLU, leaky ReLU, sigmoid, hyperbolic tangent, linear, polynomial, or softplus activations.
    
  * the regularizers Ï„_i and s_i(i=1, â€¦, N) are nonegative lower semicontinuous convex functions. Ï„_i and s_i are the squared â„“_2 norm, the â„“_1 norm, the elastic net, the indicator function of some nonempty closed convex set (such as the nonnegative closed half-space, box set or a closed interval [0,1]), or 0 if no regularization.
    
  * all these functions â„“, Ïƒ_i, Ï„_i and s_i(i=1, â€¦, N) are either real analytic or semialgebraic, and continuous on their domains.

Accordingly, the objective function â„’(ğ’², ğ’±,ğ’°,ğ’¢) inÂ (<ref>) has Kurdyka Åojasiewicz (KÅ) property. 
 



On the loss function â„“: Since these losses are all nonnegative and continuous on their domains, they are proper lower semicontinuous and lower bounded by 0. In the following, we only verify that they are either real analytic or semialgebraic.
  

    
  * If â„“(t) is the squared (t^2) or exponential  (e^t) loss, then according toÂ <cit.>, they are real analytic.
    
  * If â„“(t) is the logistic loss (log (1+e^-t)), since it is a composition of logarithm and exponential functions which both are real analytic, thus according toÂ <cit.>, the logistic loss is real analytic.
    
  * If â„“(u ; y) is the cross-entropy loss, i.e., given yâˆˆâ„^d_N, â„“(u ; y)=-1/d_N[âŸ¨y, logy(u)âŸ©+âŸ¨1-y, log (1-y(u))âŸ©], where log is performed elementwise and (y(u)_i)_1 â‰¤ i â‰¤ d_N:=((1+e^-u_i)^-1)_1 â‰¤ i â‰¤ d_N for any uâˆˆâ„^d_N, which can be viewed as a linear combination of logistic functions, then by (a2) and <cit.>, it is also analytic.
    
  * If â„“ is the hinge loss, i.e., given yâˆˆâ„^d_N, â„“(u ; y):=max{0,1-âŸ¨u, yâŸ©} for any uâˆˆâ„^d_N, byÂ <cit.>, it is semialgebraic, because its graph is cl(ğ’Ÿ), the closure of the set ğ’Ÿ, where ğ’Ÿ={(u, z): 1-âŸ¨u, yâŸ©-z=0, 1-uâ‰» 0}âˆª{(u, z): z=0,âŸ¨u, yâŸ©-1>0}.




On the activation function Ïƒ_i: Since all the considered specific activations are continuous on their domains, they are Lipschitz continuous on any bounded set. In the following, we only need to check that they are either real analytic or semialgebraic.
  

    
  * If Ïƒ_i is a linear or polynomial function, then according toÂ <cit.> is real analytic.
    
  * If Ïƒ_i(t) is sigmoid, (1+e^-t)^-1, or hyperbolic tangent, tanh(t):=e^t-e^-t/e^t+e^-t, then the sigmoid function is a composition g âˆ˜ h of these two functions where g(u)=1/1+u, u>0 and h(t)=e^-t (resp. g(u)=1-2/u+1, u>0 and h(t)=e^2 t in the hyperbolic tangent case). According toÂ <cit.>, g and h in both cases are real analytic. Thus,  sigmoid and hyperbolic tangent functions are real analytic.
    
  * If Ïƒ_i is ReLU, i.e., Ïƒ_i(u):=max{0, u}, then we can show that ReLU is semialgebraic since its graph is cl( .ğ’Ÿ), the closure of the set ğ’Ÿ, where ğ’Ÿ={(u, z): u-z=0, u>0}âˆª{(u, z): z=0,-u>0}.
    
  * Similar to the ReLU case, if Ïƒ_i is leaky ReLU, i.e., Ïƒ_i(u)=u if u>0, otherwise Ïƒ_i(u)=a u for some a>0, then we can similarly show that leaky ReLU is semialgebraic since its graph is cl(ğ’Ÿ), the closure of the set ğ’Ÿ, where ğ’Ÿ={(u, z): u-z=0, u>0}âˆª{(u, z): a u-z=0,-u>0}.
    
  * If Ïƒ_i is polynomial, then according toÂ <cit.>, it is real analytic.
    
  * If Ïƒ_i is softplus, i.e., Ïƒ_i(u)=1/tlog (1+e^t u) for some t>0, since it is a composition of two analytic functions 1/tlog (1+u) and e^t u, then according toÂ <cit.>, it is real analytic.

On Ï„_i(W_i), s_i(V_i): By the specific forms of these regularizers, they are nonnegative, lower semicontinuous and continuous on their domain. In the following, we only need to verify they are either real analytic and semialgebraic.
  


  * the squared â„“_2 norm Â·_2^2: According toÂ <cit.>, the â„“_2 norm is semialgebraic, so is its square where g(t)=t^2 and h(W)=W_2.


  * the squared Frobenius norm Â·_F^2: The squared Frobenius norm is semiaglebraic since it is a finite sum of several univariate squared functions.


  * the elementwise 1-norm Â·_1,1: Note that W_1,1=âˆ‘_i, j|W_i j| is the finite sum of absolute functions h(t)=|t|. According toÂ <cit.>, the absolute value function is semialgebraic since its graph is the closure of the following semialgebraic set ğ’Ÿ={(t, s): t+s=0,-t>0}âˆª{(t, s): t-s=0, t>0}. Thus, the elementwise 1-norm is semialgebraic.


  * the elastic net: Note that the elastic net is the sum of the elementwise 1-norm and the squared Frobenius norm. Thus, by (c2), (c3), andÂ <cit.>, the elastic net is semialgebraic.


  * If Ï„_i or s_i is the indicator function of nonnegative closed half-space or a closed interval (box constraints), byÂ <cit.>, any polyhedral set is semialgebraic such as the nonnegative orthant â„_+^p Ã— q={Wâˆˆâ„^p Ã— q, W_i jâ‰¥ 0, âˆ€ i, j}, and the closed interval. Thus, Ï„_i or s_i is semialgebraic in this case.


We first verify the KÅ property of â„’. FromÂ (<ref>), we have

    â„’(ğ’², ğ’±,ğ’°,ğ’¢)
            :=   â„›_n(V_N ; Y)+âˆ‘_i=1^N r_i(W_i)+âˆ‘_i=1^N s_i(V_i)
    
            +   Î³/2âˆ‘_i=1^NV_i-Ïƒ_i(U_i)_F^2+Ï/2âˆ‘_i=1^NU_i-W_iV_i-1_F^2+ Ï„/2âˆ‘_i=1^Nğ’²_i - TTD(r_i)_F^2,

which mainly includes the following types of functions, i.e.,

    â„›_n(V_N ; Y), Ï„_i(W_i), s_i(V_i),V_i-Ïƒ_i(U_i)_F^2,U_i-W_iV_i-1_F^2,âˆ‘_i=1^Nğ’²_i - TTD(r_i)_F^2.

To verify the KÅ property of the function â„’, we consider the above functions one. 

On â„›_n(V_N ; Y): Note that given the output data Y, â„›_n(V_N ; Y):=1/nâˆ‘_j=1^nâ„“((V_N)_: j, y_j), where â„“: â„^d_NÃ—â„^d_Nâ†’ â„_+âˆª{0} is some loss function. If â„“ is real analytic (resp. semialgebraic), then â„›_n(V_N ; Y) is real-analytic (resp. semialgebraic).

On V_i-Ïƒ_i(U_i)_F^2 : Note that V_i-Ïƒ_i(U_i)_F^2 is a finite sum of simple functions of the form, |v-Ïƒ_i(u)|^2 for any u, v âˆˆâ„. If Ïƒ_i is real analytic (resp. semialgebraic), then v-Ïƒ_i(u) is real analytic (resp. semialgebraic), and further |v-Ïƒ_i(u)|^2 is also real analytic (resp. semialgebraic) since |v-Ïƒ_i(u)|^2 can be viewed as the composition g âˆ˜ h of these two functions where g(t)=t^2 and h(u, v)=v-Ïƒ_i(u).

On U_i-W_iV_i-1_F^2: Note that the function U_i-W_iV_i-1_F^2 is a polynomial function with the variables U_i, W_i and V_i-1, and thus according toÂ <cit.> and <cit.>, it is both real analytic and semialgebraic.

On Ï„_i(W_i), s_i(V_i): All Ï„_i's and s_i's are real analytic or semialgebraic.

On ğ’²_i - TTD(r_i)_F^2: Note that the function ğ’²_i - TTD(r_i)_F^2 is a polynomial function with the variables W_i, ğ’¢_i.

Since each part of the function â„’ is either real analytic or semialgebraic, â„’ is a subanalytic functionÂ <cit.>. Furthermore, by the continuity, â„’ is continuous in its domain. Therefore, â„’ is a KÅ function according to <cit.>.[Let h: â„^pâ†’â„âˆª{+âˆ} be a subanalytic function with closed domain, and assume that h is continuous on its domain, then h is a KÅ function.]





Based on LemmasÂ <ref>,Â <ref>, andÂ <ref> and conclusions inÂ <cit.>, the following main theorem can be obtained.








 
Let {ğ’«^k:=({W_i^k}_i=1^N,{V_i^k}_i=1^N,{U_i^k}_i=1^N),{ğ’¢_i^k}_i=1^N}_k âˆˆâ„• be the sequences generated from AlgorithmÂ <ref>.   Suppose that Ï„_i  and â„’ are coercive for any i=1, â€¦, N. Then for any Î±,Î³,Ï,Ï„>0 and any finite initialization ğ’«^0, the following hold
  

    
  * {â„’(ğ’«^k)}_k âˆˆâ„• converges to â„’^*.
    
  * {ğ’«^k}_k âˆˆâ„•  converges to a critical point of â„’.
    
  * If further the initialization ğ’«^0 is sufficiently close to some global minimum ğ’«^* of â„’, then ğ’«^k  converges to ğ’«^*.
    
  * Let Î¸ be the KÅ exponent of â„’ at ğ’«^*. There hold: (a) if Î¸=0, then {ğ’«^k}_k âˆˆâ„• converges in a finite number of steps; (b) if Î¸âˆˆ(0, 1/2], then ğ’«^k-ğ’«^*_Fâ‰¤ C Î·^k for all k â‰¥ k_0, for certain k_0>0, C>0, Î·âˆˆ(0,1); and (c) if Î¸âˆˆ(1/2, 1), then ğ’«^k-ğ’«^*_Fâ‰¤ C k^-1-Î¸/2 Î¸-1 for k â‰¥ k_0, for certain k_0>0, C>0. 
    
  * 1/Kâˆ‘_k=1^Kg^k_F^2â†’ 0 at the rate ğ’ª(1 / K) where g^kâˆˆ âˆ‚â„’(ğ’«^k). 







 Lispchitz differentiable property is a required for nonconvex optimizations with multi-block variables to build the convergence in the existing literatureÂ <cit.>. However, the NN training problemÂ (<ref>) in this paper generally does not satisfy such a condition. For example, when ReLU activation is used. TheoremÂ <ref> establishes the global convergence under a very mild condition that most NN models satisfy.
 
Extension to ResNetsÂ <cit.>: the theoretical results in TheoremÂ <ref> can be extended to ResNets by considering the following optimization problem

    min _ğ’², ğ’±â„’_0(ğ’², ğ’±)      subject to U_i=W_iV_i-1,V_i-V_i-1 =Ïƒ_i(U_i), ğ’²_i = TTD(r_i)   i=1, â€¦, N,
 
where the residual term V_i-V_i-1 is considered instead of V_i. The corresponding algorithm can be easily modified from AlgorithmÂ <ref>. 




Â§ CASE STUDY
 











In this experiment, to evaluate the effectiveness and efficiency of our proposed method, NN modelÂ (<ref>) training with different compression ratios (determined by TT-rank r_i) is conducted on the image classification task. In terms of the NN model, ReLU activation, the squared loss, and the network architecture being an MLP with ten hidden layers are considered here. The number of hidden units in each layer is 2^9=512. The neural network is trained on the MNIST dataset, which is a handwritten digits dataset. The size of each input image is d_0=28Ã—28=784 and the output dimension is d_11=10.  The numbers of training and test samples are 60,000 and 10,000, respectively. For comparison, SGD is also considered as a benchmark method, where the learning rate is 0.001.



For each experiment, the same mini-batch sizes (512) and initializations for all algorithms. All the experiments are repeated ten times to obtain the average performance.  Specifically, all the weights {W_i}_i=1^N are initialized from a Gaussian distribution with a standard deviation of 0.01. The auxiliary variables {U_i}_i=1^N, state variables {V_i}_i=1^N, TT-cores {ğ’¢_i}_i=1^N are initialized by a single forward passÂ <cit.>. Under these settings, the training loss, training accuracy, and test accuracy are shown in TableÂ <ref>.  With a smaller CR (# of parameters after compression/# of parameters without compression), a higher training loss is observed. Our proposed method with CR<1 can outperform the uncompressed method and SGD. In addition, the curves of the training loss and test accuracy are plotted in FigureÂ <ref>.  FigureÂ <ref> shows that the proposed method converges with different compression rates.  The training loss of our proposed method also shows the monotone decreasing trend, which verified the statements in TheoremÂ <ref>. FigureÂ <ref> shows that, for different CR (<1), the test accuracy of our proposed method keeps increasing as the number of iterations increases. When CR=1 (the model without compression), the test accuracy increases first and then decreases. This result demonstrates that model compression can prevent overfitting. In addition, our proposed method with CR<1 can outperform SGD significantly in terms of test accuracy. 




Â§ CONCLUSION
 

In this paper, a holistic framework is proposed for tensor decomposition-based NN model compression by formulating TT decomposition-based NN training as a nonconvex optimization problem. The framework can be extended to other formats of tensor decomposition such as Tucker decomposition, and CP decomposition. For the first time in the literature on tensor decomposition-based NN model compression, global convergence is guaranteed for the proposed tensor BCD (tenBCD) algorithm. Specifically, tenBCD converges to a critical point at a rate of ğ’ª(1/k), where k is the number of iterations.  The empirical experiment shows that the proposed method can converge and run efficiently in practice. Compared with SGD, the proposed method can maintain a high compression rate and high accuracy simultaneously. 





















































































































 




    




Â§ SOLUTIONS OF SOME SUBPROBLEMS

In this section, we provide the solution to subproblemÂ (<ref>), closed-form solutions to the ReLU-involved subproblem.


 Â§.Â§ Solutions to SubproblemÂ (<ref>)

Prox-linear algorithm to subproblemÂ (<ref>): in the V_N-update of AlgorithmÂ <ref>, the empirical risk is involved in the optimization problems. It is generally hard to obtain its closed-form solution except for some special cases such as the case where the loss is the square loss. For other smooth losses such as the logistic, cross-entropy, and exponential losses, we suggest using the following prox-linear update strategies, that is, for some parameter Î±>0, the V_N-update in  AlgorithmÂ <ref> is

    V_N^k=V_Nargmin{s_N(V_N)+âŸ¨âˆ‡â„›_n(V_N^k-1 ; Y), V_N-V_N^k-1âŸ© +Î³/2V_N-U_N^k-1_F^2+Î±/2V_N-V_N^k-1_F^2},

This V_N-update can be implemented with explicit expressions. Therefore, the specific uses of these tenBCD methods are very flexible, mainly depending on users' understanding of their own problems.

The closed-form of the proximal operator of hinge loss: consider the following optimization problem

    u^*=uargmin g(u):=max{0,1-a Â· u}+Î³/2(u-b)^2,

where Î³>0

    The optimal solution to ProblemÂ (<ref>) is shown as follows
    
    hinge_Î³(a, b)= b,     if  a=0, 
     b+Î³^-1 a,     if  a â‰  0  and  a b â‰¤ 1-Î³^-1 a^2, 
     a^-1,     if  a â‰  0  and  1-Î³^-1 a^2<a b<1, 
     b,     if  a â‰  0  and  a b â‰¥ 1 .





 Â§.Â§ The Closed-form Solution to SubproblemÂ (<ref>)

From AlgorithmÂ <ref>, when Ïƒ_i is ReLU, then the U_i^k-update actually reduces to the following one-dimensional minimization problem

    u^*=uargmin f(u):=1/2(Ïƒ(u)-a)^2+Î³/2(u-b)^2,

where Ïƒ(u)=max{0, u} and Î³>0. The solution to the above one-dimensional minimization problem can be presented in the following lemma.

The optimal solution to ProblemÂ (<ref>) is shown as follows

    prox_1/2 Î³(Ïƒ(Â·)-a)^2(b)={[                      a+Î³ b/1+Î³,           if  a+Î³ b â‰¥ 0, b â‰¥ 0,;                      a+Î³ b/1+Î³,     if -(âˆš(Î³(Î³+1))-Î³) a â‰¤Î³ b<0,;                              b, if -a â‰¤Î³ b â‰¤-(âˆš(Î³(Î³+1))-Î³) a<0,;                      min{b, 0},                    if  a+Î³ b<0. ].






Â§ KEY PROOF OF THEOREMÂ <REF>

Based on LemmaÂ <ref> and under the hypothesis that â„’ is continuous on its domain and there exists a convergent subsequence, the continuity condition required in <cit.> holds naturally, i.e., there exists a subsequence {ğ’«^k_j}_j âˆˆâ„• and ğ’«^* such that

    ğ’«^k_jâ†’ğ’«^*   and â„’(ğ’«^k_j) â†’â„’(ğ’«^*) , as  j â†’âˆ

Based on LemmasÂ <ref>,Â <ref>, andÂ <ref>, we can justify the global convergence of ğ’«^k stated in TheoremÂ <ref>, following the proof idea ofÂ <cit.>. For the completeness of the proof, we still present the detailed proof as follows.

Before presenting the main proof, we establish a local convergence result of ğ’«^k, i.e., the convergence of ğ’«^k when ğ’«^0 is sufficiently close to some point ğ’«^*. Specifically, let (Ï†, Î·, U) be the associated parameters of the KÅ property of â„’ at ğ’«^*, where Ï† is a continuous concave function, Î· is a positive constant, and U is a neighborhood of ğ’«^*. Let Ï be some constant such that ğ’©(ğ’«^*, Ï):={ğ’«:ğ’«-ğ’«^*_Fâ‰¤Ï}âŠ‚ U, â„¬:=Ï+ğ’«^*_F, and L_â„¬ be the uniform Lipschitz constant for Ïƒ_i, i=1, â€¦, N-1, within ğ’©(ğ’«^*, Ï). Assume that ğ’«^0 satisfies the following condition

    Î´Ì…/Î»Ï†(â„’(ğ’«^0)-â„’(ğ’«^*))+3 âˆš(â„’(ğ’«^0)/Î»)+ğ’«^0-ğ’«^*_F<Ï,

where Î´Ì…=Î´âˆš(4 N), Î» and Î´ are defined in LemmasÂ <ref> andÂ <ref>, respectively.

     Under the conditions of Theorem 5, suppose that ğ’«^0 satisfies the conditionÂ (<ref>), and â„’(ğ’«^k)>â„’(ğ’«^*) for k âˆˆâ„•, then

    
    âˆ‘_i=1^kğ’«^i-ğ’«^i-1_F   â‰¤ 2 âˆš(â„’(ğ’«^0)/Î»)+Î´Ì…/Î»Ï†(â„’(ğ’«^0)-â„’(ğ’«^*)), âˆ€ k â‰¥ 1 
    ğ’«^k   âˆˆğ’©(ğ’«^*, Ï),   âˆ€ k âˆˆâ„•.


As k goes to infinity, (<ref>) yields

    âˆ‘_i=1^âˆğ’«^i-ğ’«^i-1_F<âˆ,

which implies the convergence of {ğ’«^k}_k âˆˆâ„•.


    We will prove ğ’«^kâˆˆğ’©(ğ’«^*, Ï) by induction on k. It is obvious that ğ’«^0âˆˆğ’©(ğ’«^*, Ï). Thus, (<ref>) holds for k=0. For k=1, we have fromÂ (<ref>) and the nonnegativeness of {â„’(ğ’«^k)}_k âˆˆâ„• that
    
    â„’(ğ’«^0) â‰¥â„’(ğ’«^0)-â„’(ğ’«^1) â‰¥ ağ’«^0-ğ’«^1_F^2,

which implies ğ’«^0-ğ’«^1_Fâ‰¤âˆš(â„’(ğ’«^0)/Î»). Therefore,

    ğ’«^1-ğ’«^*_Fâ‰¤ğ’«^0-ğ’«^1_F+ğ’«^0-ğ’«^*_Fâ‰¤âˆš(â„’(ğ’«^0)/Î»)+ğ’«^0-ğ’«^*_F,

which indicates ğ’«^1âˆˆğ’©(ğ’«^*, Ï).

Suppose that ğ’«^kâˆˆğ’©(ğ’«^*, Ï) for 0 â‰¤ k â‰¤ K. We proceed to show that ğ’«^K+1âˆˆğ’©(ğ’«^*, Ï). Since ğ’«^kâˆˆğ’©(ğ’«^*, Ï) for 0 â‰¤ k â‰¤ K, it implies that ğ’«^k_Fâ‰¤â„¬:=Ï+ğ’«^* for 0 â‰¤ k â‰¤ K. Thus, by LemmaÂ <ref>, for 1 â‰¤ k â‰¤ K,

    dist(0, âˆ‚â„’(ğ’«^k)) â‰¤Î´Ì…ğ’«^k-ğ’«^k-1_F,

which together with the KÅ inequalityÂ (<ref>) yields

    1/Ï†^'(â„’(ğ’«^k)-â„’(ğ’«^*))â‰¤Î´Ì…ğ’«^k-ğ’«^k-1_F

By inequalityÂ (<ref>), the above inequality and the concavity of Ï†, for k â‰¥ 2, the following holds

    Î»ğ’«^k-ğ’«^k-1_F^2   â‰¤â„’(ğ’«^k-1)-â„’(ğ’«^k)=(â„’(ğ’«^k-1)-â„’(ğ’«^*))-(â„’(ğ’«^k)-â„’(ğ’«^*)) 
       â‰¤Ï†(â„’(ğ’«^k-1)-â„’(ğ’«^*))-Ï†(â„’(ğ’«^k)-â„’(ğ’«^*))/Ï†^'(â„’(ğ’«^k-1)-â„’(ğ’«^*))
       â‰¤Î´Ì…ğ’«^k-1-ğ’«^k-2_FÂ·[Ï†(â„’(ğ’«^k-1)-â„’(ğ’«^*))-Ï†(â„’(ğ’«^k)-â„’(ğ’«^*))],

which implies

    ğ’«^k-ğ’«^k-1_F^2â‰¤ğ’«^k-1-ğ’«^k-2_FÂ·Î´Ì…/Î»[Ï†(â„’(ğ’«^k-1)-â„’(ğ’«^*))-Ï†(â„’(ğ’«^k)-â„’(ğ’«^*))].

Taking the square root on both sides and using the inequality 2 âˆš(Î±Î²)â‰¤Î±+Î², the above inequality implies

    2ğ’«^k-ğ’«^k-1_Fâ‰¤ğ’«^k-1-ğ’«^k-2_F+Î´Ì…/Î»[Ï†(â„’(ğ’«^k-1)-â„’(ğ’«^*))-Ï†(â„’(ğ’«^k)-â„’(ğ’«^*))].

Summing the above inequality over k from 2 to K and adding ğ’«^1-ğ’«^0_F to both sides, it yields

    ğ’«^K-ğ’«^K-1_F+âˆ‘_k=1^Kğ’«^k-ğ’«^k-1_Fâ‰¤ 2ğ’«^1-ğ’«^0_F+Î´Ì…/Î»[Ï†(â„’(ğ’«^0)-â„’(ğ’«^*))-Ï†(â„’(ğ’«^K)-â„’(ğ’«^*))]

which implies

    âˆ‘_k=1^Kğ’«^k-ğ’«^k-1_Fâ‰¤ 2 âˆš(â„’(ğ’«^0)/Î»)+Î´Ì…/Î»Ï†(â„’(ğ’«^0)-â„’(ğ’«^*)),

and further,

    ğ’«^K+1-ğ’«^*_Fâ‰¤ğ’«^K+1-ğ’«^K_F+âˆ‘_k=1^Kğ’«^k-ğ’«^k-1_F+ğ’«^0-ğ’«^*_F
       â‰¤âˆš(â„’(ğ’«^K)-â„’(ğ’«^K+1)/Î»)+2 âˆš(â„’(ğ’«^0)/Î»)+Î´Ì…/Î»Ï†(â„’(ğ’«^0)-â„’(ğ’«^*))+ğ’«^0-ğ’«^*_F
       â‰¤ 3 âˆš(â„’(ğ’«^0)/Î»)+Î´Ì…/Î»Ï†(â„’(ğ’«^0)-â„’(ğ’«^*))+ğ’«^0-ğ’«^*_F<Ï,

where the second inequality holds forÂ (<ref>) andÂ (<ref>), the third inequality holds for â„’(ğ’«^K)-â„’(ğ’«^K+1) â‰¤â„’(ğ’«^K) â‰¤ â„’(ğ’«^0). Thus, ğ’«^K+1âˆˆğ’©(ğ’«^*, Ï). Therefore, we prove this lemma.

We prove the whole sequence convergence stated in TheoremÂ <ref> according to the following two cases.

Case 1: â„’(ğ’«^k_0)=â„’(ğ’«^*) at some k_0. In this case, by LemmaÂ <ref>, ğ’«^k=ğ’«^k_0=ğ’«^* holds for all k â‰¥ k_0, which implies the convergence of ğ’«^k to a limit point ğ’«^*.

Case 2: â„’(ğ’«^k)>â„’(ğ’«^*) for all k âˆˆâ„•. In this case, since ğ’«^* is a limit point and â„’(ğ’«^k) â†’â„’(ğ’«^*), by Theorem 4 , there must exist an integer k_0 such that ğ’«^k_0 is sufficiently close to ğ’«^* as required in LemmaÂ <ref> (see the inequalityÂ (<ref>)). Therefore, the whole sequence {ğ’«^k}_k âˆˆâ„• converges according to LemmaÂ <ref>. Since ğ’«^* is a limit point of {ğ’«^k}_k âˆˆâ„•, we have ğ’«^kâ†’ğ’«^*.

Next, we show ğ’«^* is a critical point of â„’. By  lim _k â†’âˆğ’«^k-ğ’«^k-1_F=0. Furthermore, by LemmaÂ <ref>,

    lim _k â†’âˆdist(0, âˆ‚â„’(ğ’«^k))=0,

which implies that any limit point is a critical point. Therefore, we prove the global convergence of the sequence generated by AlgorithmÂ <ref>.

The convergence to a global minimum is a straightforward variant of LemmaÂ <ref>.

The ğ’ª(1 / k) rate of convergence is a direct claim according to the proof of LemmaÂ <ref> and lim _k â†’âˆğ’«^k-ğ’«^k-1_F=0.







































































































