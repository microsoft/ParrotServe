
Better than square-root cancellation]Better than square-root cancellation for random multiplicative functions


Department of Mathematics, Stanford University, Stanford, CA, USA
maxxu@stanford.edu




    We investigate when the better than square-root cancellation phenomenon exists for âˆ‘_nâ‰¤ Na(n)f(n), where a(n)âˆˆâ„‚ and f(n) is a random multiplicative function. We focus on the case where a(n) is the indicator function of R rough numbers. We prove that loglog R â‰ (loglog x)^1/2 is the threshold for the better than square-root cancellation phenomenon to disappear.

[
    Max Wenqiang Xu
    
===================





Â§ INTRODUCTION


The study of random multiplicative functions has attracted intensive attention. Historically, they were introduced to model arithmetic functions. A Steinhaus random multiplicative function f(n) is a completely multiplicative function defined on positive integers such that f(p) are independently and uniformly distributed on the complex unit circle for all primes p. One may view it as a random model for arithmetic functions like Dirichlet characters Ï‡(n) or n^it. Another popular model is the Rademacher random multiplicative function f(n) which was first used by Wintner<cit.> as a random model for MÃ¶bius function Î¼(n). In this note, we focus on the Steinhaus case. The obvious dependence between random variables f(m) and f(n) whenever (m, n)â‰  1 makes the study of random multiplicative functions intriguing.


Arguably the most striking result so far in the study of random multiplicative functions is Harper's <cit.> remarkable resolution of Helson's conjecture<cit.>, that is, the partial sums of random multiplicative functions enjoy better than square-root cancellation

    [|âˆ‘_nâ‰¤ x f(n) |] â‰âˆš(x)/(loglog x)^1/4,

where f(n) are random multiplicative functions. 
In particular, with the natural normalization âˆš(x), the partial sums âˆ‘_nâ‰¤ x f(n) do not converge in distribution to the standard complex normal distribution (see also <cit.>). Before Harper's result <cit.>, there was progress on proving good lower bounds close to âˆš(x), e.g. <cit.>, and it was not clear that such better than square-root cancellation in (<ref>) would  appear until Harper's proof. See also recent companion work on analogous results in the character sums and zeta sums cases established by Harper <cit.>.
It is known that the better than square-root cancellation phenomenon in random multiplicative functions is connected to the â€œcritical multiplicative chaos" in the probability literature. We point out references <cit.> for related discussions.  

A closely related important question in number theory is to understand the distribution of the Riemann zeta function over typical intervals of length 1 on the critical line 
â„œğ”¢(s)=1/2. One may crudely see the connection by viewing Î¶(s) as a sum of n^-1/2-it for a certain range of n and n^it behaves like a Steinhaus random multiplicative function for randomly chosen t. A conjecture of Fyodorov, Hiary, and Keating (see e.g. <cit.>) suggests that there is a subtle difference between the true order of local maximal of log|Î¶(1/2+it)| and one's first guess based on Selberg's central limit theorem for log|Î¶(1/2+it)|. The existence of this subtle difference and the appearance of the better than square-root cancellation for random multiplicative functions both show that the corresponding nontrivial dependence can not be ignored. We refer readers to <cit.> for related discussions about partial sums of random multiplicative functions and zeta values distribution.

In this paper, we are interested in further exploring Harper's result (<ref>) and methods used there, by considering the problem in a more general context. 
 Let a(n) be a sequence in . 
When does the better than square-root cancellation phenomenon hold for âˆ‘_nâ‰¤ N a(n)f(n), i.e.

    [|âˆ‘_nâ‰¤ Na(n) f(n)|] = o(âˆš(âˆ‘_nâ‰¤ N|a(n)|^2))ÌŠ?



We first make some simple observations in the situations where a(n) is â€œtypical" or a(n) has a rich multiplicative structure. Then we focus on a particular case where the coefficient a(n) is an indicator function of a multiplicative set. 


 Â§.Â§ Typical coefficients

If partial sums âˆ‘_nâ‰¤ Na(n)f(n) with the square-root size normalization behave like the complex standard Gaussian variable, then there is just square-root cancellation. One may attempt to prove such a central limit theorem by computing the high moments, however, the moments usually blow up and such a strategy does not work here (see e.g. <cit.> for moments computation results). It turns out that for â€œtypical" choices of a(n), such a central limit theorem does hold. It has been carried out in the concrete case where a(n)=e^2Ï€ i n Î¸ for some fixed real Î¸ without too good Diophantine approximation property (such Î¸ has relative density 1 in , e.g. one can take Î¸=Ï€) by Soundararajan and the author <cit.>, and also an average version of the result is proved by Benatar, Nishry and Rodgers <cit.>. The proof of the result in <cit.> is based on McLeish's martingale central limit theorem<cit.>, and the method was pioneered by Harper in <cit.>.  
The proof reveals the connection between the existence of such a central limit theorem and a quantity called multiplicative energy of a:= {a(n): 1â‰¤ nâ‰¤ N} 

    E_Ã—(ğš): = âˆ‘_m_1, n_1, m_2, n_2 â‰¤ N
     m_1m_2=n_1n_2a(m_1)a(m_2) a(n_1)a(n_2).

A special case of a(n) is an indicator function of a set , and the quantity E_Ã—() is a popular object studied in additive combinatorics. It is now known <cit.> that a crucial condition for such a central limit theorem holds for âˆ‘_nâ‰¤ Na(n)f(n) is that the set  has multiplicative energy â‰¤(2+Ïµ)||^2. See <ref> for more discussions on a(n) being a â€œtypical" choice.  We refer readers who are interested in seeing more examples of when a central limit theorem holds for partial (restricted) sums of random multiplicative functions to <cit.>.  



 Â§.Â§ Large multiplicative energy and sparse sets
 Let us focus on the case that a_n is an indicator function of a set . As we mentioned that if the set  has small multiplicative energy (among other conditions), then partial sums exhibit square-root cancellation. Suppose we purposely choose a set  with very large multiplicative energy, will it lead to better than square-root cancellation? One extreme example is = {p^n: 1â‰¤ n â‰¤log_p N} being a geometric progression, where p is a fixed prime. A standard calculation gives that

    [| âˆ‘_nâˆˆ f(n) |] =  âˆ«_0^1 |âˆ‘_nâ‰¤log_p Ne(Î¸ n)| dÎ¸â‰loglog N,

while [| âˆ‘_nâˆˆ f(n) |^2] 
 = ||â‰log N. 
It shows that there is a great amount of cancellation in this particular example. One may also take  to be some generalized (multidimensional) geometric progression and get strong cancellation of this type. We note that the sets mentioned here with very rich multiplicative structures all have small sizes. 

Based on the initial thoughts above, we may lean toward believing that better than square-root cancellation only appears when a(n) has some particular structure that is perhaps related to multiplicativity.  
To fully answer QuestionÂ <ref> seems hard. The majority of the paper is devoted to a special case, where a(n) is an indicator function of a set with multiplicative features. We focus on fairly large subsets.


 Â§.Â§ Main results: multiplicative support

Suppose now that a(n) is a multiplicative function with |a(n)|â‰¤ 1. 
The particular example we study in this paper is that a(n) is the indicator function of R-rough numbers, although the proof here may be adapted to other cases when a(n) is multiplicative.  
We write 

    _R(x): = {nâ‰¤ x: p|n  pâ‰¥ R}.


By a standard sieve argument, for all 2â‰¤ Râ‰¤ x/2 (the restriction Râ‰¤ x/2 is only needed for the lower bound), we have asymptotically 

    |_R(x)| â‰x/log R.

We expect the following threshold behavior to happen. If R is very small, the set _R(x) is close to [1,x] and better than square-root cancellation appears as in <cit.>. If R is sufficiently large, then weak dependence  may even lead to a central limit theorem. Indeed, an extreme case is that R> âˆš(x), in which _R(x) is a set of primes and {f(n): nâˆˆ} is a set of independent random variables. It is natural to ask to what extent the appearance of small primes is needed to guarantee better than square-root cancellation.
Our TheoremÂ <ref> and TheoremÂ <ref> answer the question. We show that loglog R â‰ˆ (loglog x)^1/2 is the threshold.



  Let f(n) be a Steinhaus random multiplicative function and x be large. Let _R(x) be the set of 
  R rough numbers up to x. For any loglog Râ‰ª (loglog x)^1/2, we have   
  
    [|âˆ‘_nâˆˆ_R(x) f(n) |] â‰ªâˆš(|_R(x)|)Â· ( loglog R +  logloglog x/âˆš(loglog x))^1/2 .

 In particular, if loglog R = o((loglog x)^1/2), then  
   
    [|âˆ‘_nâˆˆ_R(x) f(n) |] =o ( âˆš(|_R(x)|)).


The term logloglog x is likely removable. But for the convenience of the proof, we state the above version. See RemarkÂ <ref> for more discussions.


  Let f(n) be a Steinhaus random multiplicative function and x be large. Let _R(x) be the set of 
  R rough numbers up to x. For any loglog Râ‰«  (loglog x)^1/2, we have   
  
    [|âˆ‘_nâˆˆ_R(x) f(n) |] â‰«âˆš(|_R(x)|) .


One probably can prove a lower bound of the shape âˆš(||)Â·(loglog R / âˆš(loglog x))^-1/2
 when loglog R =o(âˆš(loglog x)). We do not pursue this as we focus on finding the threshold value of R instead of caring about the quantification of the exact cancellation. 


We note that one way to derive a lower bound on L^1 norm is by proving an upper bound on L^4 norm. A simple application of HÃ¶lder's inequality gives that

    |_R(x)| = [|âˆ‘_nâˆˆ_R(x) f(n) |^2] â‰¤([|âˆ‘_nâˆˆ_R(x) f(n) |^4])^1/3([|âˆ‘_nâˆˆ_R(x) f(n) |])^2/3.

The fourth moment  â‰ª||^2 would imply that L^1 norm â‰«âˆš(||). However, to achieve such a bound on the fourth moment, one needs log R â‰« (log x)^c for some constant c, and thus this approach would not give the optimal range as in TheoremÂ <ref>.

Another reason for studying  the fourth moment (multiplicative energy) is to understand the distribution. As mentioned before, this is the key quantity that needs to be understood in order to determine if random sums have Gaussian limiting distribution, via the criteria in <cit.>. One may establish a central limit theorem in the range Râ‰«exp((log x)^c) for some small positive constant c[One trick to get a smaller c than by directly computing the fourth moment over the full sum is to take the anatomy of integers into account. We refer interested readers to <cit.> to see how this idea is connected to the correct exponent in extremal sum product conjecture of Elekes and Ruzsa <cit.>.]. Interested readers are suggested to adapt the proof of <cit.>. We do not pursue results along this direction in this note. 

TheoremÂ <ref> and TheoremÂ <ref> are both proved by adapting Harper's robust method in <cit.>, with some modifications, simplifications and new observations, and we sketch the strategy with a focus on how we find the threshold. We also refer readers to a model problem in the function field case by Soundararajan and Zaman <cit.>.  The first step is to reduce the L^1 norm estimate to a certain average of the square of random Euler products. Basically, we prove that 

    [|âˆ‘_nâˆˆ f(n)|] â‰ˆ(x/log x)^1/2Â·[(âˆ«_-1/2^1/2 |F^(R)(1/2 + it)|^2 dt  )^1/2 ],

where F^(R)(1/2+it) := âˆ_Râ‰¤ pâ‰¤ x (1-f(p)/p^1/2+it)^-1 is the random Euler product over primes Râ‰¤ p â‰¤ x. The challenging part is to give a sharp bound on the above expectation involving |F^(R)(1/2+it)|^2 for |t|â‰¤ 1/2. 

We first discuss the upper bound proof. If we directly apply HÃ¶lder's inequality (i.e. moving the expectation inside the integral in (<ref>)), then 
we would only get the trivial upper bound  â‰ªâˆš(||) as [|F^(R)(1/2+it)|^2]â‰ˆlog x/log R. Harper's method starts with putting some â€œbarrier events" on the growth rate of all random partial Euler products for all t. Roughly speaking, it requires that for all k,

    âˆ_x^e^-(k+1)â‰¤ p â‰¤ x^e^-k |1-f(p)/p^1/2+it|^-1Â â€œgrows as expected" for all |t|â‰¤ 1.

Denote such good events by ğ’¢ and write s=1/2+it. By splitting the probability space based on the event ğ’¢ holding or not, and applying Cauchyâ€“Schwarz inequality, we have 

    [(âˆ«_-1/2^1/2 |F^(R)(s)|^2 dt  )^1/2 ]
       â‰ˆ[(âˆ«_-1/2^1/21_ğ’¢ |F^(R)(s)|^2 dt  )^1/2] +  [(âˆ«_-1/2^1/21_ğ’¢Â fail |F^(R)(s)|^2 dt  )^1/2]  
       â‰ª[(âˆ«_-1/2^1/21_ğ’¢ |F^(R)(s)|^2 dt  )^1/2] + (1_ğ’¢Â fail)^1/2 ([|F^(R)(s)|^2])^1/2 .

According to the two terms above, there are two tasks that remain to be done. 

    
  * Task 1: Show that the expectation is small, conditioning on 1_ğ’¢.

  * Task 2: Show that (1_ğ’¢Â fail) is sufficiently small. 
 
To accomplish task 1, Harper's method connects such an estimate to the â€œballot problem" or say Gaussian random walks (see <ref>), which is used to estimate the probability of partial sums of independent Gaussian variables having a certain barrier in growth. Task 2 of estimating the probability of such good events ğ’¢ happening can be done by using some concentration inequality, e.g. Chebyshev's inequality. 
Our main innovation lies in setting up â€œbarrier events" in (<ref>) properly which is not the same as in <cit.>. On one hand, it should give a strong enough restriction on the growth rate of the products so that [(âˆ«_-1/2^1/21_ğ’¢ |F^(R)(s)|^2 dt  )^1/2] has a saving, compared to it without conditioning on 1_ğ’¢. On the other hand, one needs to show that such an event ğ’¢ is indeed very likely to happen which requires that the designed â€œbarrier" can not be too restrictive. To make the two goals simultaneously achieved, we need loglog R = o( âˆš(loglog x)) and this is the limit that we can push to (see RemarkÂ <ref>).

The lower bound proof in TheoremÂ <ref> uses the same strategy as in <cit.> but is technically simpler. After the deduction step of reducing the problem to studying a certain average of the square of random Euler products (see (<ref>)), we only need to give a lower bound of the shape â‰« (log x / log R)^1/2 for the expectation on the right-hand side of (<ref>). Since the integrand |F^(R)(s)|^2 is positive, it suffices to prove such a lower bound when t is restricted to a random subset â„’. We choose â„’ to be the set of t such that certain properly chosen â€œbarrier events" hold.  The main difficulty is to give a strong upper bound on the restricted product [1_t_1, t_2âˆˆâ„’|F^(R)(1/2+it_1)|^2|F^(R)(1/2+it_2)|^2] in the sense that the bound is as effective as in the ideal situation where the factors |F^(R)(1/2+it_1)|^2 and |F^(R)(1/2+it_2)|^2 are independent (see PropositionÂ <ref>), and this is also the main reason that the condition loglog R â‰«âˆš(loglog x) is needed subject to our chosen â€œbarrier events". Our proof of TheoremÂ <ref> does not involve the â€œtwo-dimensional Girsanov calculation", which hopefully makes it easier for readers to follow.
 




 Â§.Â§ Organization
 We set up the proof outline of TheoremÂ <ref> in SectionÂ <ref> and defer the proof of two propositions to SectionÂ <ref> and SectionÂ <ref> respectively. We put all probabilistic preparations in SectionÂ <ref> which will be used in the proof for both theorems. The proof of TheoremÂ <ref> is done in SectionÂ <ref> and again we defer proofs of two key propositions to SectionÂ <ref> and SectionÂ <ref> respectively. Finally, we give more details about the â€œtypical" choices of a(n) in SectionÂ <ref>, as well as mentioning some natural follow-up open problems. 



 Â§.Â§ Acknowledgement

We thank Adam Harper  for helpful discussions, corrections, and comments on earlier versions of the paper and for his encouragement. We also thank Kannan Soundararajan for the interesting discussions. The author is supported by the Cuthbert C. Hurd Graduate Fellowship in the Mathematical Sciences, Stanford. 



Â§ PROOF OF THEOREMÂ <REF>

We follow the proof strategy of Harper in <cit.>.
We establish TheoremÂ <ref> in a stronger form that for 1/2â‰¤ q â‰¤ 9/10 and R in the given range loglog R â‰ª (loglog x)^1/2,  

    [|âˆ‘_nâˆˆ_R(x) f(n) |^2q] â‰ª |_R(x)|^q ( loglog R +  logloglog x/âˆš(loglog x))^q.

One should be able to push the range of q to 1 but for simplicity in notation, we omit it. Our interest is really about the case q=1/2.
Note that in the given range of R, by (<ref>), it is the same as proving 

    [|âˆ‘_nâˆˆ_R(x) f(n) |^2q] â‰ª(x/log R)^q ( loglog R +  logloglog x/âˆš(loglog x))^q.


The first step (PropositionÂ <ref>) is to connect the L^1 norm of the random sums to a certain average of the square of random Euler products. We define for all s with â„œğ”¢(s)>0 and integers 0â‰¤ kâ‰¤loglog x -loglog R, the random Euler products

    F_ k^(R)(s) : = âˆ_Râ‰¤ pâ‰¤ x^e^-(k+1) (1- f(p)/p^s)^-1 = âˆ‘_nâ‰¥ 1 
     p|n Râ‰¤  pâ‰¤ x^e^-(k+1)f(n)/n^s.

We also write 

    F^(R)(s): = âˆ_Râ‰¤ pâ‰¤ x (1- f(p)/p^s)^-1 = âˆ‘_nâ‰¥ 1 
     p|n Râ‰¤  pâ‰¤ xf(n)/n^s.

We use the notation X_2q: = ([|X|^2q])^1/2q for random variable X. 

Let f(n) be a Steinhaus random multiplicative function and x be large. Let F_k^(R)(s) be defined as in (<ref>) and loglog R â‰ª (loglog x)^1/2. Set ğ’¦: =âŒŠlogloglog x âŒ‹. Then uniformly for all 1/2 â‰¤ qâ‰¤ 9/10, we have 

    âˆ‘_nâˆˆ f(n)_2qâ‰¤âˆš(x/log x)âˆ‘_0â‰¤ k â‰¤ğ’¦âˆ«_-1/2^1/2 | F_ k^(R)(1/2 - k/log x+it)|^2dt_q^1/2 + âˆš(x/log x).


We remind the readers that the upper bound we aim for in TheoremÂ <ref> is very close to âˆš(x/log R). The second term in (<ref>) is harmless since log R is much smaller than log x. 



The second step deals with the average of the square of random Euler products in (<ref>), which lies at the heart of the proof.  



Let F_k^(R)(s) be defined as in (<ref>) and loglog R â‰ª (loglog x)^1/2. Then for all 0â‰¤ k â‰¤ğ’¦=âŒŠlogloglog x âŒ‹, and uniformly for all 1/2â‰¤ qâ‰¤ 9/10, we have 

    [(âˆ«_-1/2^1/2 |F_ k^(R)(1/2 - k/log x + it)|^2dt)ÌŠ^q]ÌŠâ‰ª e^-k/2Â·(log x/log R)ÌŠ^q ( loglog R + logloglog x/âˆš(loglog x))^q .






Apply PropositionÂ <ref> and PropositionÂ <ref> with q=1/2. Notice that when  loglog R â‰ª (loglog x)^1/2, the term âˆš(x/log x) in (<ref>) is negligible and we complete the proof. 














Â§ PROBABILISTIC PREPARATIONS

In this section, we state some probabilistic results that we need to use later. The proof can be found in <cit.> (with at most very mild straightforward modification). 


 Â§.Â§ Mean square calculation

We first state results on mean square calculations.


Let f be a Steinhaus random multiplicative function. Then for any 400<xâ‰¤ y and Ïƒ>-1/log y, we have

    [âˆ_x<pâ‰¤ y |1-f(p)/p^1/2+Ïƒ|^-2] = exp( âˆ‘_x<pâ‰¤ y1/p^1+2Ïƒ + O(1/âˆš(x)log x) ).



The proof is basically using the Taylor expansion and the orthogonality deduced from the definition of a Steinhaus random multiplicative function. See <cit.>. 


We also quote the following result on two-dimensional mean square calculations. This will be used in proving the lower bound in TheoremÂ <ref>. 

   Let f be a Steinhaus random multiplicative function. Then for any 400<xâ‰¤ y and Ïƒ>-1/log y, we have
 
    [âˆ_x<pâ‰¤ y |1-f(p)/p^1/2+Ïƒ|^-2 |1-f(p)/p^1/2+Ïƒ+it|^-2 ] = exp( âˆ‘_x<pâ‰¤ y2+2cos(tlog p)/p^1+2Ïƒ + O(1/âˆš(x)log x) )ÌŠ.
  
Moreover, if x>e^1/|t|, then we further have 

    = exp( âˆ‘_x<pâ‰¤ y2/p^1+2Ïƒ + O(1) ) .



The proof of (<ref>) is in <cit.>. To deduce (<ref>), we only need to show the contribution involves cos(tlog p) terms are â‰ª 1, which follows from a strong form of prime number theorem. See how it is done in <cit.> and <cit.>. 




 Â§.Â§ Gaussian random walks and the ballot problem

A key probabilistic result used in Harper's method is the following (modification of) a classical result about Gaussian random walks, which is connected to the â€œballot problem". 


    Let a â‰¥ 1. For any integer n > 1, let G_1, â€¦ , G_n be independent real
Gaussian random variables, each having mean zero and variance between 1/
20 and 20, say. Let
h be a function such that |h(j)| â‰¤ 10 log j. Then

    (âˆ‘_m=1^j G_m â‰¤ a + h(j),   âˆ€ 1â‰¤ jâ‰¤ n) â‰min{1, a/âˆš(n)}.


Without the term h(j), it is a classical result and actually that is all we need in this paper. However, we state this stronger form as the h(j) term can be crucial if one wants to remove the logloglog x factor in TheoremÂ <ref>. We expect the random sum is fluctuating on the order of âˆš(j) (up to step j) and so the above result is expected to be true. The quantity h(j) is much smaller compared to âˆš(j) so it is negligible in computing the probability. 

We do not directly use the above lemma. We shall use an analogous version for random Euler products (PropositionÂ <ref>). We do the 
Girsanov-type calculation in our study. As in <cit.>, we introduce the probability measure (here x is large and |Ïƒ|â‰¤ 1/100, say)

    (A) : = [1_A âˆ_pâ‰¤ x^1/e |1-f(p)/p^1/2+Ïƒ|^-2  ]/[âˆ_pâ‰¤ x^1/e |1-f(p)/p^1/2+Ïƒ|^-2] .

For each â„“âˆˆâ„•âˆª{0}, we denote the â„“-th increment of the Euler product 

    I_â„“(s):= âˆ_x^e^-(â„“+2)<pâ‰¤ x^e^-(â„“+1) (1-f(p)/p^s)^-1.

Since we are restricted to R-rough numbers n,  the parameter â„“ lies in the range 0â‰¤â„“â‰¤loglog x - loglog R. All the rest setup is exactly the same as in <cit.>. 


There is a large natural number B such that the following is true.
Let nâ‰¤loglog x - loglog R - (B+1), and define the decreasing sequence (â„“_j)_j=1^n of non-negative integers by â„“_j = âŒŠloglog x -loglog R âŒ‹ -(B+1) - j. Suppose that |Ïƒ|â‰¤1/e^B+n+1, and
that (t_j)_j=1^n is a sequence of real numbers satisfying |t_j|â‰¤1/j^2/3 e^B+j+1 for all j.

Then uniformly for any large a and any function h(n) satisfying |h(n)| â‰¤ 10 log n, and with I_â„“(s) defined as in (<ref>), we have  

    (-a -Bj â‰¤âˆ‘_m=1^jlog |I_â„“_m (1/2+Ïƒ + it_m)| â‰¤ a + j + h(j),   âˆ€ jâ‰¤ n ) â‰min{1, a/âˆš(n)} .


One may view the above sum approximately as a sum of j independent random variables and each with mean â‰ˆâˆ‘_x^e^-(â„“+2)<pâ‰¤ x^e^-(â„“+1)1/pâ‰ˆ 1 and with constant variance between 1/20 and 20. This shows the connection to LemmaÂ <ref>. The deduction of PropositionÂ <ref> from LemmaÂ <ref> can be found in the proof of <cit.>. The only modification is changing the upper bound restriction from n â‰¤loglog x -(B+1) to n â‰¤loglog x - loglog R -(B+1) and all conditions remaining satisfied. 




Â§ PROOF OF PROPOSITIONÂ <REF>

The proof follows closely to the proof of <cit.>. 
For any integer 0â‰¤ k â‰¤ğ’¦= âŒŠlogloglog x âŒ‹, let 

    I_k: =(x_k+1, x_k] :=  (x^e^-(k+1) , x^e^-k].

Let P(n) be the largest prime factor of n. For simplicity, we use _n to denote the sum where the variable n is R-rough.
By using Minkowski's inequality (as 2qâ‰¥ 1), 

    âˆ‘_nâˆˆ f(n)_2qâ‰¤âˆ‘_0â‰¤ k â‰¤ğ’¦_nâ‰¤ x 
     P(n)âˆˆ I_k f(n)_2q + _nâ‰¤ x
    P(n)â‰¤ x^e^-(ğ’¦+1) f(n)_2q
    .

We first bound the last term by only using the smoothness condition and it is bounded by 
â‰¤Î¨ (x, x^1/loglog x  )^1/2â‰ªâˆš(x) (log x)^-clogloglog  x, which is acceptable. 
The main contribution to the upper bound in (<ref>) can be written as

    = âˆ‘_0â‰¤ k â‰¤ğ’¦âˆ‘_mâ‰¤ x 
     p|m  p âˆˆ I_kf(m) _nâ‰¤ x/m 
     nÂ is x_k+1-smooth f(n) _2q.

We now condition on f(p) for p small but at least R. Write ^(k) to denote the expectation conditional on (f(p))_pâ‰¤ x_k+1. Then the above is

    = âˆ‘_0â‰¤ k â‰¤ğ’¦ (^(k) [|âˆ‘_mâ‰¤ x
     p|m  p âˆˆ I_k f(m) _nâ‰¤ x/m
     nÂ is x_k+1-smooth f(n)|^2q])^1/2q
       â‰¤âˆ‘_0â‰¤ k â‰¤ğ’¦ ([(^(k) [|âˆ‘_mâ‰¤ x
     p|m  p âˆˆ I_kf(m) _nâ‰¤ x/m
     nÂ is x_k+1-smooth f(n)|^2])^q])^1/2q
        = âˆ‘_0â‰¤ k â‰¤ğ’¦ ( [( âˆ‘_mâ‰¤ x 
     p|m  pâˆˆ I_k  |_nâ‰¤ x/m
     nÂ is x_k+1-smooth f(n)|^2)^q] )^1/2q.

Then we only need to show that for each expectation in the sum, it is bounded as in (<ref>). Replace the discrete mean value with a smooth version. Set X=e^âˆš(log x), and we have the expectation involving primes in I_k is 

    â‰ª[(âˆ‘_mâ‰¤ x 
     p|m  pâˆˆ I_kX/mâˆ«_m^m(1+1/X) |_nâ‰¤ x/t
     nÂ is x_k+1-smooth f(n)|^2 dt )ÌŠ^q]ÌŠ
        + [( âˆ‘_mâ‰¤ x 
     p|m  pâˆˆ I_kX/mâˆ«_m^m(1+1/X) |_x/t â‰¤ nâ‰¤ x/m
    nÂ is x_k+1-smooth f(n)|^2 dt )ÌŠ^q]ÌŠ.

By using HÃ¶lder's inequality, we upper bound the second term in (<ref>) by the q-th power of

    âˆ‘_mâ‰¤ x 
     p|m  pâˆˆ I_kX/mâˆ«_m^m(1+1/X) [|_x/t â‰¤ nâ‰¤ x/m
    nÂ is x_k+1-smooth f(n)|^2] dt .

Do the mean square calculation (<ref>) and throw away the restriction on the R-rough numbers. Then (<ref>) is at most â‰ª  2^-e^k x/log x and thus the second term in (<ref>) is â‰ª(2^-e^k x/log x)^q. Summing over kâ‰¤ğ’¦, this is acceptable and thus we only need to focus on the first term in (<ref>). By swapping the summation, it is at most

    [ ( âˆ«_x_k+1^x |_nâ‰¤ x/t 
     nÂ is x_k+1-smoothf(n) |^2âˆ‘_t/(1+1/X)â‰¤ m â‰¤ t
     p|m  pâˆˆ I_k  X/m dt)ÌŠ^q]ÌŠ.

We upper bound the sum over m by dropping the prime divisibility condition and using a simple sieve argument to derive that the above is at most 

    [  ( âˆ«_x_k^x |_nâ‰¤ x/t 
     nÂ is x_k+1-smoothf(n) |^2dt/log t)ÌŠ^q]ÌŠ = x^q[  ( âˆ«_1^x/x_k+1 |_nâ‰¤ z 
     nÂ is x_k+1-smoothf(n)|^2dz/z^2log(x/z))ÌŠ^q]ÌŠ,

where in the equality above we used the substitution z: =x/t. 
A simple calculation shows that we can replace log(x/z) by log x without much loss. Indeed, if zâ‰¤âˆš(x) then log(x/z)â‰«log x; if âˆš(x)â‰¤ z â‰¤ x/x_k+1 then log (x/z) â‰¥ z^-2k/log xlog x.  Thus, we further have the bound

    â‰ª(x/log x)ÌŠ^q[ ( âˆ«_1^x/x_k+1 |_nâ‰¤ z 
     nÂ is x_k+1-smoothf(n)|^2dz/z^2-2k/log x)ÌŠ^q]ÌŠ .

To this end, we apply the following version of Parseval's identity, and its proof can be found in <cit.>.


    Let (a_n)_n=1^âˆ be any sequence of complex numbers, and let A(s): = âˆ‘_n=1^âˆa_n/n^s denote the corresponding Dirichlet series, and Ïƒ_c denote its abscissa of convergence. Then for any Ïƒ> max{0, Ïƒ_c}, we have 
    
    âˆ«_0^âˆ|âˆ‘_nâ‰¤ xa_n|^2/x^1+2Ïƒdx = 1/2Ï€âˆ«_-âˆ^+âˆ|A(Ïƒ + it)/Ïƒ + it|^2 dt.


Apply LemmaÂ <ref> and the expectation in (<ref>) is

    =
    [  (âˆ«_-âˆ^+âˆ|(1/2-k/log x +it)|^2/|1/2-k/log x +it|^2 dt)ÌŠ^q]ÌŠâ‰¤âˆ‘_nâˆˆ[( âˆ«_n-1/2^n+1/2|(1/2-k/log x +it)|^2/|1/2-k/log x +it|^2 dt )ÌŠ^q]ÌŠ.

Since f(m)m^it has the same law as f(m) for all m, for any fixed n we have

    [(âˆ«_n-1/2^n+1/2 |(1/2-k/log x +it)|^2 dt )ÌŠ^q]ÌŠ = [ (âˆ«_-1/2^1/2 |(1/2-k/log x +it)|^2 dt )ÌŠ^q]ÌŠ.

For n-1/2â‰¤ tâ‰¤ n+1/2, we have 
1/|1/2-k/log x +it|^2â‰ 1/n^2 which is summable over n. 
We complete the proof by inserting the above estimates into (<ref>). 








Â§ PROOF OF PROPOSITIONÂ <REF>


This is the key part of the proof that reveals how loglog R â‰ˆâˆš(loglog x) could become the transition range. 
We begin with a discretization process which is the same as in <cit.>.  For each |t|â‰¤1/2, set t(-1)=t, and then iteratively for each 0â‰¤ j â‰¤log(log x /log R) -2 define 

    t(j): = max{uâ‰¤ t(j-1): u = n/((log x) /e^j+1)log ((log x) /e^j+1)Â for some nâˆˆ}.

By the definition, we have <cit.>

    |t-t(j)|â‰¤2/((log x /e^j+1)log ((log x)/e^j+1).

Given this notation, let B be the large fixed natural number from PropositionÂ <ref>. Let ğ’¢(k) denote the event that for all |t|â‰¤1/2 and for all kâ‰¤ j â‰¤loglog x - loglog R -B -2, we have

    (log x/e^j+1log R e^C(x)  )^-1â‰¤âˆ_â„“ = j ^âŒŠloglog x -loglog R âŒ‹-B-2
    |I_â„“(1/2-k/log x +it(â„“)) | â‰¤log x/e^j+1log R e^C(x),

where notably, our C(x) is chosen as the 





    C(x):=loglog R + 100 logloglog x.

We shall establish the following two key propositions. The first proposition says that when we are restricted to the good event ğ’¢(k), the q-th moment is small. 

Let x be large and loglog R â‰ª (loglog x)^1/2. Let 
C(x) be defined as in (<ref>). Let F_k^(R) be defined as in (<ref>) and ğ’¢(k) be defined as in (<ref>). For all 0â‰¤ k â‰¤ğ’¦ = âŒŠlogloglog xâŒ‹ and 1/2â‰¤ q â‰¤ 9/10, we have 

    [(âˆ«_-1/2^1/21_ğ’¢(k) |F_ k^(R)(1/2 - k/log x + it)|^2dt)ÌŠ^q]ÌŠâ‰ª( log x /e^klog R)ÌŠ^q ( C(x)/âˆš(loglog x))^q.


The second proposition is to show that indeed 1_ğ’¢(k) happens with high probability. 


Let ğ’¢(k) be defined as in (<ref>). For all 0â‰¤ k â‰¤ğ’¦= âŒŠlogloglog xâŒ‹ and uniformly for all 1/2â‰¤ q â‰¤ 9/10 and
C(x) defined in (<ref>), we have

    (ğ’¢(k)Â fails) â‰ª e^-C(x).
 

The above two key propositions imply PropositionÂ <ref>. 

   According to the good event ğ’¢(k) happening or not, we have 
   
    [(âˆ«_-1/2^1/2  |F_ k^(R)(1/2 - k/log x + it)|^2dt)ÌŠ^q]ÌŠ
       â‰¤[(âˆ«_-1/2^1/21_ğ’¢(k) |F_ k^(R)(1/2 - k/log x + it)|^2dt)ÌŠ^q]ÌŠ +   [(âˆ«_-1/2^1/21_ğ’¢(k)fails |F_ k^(R)(1/2 - k/log x + it)|^2dt)ÌŠ^q]ÌŠ
       â‰¤( log x /e^klog R)ÌŠ^q ( C(x)/âˆš(loglog x))^q +   (âˆ«_-1/2^1/2 [|F_ k^(R)(1/2 - k/log x + it)|^2]dt )ÌŠ^q(ğ’¢(k)Â fails)^1-q,

  where in the first term we used PropositionÂ <ref> and we applied HÃ¶lder's inequality with exponents 1/q, 1/1-q to get the second term. We next apply the mean square calculation (<ref>) to derive that the above is 
    
    â‰ª( log x /e^klog R)ÌŠ^q(  ( C(x)/âˆš(loglog x))^q +  (ğ’¢(k)Â fails)^1-q)ÌŠ.

  Plug in the definition of C(x) and use PropositionÂ <ref> with 1-q â‰¥ 1/10 (and then the exceptional probability to the power 1/10 is negligible) to deduce that 
  
    â‰ª   e^-k/2( log x /log R)ÌŠ^qÂ· ( C(x)/âˆš(loglog x))^q,

  which completes the proof. 



We remark that in (<ref>), the quantity C(x)= loglog R +100 logloglog x is different from just being a constant C in <cit.>. The reason for our choice of C(x) is the following. Firstly, to keep the q-th moment in PropositionÂ <ref> has a saving (i.e. to make ( C(x)/âˆš(loglog x))^q small), we require that C(x)= o(âˆš(loglog x)).  Secondly, it turns out that in order to make the exceptional probability in PropositionÂ <ref> small enough, one has the constraint loglog R â‰ª C(x). The combination of the above two aspects together leads to loglog R =o(âˆš(loglog x)). 




   In the deduction of PropositionÂ <ref>, we did not use an iterative process as used in <cit.>. Instead, we added an extra term 100logloglog x for the purpose of getting strong enough bounds on (ğ’¢(k)Â fails). We simplified the proof by getting a slightly weaker upper bound in TheoremÂ <ref> as compensation.





 Â§.Â§ Proof of PropositionÂ <ref>



The proof of PropositionÂ <ref> is a simple modification of the proof of Key Proposition 1 in <cit.>. We emphasize again the main difference is instead of using a large constant C as in <cit.> but replacing it with C(x) defined in (<ref>), and we do not need the extra help from the quantity h(j) which hopefully makes the proof conceptually easier.  

By using HÃ¶lder's inequality, it suffices to prove that 

    [âˆ«_-1/2^1/2 |F_k^(R)(1/2-k/log x + it)|^2dt ]â‰ª e^-kÂ·log x/log RÂ·C(x)/âˆš(loglog x) ,

uniformly for 0â‰¤ k â‰¤ğ’¦ = âŒŠlogloglog x âŒ‹ and 1/2 â‰¤ qâ‰¤ 9/10. 
We can upper bound the left-hand side of (<ref>) by 

    â‰¤âˆ«_-1/2^1/2[ |F_k^(R)(1/2-k/log x + it)|^2] dt

where  is the event that 

    (log x/e^j+1log R e^C(x)  )^-1â‰¤âˆ_â„“ = j ^âŒŠloglog x -loglog R âŒ‹-B-2
    |I_â„“(1/2-k/log x +it(â„“)) | â‰¤log x/e^j+1log R e^C(x)

for all kâ‰¤ j â‰¤loglog x -loglog R -B -2. This is an upper bound as  is the event of  holds for all |t|â‰¤1/2. By the fact that f(n) has the same law as f(n)n^it, we have

    âˆ«_-1/2^1/2[ |F_k^(R)(1/2-k/log x + it)|^2] dt = âˆ«_-1/2^1/2[ |F_k^(R)(1/2-k/log x )|^2 ]dt,

where  denotes the event that 

    (log x/e^j+1log R e^C(x)  )^-1â‰¤âˆ_â„“ = j ^âŒŠloglog x -loglog R âŒ‹-B-2
    |I_â„“(1/2-k/log x +i(t(â„“)-t)) | â‰¤log x/e^j+1log R e^C(x),

for all kâ‰¤ j â‰¤loglog x- loglog R - B -2. 
 We next apply PropositionÂ <ref>. 






It is clear that â„‹(k,t) is the event treated in PropositionÂ <ref> with n=âŒŠloglog x- loglog R âŒ‹ -(B+1)-k; Ïƒ= k/log x and t_m = t(âŒŠloglog x- loglog R âŒ‹ -(B+1) -m)-t for all m; and 

    a = C(x) + B+1,    h(j)=0.

The parameters indeed satisfy |Ïƒ|â‰¤1/e^B+n+1 and |t_m|â‰¤1/m^2/3e^B+m+1
for all m. Apply PropositionÂ <ref> to derive

    [ |F_k^(R)(1/2-k/log x)|^2]/[|F_k^(R)(1/2-k/log x)|^2] = (â„‹(k,t)) â‰ªmin{ 1, a/âˆš(n)}.

A simple mean square calculation (see (<ref>)) gives that 

    [|F_k^(R)(1/2-k/log x)|^2] = exp(âˆ‘_Râ‰¤ p â‰¤ x^e^-(k+1)1/p^1-2k/log x +O(1))ÌŠâ‰ªlog x/e^klog R.

Combining the above two inequalities and the relation in (<ref>), we get the desired upper bound for the quantity in (<ref>). Thus, we complete the proof of (<ref>) and PropositionÂ <ref>.  






 Â§.Â§ Proof of PropositionÂ <ref>

In the proof, we will see why it is necessary to make C(x) large enough compared to loglog R. The proof starts with the union bound. We have 

    (ğ’¢(k)Â fails) â‰¤_1 +_2,

where

    _1 = âˆ‘_kâ‰¤ j â‰¤log (log x/log R) -B-2( âˆ_â„“ = j ^âŒŠlog (log x/log R) âŒ‹-B-2
    |I_â„“(1/2-k/log x + i t(â„“)) | >log x/e^j+1log R e^C(x)Â for some t)ÌŠ

and 

    _2 = âˆ‘_kâ‰¤ j â‰¤log (log x/log R) -B-2( âˆ_â„“ = j ^âŒŠlog (log x/log R) âŒ‹-B-2
    |I_â„“(1/2-k/log x +i t(â„“)) |^-1 >log x/e^j+1log R e^C(x)Â for some t)ÌŠ,

where |t|â‰¤ 1/2. 
We focus on bounding _1, and _2 can be estimated similarly. Replace the set of all |t|â‰¤ 1/2 by the discrete set 

    ğ’¯(x, j): = {n/((log x)/e^j+1) log ((log x)/e^j+1) : |n|â‰¤ ((log x)/e^j+1) log ((log x)/e^j+1)  }ÌŠ,

and apply the union bound to get 

    _1 = âˆ‘_kâ‰¤ j â‰¤log (log x/log R) -B-2
     t(j) âˆˆğ’¯(x,j)( âˆ_â„“ = j ^âŒŠlog (log x/log R) âŒ‹-B-2
    |I_â„“(1/2-k/log x +it(â„“)) | >log x/e^j+1log R e^C(x))ÌŠ.

By using Chebyshev's inequality this is at most 

    â‰¤âˆ‘_kâ‰¤ j â‰¤log (log x/log R) -B-2
     t(j) âˆˆğ’¯(x,j)1/(log x/e^j+1log R e^C(x))^2[ âˆ_â„“ = j ^âŒŠlog (log x/log R) âŒ‹-B-2
    |I_â„“(1/2-k/log x +it(â„“)) |^2  ].

Since f(n) and f(n)n^it have the same law, the above is 

    â‰ªâˆ‘_kâ‰¤ j â‰¤log (log x/log R) -B-2| ğ’¯(x,j)|/(log x/e^j+1log R e^C(x))^2[ âˆ_â„“ = j ^âŒŠlog (log x/log R) âŒ‹-B-2
    |I_â„“(1/2-k/log x ) |^2  ].

The expectation here is, again through a mean square calculation (<ref>), â‰ªlog x/e^j+1log R. Note |ğ’¯(x, j)| â‰¤ ((log x)/e^j+1) log ((log x)/e^j+1). 
We conclude that 

    _1 â‰ªâˆ‘_kâ‰¤ j â‰¤log (log x/log R) -B-2e^loglog R-2C(x) + loglog ( log x / e^j+1)   â‰ª e^-C(x),

where in the last step we used that C(x)=   loglog R + 100 logloglog x. Thus we complete the proof of PropositionÂ <ref>. 






Â§ PROOF OF THEOREMÂ <REF>

 We first notice that if R>x^1/A for any fixed large constant A, then _R(x) is a set of elements with only O_A(1) number of prime factors. This would immediately imply that [|âˆ‘_nâˆˆ f(n)|^4] â‰ª_A ||^2 and by (<ref>), the conclusion follows. From now on, we may assume that 

    Râ‰¤ x^1/A.

The proof strategy of TheoremÂ <ref> again follows from <cit.>. The main differences lie in the design of the barrier events and taking advantage of R being large.  In particular, we do not need a â€œtwo-dimensional Girsanov-type" calculation which makes our proof less technical. 
We first do the reduction step to reduce the problem to understanding certain averages of random Euler products, as in the upper bound proof. 

There exists a large constant C such that the following is true. Let x be large and loglog R â‰«âˆš(loglog x). Let F^(R)(s) be defined as in (<ref>). Then,
uniformly for all 1/2 â‰¤ qâ‰¤ 9/10 and any large V, we have âˆ‘_nâˆˆ f(n)  _2q

    â‰«âˆš(x/log x)( âˆ«_-1/2^1/2 | F^(R)(1/2 +4V/log x + it)|^2 dt_q^1/2  - C/e^Vâˆ«_-1/2^1/2 | F^(R)(1/2 +2V/log x + it)|^2 dt_q^1/2 -C ).



The remaining  tasks are to give a desired lower bound on F^(R)(1/2 +4V/log x + it)_q^1/2 and  an upper bound on F^(R)(1/2 +2V/log x + it)_q^1/2. 
The upper bound part is simple. Indeed, simply apply HÃ¶lder's inequality and do a mean square calculation (<ref>) to get

    [(âˆ«_-1/2^1/2|F^(R)(1/2 +2V/log x + it)|^2  dt)^q] â‰ª(âˆ«_-1/2^1/2[|F^(R)(1/2 +2V/log x + it)|^2]  dt )^qâ‰ª(log x/Vlog R)^q .


We next focus on the main task, giving a good lower bound on F^(R)(1/2 +4V/log x + it)_q^1/2. For each tâˆˆâ„, 
we use L(t) denote the event for all âŒŠlog V âŒ‹ +3 â‰¤ j â‰¤loglog x - loglog R -B -2, the following holds

    (log x/e^j+1log R e^D(x)  )^-Bâ‰¤âˆ_â„“ = j ^âŒŠloglog x -loglog R âŒ‹-B-2
    |I_â„“(1/2+4V/log x +it) | â‰¤log x/e^j+1log R e^D(x),

where D(x):= câˆš(loglog x -loglog R) with 

    c= 1/4min{loglog R/âˆš(loglog x-loglog R) , 1 }â‰ 1.

We are now ready to define a random set 

    â„’: = {-1/2â‰¤ t â‰¤ 1/2: L(t)Â defined by (<ref>) holds}.

It is clear that

    [(âˆ«_-1/2^1/2|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] â‰¥[(âˆ«_â„’|F^(R)(1/2 +4V/log x + it)|^2  dt)^q].

We use the following estimate and defer its proof to SectionÂ <ref>.

Let x be large and loglog R â‰«âˆš(loglog x). Let F^(R)(s) be defined as in (<ref>) and V be a large constant. Let â„’ be the random set defined in (<ref>). Then uniformly for any 1/2â‰¤ qâ‰¤ 9/10, we have

    [(âˆ«_â„’|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] â‰«(log x/Vlog R)^q .


Plug (<ref>), (<ref>) and (<ref>) into PropositionÂ <ref> with q=1/2
(and choosing V to be a sufficiently large fixed constant so that C/e^V kills the implicit constant) to get that 
 
    [|âˆ‘_nâˆˆ_R(x) f(n) |] â‰«âˆš(|_R(x)|) .

This completes the proof of TheoremÂ <ref>.




Â§ PROOF OF PROPOSITIONÂ <REF>


The proof proceeds the same as in <cit.> (see also <cit.>) and we provide a self-contained proof here and highlight some small modifications. 

Let P(n) denote the largest prime factor of n as before. We have assumed that (<ref>) holds, e.g. Râ‰¤âˆš(x) (This restriction is not crucial but makes the notation later easier). Let Ïµ denote a Rademacher random variable independent of f(n), and recall that  indicates that the variable n under the summation is R rough. For 1/2â‰¤ qâ‰¤ 9/10, we have 

    [|_nâ‰¤ x
     P(n)>âˆš(x)f(n) |^2q]     = 1/2^2q[|_nâ‰¤ x
     P(n)â‰¤âˆš(x)f(n) + _nâ‰¤ x
     P(n)>âˆš(x)f(n)+_nâ‰¤ x
     P(n)>âˆš(x)f(n)-_nâ‰¤ x
     P(n)â‰¤âˆš(x)f(n)|^2q]
       â‰¤[|_nâ‰¤ x
     P(n)â‰¤âˆš(x)f(n) + _nâ‰¤ x
     P(n)>âˆš(x)f(n)|^2q] + [|_nâ‰¤ x
     P(n)>âˆš(x)f(n)-_nâ‰¤ x
     P(n)â‰¤âˆš(x)f(n)|^2q]
        = 2[|Ïµ_nâ‰¤ x
     P(n)>âˆš(x)f(n) + _nâ‰¤ x
     P(n)â‰¤âˆš(x)f(n)|^2q] = 2[|_nâ‰¤ x f(n)|^2q],

where the last step we used the law of 
    Ïµ_nâ‰¤ x
     P(n)>âˆš(x)f(n)= Ïµâˆ‘_âˆš(x)<pâ‰¤ x f(p) _mâ‰¤ x/p f(m)
 conditional on (f(p))_Râ‰¤ p â‰¤âˆš(x) is the same as the law of _nâ‰¤  x
 P(n)>âˆš(x)f(n). By the above deduction, it suffices to give a lower bound on _nâ‰¤ x
 P(n)>âˆš(x)f(n)_2q. 
Do the decomposition

    _nâ‰¤ x
     P(n)>âˆš(x)f(n) =âˆ‘_âˆš(x)â‰¤ p â‰¤ x f(p) _mâ‰¤ x/pf(m).

The inner sum is determined by (f(p))_Râ‰¤ pâ‰¤âˆš(x) and apply the Khintchine's inequality <cit.> to get

    [|_nâ‰¤ x
     P(n)>âˆš(x)f(n)|^2q] â‰«[(âˆ‘_âˆš(x)< p â‰¤ x |_mâ‰¤ x/pf(m)|^2 )^q] â‰¥1/(log x)^q[(âˆ‘_âˆš(x)< p â‰¤ xlog pÂ·|_mâ‰¤ x/pf(m)|^2)^q].

Next, do the smoothing step as we did in the upper bound case. Again set X = e^âˆš(log x). 
Write 

    âˆ‘_âˆš(x)< pâ‰¤ xlog p Â· |_mâ‰¤ x/pf(m)|^2 = âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X) |_mâ‰¤ x/p f(m)|^2dt.

One has |a+b|^2â‰¥ a^2/4 - min{|b|^2, |a/2|^2}â‰¥ 0 and thus the above is at least 

    1/4âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X) |_mâ‰¤ x/t f(m)|^2dt 
        -âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X)min{|_x/tâ‰¤ mâ‰¤ x/p f(m)|^2, 1/4 |_mâ‰¤ x/t f(m)|^2}.

It follows that the quantity we are interested in has the lower bound

    [|_nâ‰¤ x
     P(n)>âˆš(x)f(n)|^2q] â‰¥   1/(log x)^q[(1/4âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X) |_mâ‰¤ x/t f(m)|^2dt)^q] 
        - 1/(log x)^q[(âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X) |_ x/t<mâ‰¤ x/p f(m)|^2dt)^q].

Use HÃ¶lder's inequality and throw away the R-rough condition to upper bound the subtracted term in (<ref>) by

    â‰¤1/(log x)^q(âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X)[ |âˆ‘_ x/t<mâ‰¤ x/p f(m)|^2]dt)ÌŠ^q
       â‰ª1/(log x)^q(âˆ‘_âˆš(x)<pâ‰¤ x log p Â·  (x/pX +1) )^qâ‰ª1/(log x)^q(xlog x/X +x)^qâ‰ª (x/log x)^q.

The first term in (<ref>) (without the factor 1/4(log x)^q) is 

    [(âˆ‘_âˆš(x)<pâ‰¤ xlog p Â·X/pâˆ«_p^p(1+1/X) |_mâ‰¤ x/t f(m)|^2dt)^q] 
        = [(âˆ«_âˆš(x)^x_t/1+1/X <pâ‰¤ t log p Â·X/p|_mâ‰¤ x/tf(m)|^2 dt )^q]
       â‰« [(âˆ«_âˆš(x)^x|_mâ‰¤ x/tf(m)|^2dt )^q] = x^q[(âˆ«_1^âˆš(x)  |_mâ‰¤ z f(m) |^2dz/z^2)^q].

To this end, we impose the smooth condition to invert the sums to Euler products. We have for any large V, 

    [(âˆ«_1^âˆš(x)  |_mâ‰¤ z f(m) |^2dz/z^2)^q] â‰¥[(âˆ«_1^âˆš(x)  |_mâ‰¤ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]
       â‰¥[(âˆ«_1^+âˆ  |_mâ‰¤ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]- [(âˆ«_âˆš(x)^+âˆ  |_mâ‰¤ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]
       â‰¥[(âˆ«_1^+âˆ  |_mâ‰¤ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]- 1/e^2Vq[(âˆ«_1^+âˆ  |_mâ‰¤ z
     x-smooth f(m) |^2dz/z^2+4V/log x)^q].

Apply LemmaÂ <ref> to get that the first term is 

    â‰«[(âˆ«_-1/2^1/2|F^(R)(1/2 + 4V/log x + it)|^2 dt )^q].

For the second term, an application of LemmaÂ <ref> gives

    â‰ª e^-2Vq[(âˆ«_-âˆ^+âˆ|F^(R)(1/2 + 2V/log x + it)/|1/2 + 2V/log x + it|^2 dt )^q] â‰ª e^-2Vq[(âˆ«_-1/2^1/2 |F^(R)(1/2 + 2V/log x + it)|^2 )^q]

where in the last step we used the fact that f(n)n^it has the same law as f(n) and âˆ‘_n â‰¥ 1 n^-2 converges. Bounds in (<ref>) and (<ref>) together give the desired bound for the first term in (<ref>) and we complete the proof. 





Â§ PROOF OF PROPOSITIONÂ <REF>

In this section, 
we prove PropositionÂ <ref>. The proof significantly relies on the following proposition, which is a mean value estimate of the product of |F^(R)(Ïƒ + it_1)|^2
and |F^(R)(Ïƒ + it_2)|^2. Our upper bound matches the guess if you pretend the two products are independent. 

Let x be large and loglog R â‰«âˆš(loglog x). Let F^(R)(s) be defined as in (<ref>) and V be a large constant. Let â„’ be the random set defined in (<ref>). Then we have
    
    [(âˆ«_â„’|F^(R)(1/2 +4V/log x + it)|^2  dt)^2]  â‰ª (log x/Vlog R)^2.






The proof starts with an application of HÃ¶lder's inequality. We have 

    [(âˆ«_â„’|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] â‰¥([âˆ«_â„’|F^(R)(1/2 +4V/log x + it)|^2  dt])^2-q/([(âˆ«_â„’|F^(R)(1/2 +4V/log x + it)|^2  dt)^2])^1-q.

PropositionÂ <ref> gives a desired upper bound for the denominator. We next give a lower bound on the numerator. By using that f(n)n^it has the same law as f(n), the numerator is

    (âˆ«_-1/2^1/2 [1_L(t)|F^(R)(1/2 +4V/log x + it)|^2]  dt)^2-q= ([ 1_L(0)|F^(R)(1/2 +4V/log x )|^2] )^2-q .

We next use PropositionÂ <ref> by taking n=âŒŠloglog x - loglog R âŒ‹ - (B+1) - âŒŠlog V âŒ‹, a =D(x)=câˆš(loglog x -loglog R ) and h(j)=0 to conclude that (1_L(0))â‰« 1. Combining with the mean square calculationÂ (<ref>), we have

    [1_L(0)|F^(R)(1/2 +4V/log x + it)|^2] â‰«(1_L(0))Â· [|F^(R)(1/2 +4V/log x + it)|^2] â‰«log x/V log R.

We complete the proof by plugging (<ref>) and (<ref>) into (<ref>). 


The proof of PropositionÂ <ref> is a bit involved and its proof is inspired by <cit.> and <cit.>. We are not using the â€œtwo-dimensional Girsanov-type" computation as used in <cit.> which significantly simplified the proof. We do not expect any further savings when R is as large as stated in PropositionÂ <ref> while for a smaller R, one might expect there could be further cancellation as in <cit.> which may be verified by adapting the â€œtwo-dimensional Girsanov-type" calculation. 



    Expand the square and it equals
    
    [ âˆ«_-1/2^1/21_L(t_1) |F^(R)(1/2 +4V/log x + it_1)|^2 dt_1 âˆ«_-1/2
        ^1/21_L(t_2)|F^(R)(1/2 +4V/log x + it_2)|^2 dt_2   ].

   By using that f(n)n^it has the same law as f(n), we write the above as (t:= t_1-t_2)
   
    âˆ«_-1^1 [1_L(0) |F^(R)(1/2 +4V/log x )|^21_L(t)|F^(R)(1/2 +4V/log x + it)|^2 ] dt.

For |t| large enough, the two factors behave independently, which is the easier case. Indeed,  if |t|>1/log R, drop the indicator functions and bound the corresponding integration by

    â‰ªmax_ 1/log R < |t|â‰¤ 1 [  |F^(R)(1/2 +4V/log x)|^2Â· |F^(R)(1/2 +4V/log x + it)|^2   ]  .
 
 Apply the two dimensional mean square calculation (<ref>) with (x, y)=(R, x)  to conclude that the above is 
 
    â‰ª(log x/V log R)^2.

 
We next focus on the case |t|â‰¤ 1/log R. Since f(p) are independent of each other, we can decompose the Euler products into pieces and analyze their contributions to (<ref>) separately.
Define the following three sets of primes based on the sizes of primes

    ğ’«_1: = {pÂ prime: Râ‰¤ p < x^e^-(âŒŠloglog x -loglog R âŒ‹-B-2)},


    ğ’«_2: = {pÂ prime:  x^e^-(âŒŠloglog x -loglog R âŒ‹-B-2)â‰¤ p â‰¤ x^e^-(âŒŠlog V âŒ‹ +3)},

and 

    ğ’«_3: = {pÂ prime: x^e^-(âŒŠlog V âŒ‹ +3) < p â‰¤ x }.

We proceed as follows. Note that the events L(0) and L(t) are irrelevant to f(p) for pâˆˆğ’«_1 âˆªğ’«_3. For partial products over primes pâˆˆğ’«_1 âˆªğ’«_3, we directly do mean square calculations.
For partial products over primes pâˆˆğ’«_2, we will crucially use the indicator functions 1_L(0) and 1_L(t) defined in (<ref>) with j= âŒŠlog V âŒ‹ +3. This separation gives that the integration in (<ref>) over |t|â‰¤ 1/log R is

    âˆ«_|t|â‰¤1/log R[âˆ_pâˆˆğ’«_1 âˆªğ’«_3  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2] 
    Ã—[1_L(0)1_L(t)âˆ_pâˆˆğ’«_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt.

We first upper bound the expectation over primes in ğ’«_1 âˆªğ’«_3 uniformly over all t. By using independence between f(p) and (<ref>), we can bound it as 

    â‰ªexp( âˆ‘_pâˆˆğ’«_14/p^1+8V/log x + âˆ‘_pâˆˆğ’«_34/p^1+8V/log x).

By simply using the prime number theorem and the definition of ğ’«_1 and ğ’«_3, one has that both sums in (<ref>)  are â‰ª 1 so that (<ref>) is â‰ª 1, where we remind readers that B is a fixed constant. Now our task is reduced to establishing the following

    âˆ«_|t|â‰¤1/log R[1_L(0)1_L(t)âˆ_pâˆˆğ’«_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt â‰ª(log x/V log R)^2.


Our strategy would be, roughly speaking, using the barrier event 1_L(t) to bound certain partial products involved with t directly and then use the mean square calculation to deal with the rest of the products. The exact partial products that we will apply barrier events would depend on the size of t.  

We first do a simple case, which helps us get rid of the very small t, say |t|<V/log x. We use the the condition 1_L(t) and pull out the factors related to L(t) to get that the contribution from |t|<V/log x is at most 

    â‰ªâˆ«_|t|â‰¤V/log x e^2câˆš(loglog x- loglog R)Â· (log x/V log R)^2Â·[1_L(0)âˆ_pâˆˆğ’«_2  |1-f(p)/p^1/2 +4V/log x|^-2]  dt 
       â‰ªV/log xÂ·  e^2câˆš(loglog x- loglog R)Â·(log x/V log R)^2Â·[âˆ_pâˆˆğ’«_2  |1-f(p)/p^1/2 +4V/log x|^-2] 
       â‰ª(log x/V log R)^2,

where in the second last step we dropped the 1_L(0) condition, and in the last step we applied (<ref>) together with log R â‰¥exp(4c  âˆš(loglog x)) where c is defined in (<ref>). Thus we only need to establish the following 

    âˆ«_V/log xâ‰¤ |t|â‰¤1/log R[1_L(0)1_L(t)âˆ_pâˆˆğ’«_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt â‰ª(log x/V log R)^2 .


We now enter the crucial part where we will apply the barrier events according to the size of |t|.
We decompose the set ğ’«_2 into two parts according to |t|.
For each fixed V/log xâ‰¤ |t| â‰¤ 1/log R, we write 

    ğ’«_2 = ğ’®(t) âˆªâ„³(t),

where 

    ğ’®(t):={pÂ prime:  x^e^-(âŒŠloglog x -loglog R âŒ‹-B-2)â‰¤ p â‰¤ e^V/|t|},

and 

    â„³(t):= {pÂ prime:  e^V/|t|â‰¤ p â‰¤ x^e^-(âŒŠlog V âŒ‹ +3)}.

The set of primes ğ’®(t) would be those we will apply barrier events and â„³(t) would be estimated by a mean square calculation. Note that for pâˆˆâ„³(t), there is a nice decorrelation as we needed in (<ref>) due to that pâ‰¥ e^V/|t|. 
Let us now see how such a decomposition of ğ’«_2 would help us. We use a local notation

    G(p, t): = |1-f(p)/p^1/2 + 4V/log x+it|^-2.

Then the quantity in (<ref>) is the same as 

    âˆ«_V/log xâ‰¤ |t|â‰¤1/log R[1_L(0)1_L(t)âˆ_pâˆˆğ’«_2 G(p, 0) âˆ_pâˆˆğ’®(t) G(p, t) âˆ_pâˆˆâ„³(t) G(p, t)  ]  dt.

We apply the barrier events condition 1_L(t) to bound the product over pâˆˆğ’®(t) so that the above is at most 

    â‰ª(V/log R)^2Â· e^2câˆš(loglog x - loglog R)Â·âˆ«_V/log xâ‰¤ |t|â‰¤1/log R1/t^2[1_L(0)âˆ_pâˆˆğ’«_2 G(p, 0) âˆ_pâˆˆâ„³(t) G(p, t)  ]  dt.

We next upper bound the expectation in (<ref>) uniformly for all V/log xâ‰¤ |t| â‰¤ 1/log R. We first drop the indicator function and rewrite the product based on the independence between f(p) to derive that

    [1_L(0)âˆ_pâˆˆğ’«_2 G(p, 0) âˆ_pâˆˆâ„³(t) G(p, t)]â‰¤[ âˆ_pâˆˆğ’®(t) G(p, 0)] Â· [âˆ_pâˆˆâ„³(t) G(p, 0)G(p, t)].

 Use the mean square calculation results in (<ref>) and (<ref>) to further get an upper bound on the expectation
 
    â‰ªV/|t|/log RÂ·(tlog x/V^2)^2â‰ª|t|(log x)^2/V^3log R .

Now we plug the above bound to (<ref>) to get that (<ref>) is crudely bounded by 

    (log x/log R)^2Â·e^2câˆš(loglog x- loglog R)/Vlog RÂ·âˆ«_V/log xâ‰¤ |t|â‰¤1/log R1/|t| dt â‰ª(log x/log R)^2.

In the last step we used that log R â‰¥exp(4c  âˆš(loglog x)) where c is defined in (<ref>).  This completes the proof of (<ref>) and thus the proof of the proposition. 




Â§ CONCLUDING REMARKS




 Â§.Â§ Typical behavior and small perturbations

We give a sketch of the situation when a(n) itself is independently and randomly chosen. 
We write 

    a(n) = r(n) X(n)

where r(n)>0 is deterministic and X(n) are independently distributed with [|X(n)|^2]=1. We may naturally assume that there is some r such that

    r(n) â‰ r(m) â‰ r

for all n, m, i.e. no particular random variable would dominate the whole sum in size. One may also just assume r=1 throughout the discussion here. 
We claim that for typical X(n), the random sums satisfy the sufficient condition established in <cit.> on having a Gaussian limiting distribution.

The key condition one needs to verify is that almost surely (in terms of over X(n)), we have 

    R_N(a) : =âˆ‘_ m_i, n_jâ‰¤ N 
    
     m_iâ‰  n_j 
     m_1m_2=n_1n_2   a(n_1)a(n_2) a(m_1) a(m_2) = o(r^4N^2).

The proof of (<ref>) is straightforward. By using the divisor bound, we know there are â‰ª N^2+Ïµ number of quadruples (m_1, m_2, n_1, n_2) under the summation. If we expect some square-root cancellation among a(n_1)a(n_2) a(m_1) a(m_2), then R_N(a) above should be around r^4N^1+ typically. 
Indeed, by using the fact that all a(n) are independent, we have the L^2 bound 

    [|R_N|^2] = [R_N R_N] â‰ª r^8 N^2+.

This leads to, 
almost surely (in terms of over X(n)), that we have 

    R_N(a) = o(r^4N^2).

To this end, by using <cit.>, almost surely, we have a central limit theorem for the random partial sums of a Steinhaus random multiplicative function.
See <cit.> for a closely related result where they used the method of moments.  




In QuestionÂ <ref>, we asked if it is possible to characterize the choices of a(n) that give better than square-root cancellation. On one hand, as discussed above, we know for typical a(n), there is just square-root cancellation. On the other hand, if a(n) is a deterministic multiplicative function taking values on the unit circle, then by the fact that a(n)f(n) has the same distribution as f(n) and the result established by Harper (<ref>), the partial sums âˆ‘_nâ‰¤ N a(n)f(n) have better than square-root cancellation. Our main theorems study one particular example of multiplicative nature.  Combining these observations, 
we believe that any small perturbation coming from a(n) that destroys the multiplicative structure would make the better than square-root cancellation in (<ref>) disappear. We ask the following question in a vague way as a sub-question of QuestionÂ <ref>.

Is it true that the only â€œessential choice" of a(n) leading to better than square-root cancellation is of multiplicative nature? 




 Â§.Â§ Threshold in other settings and the limiting distribution

The main theorems of this paper prove that there is square-root cancellation for loglog R â‰« (loglog x)^1/2. What is the limiting distribution then? We have remarked earlier that one may establish a central limit theorem when Râ‰«exp((log x)^c) for some  constant c<1 by understanding the corresponding multiplicative energy. It becomes less clear for smaller R. 


 What is the limiting distribution of  âˆ‘_nâˆˆ_R(x) f(n) with â€œproper" normalization, for all ranges of R?   


We finally comment that there is another family of partial sums that naturally has the threshold behavior for better than square-root cancellation. Let = [x, y] with yâ‰¤ x. We would like to know for what range of y, typically, 

    âˆ‘_xâ‰¤ n â‰¤ x+y f(n) = o(âˆš(y)).

We believe one can adapt the argument here to find that the threshold behavior is around log (x/y) â‰ˆâˆš(loglog x). It is certainly interesting to understand the limiting distribution for the short interval case thoroughly, beyond the previous result in <cit.>.  






plain

