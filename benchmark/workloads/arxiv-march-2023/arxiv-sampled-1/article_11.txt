
	
	
	1]Juan Chen
	1]Yingchun Zhou Corresponding author:  yczhou@stat.ecnu.edu.cn
	[1]Key Laboratory of Advanced Theory and Application in Statistics
		and Data Science-MOE, School of Statistics, East China Normal University.
	
	
	Weighted Euclidean balancing for a matrix exposure in estimating causal effect
    [
    
==============================================================================


		In many scientific fields such as biology, psychology and sociology, there is an increasing interest in estimating the causal effect of a matrix exposure on an outcome.  Covariate balancing is crucial in causal inference and both exact balancing and approximate balancing methods have been proposed in the past decades. However, due to the large number of constraints, it is difficult to achieve exact balance or to select the threshold parameters for approximate balancing methods when the treatment is a matrix. To meet these challenges, we propose the weighted Euclidean balancing method, which approximately balance covariates from an overall perspective. This method is also applicable to high-dimensional covariates scenario. Both parametric and nonparametric methods are proposed to estimate the causal effect of matrix treatment and theoretical properties of the two estimations are provided. Furthermore, the simulation results show that the proposed method outperforms other methods in various cases. Finally, the method is applied to investigating the causal relationship between children's participation in various training courses and their IQ. The results show that the duration of attending hands-on practice courses for children at 6-9 years old has a siginificantly positive impact on children's IQ.
	
	Keywords: causal inference, matrix treatment, weighting methods, overall imbalance, observational study. 
	
	

Â§ INTRODUCTION

	For decades, causal inference has been widely used in many fields, such as biology, psychology and economics, etc. Most of the current research is based on univariate treatment (binary, multivalued or continuous treatment)  (<cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>). However, one may be interested in the causal effect of a matrix treatment. For example, in studying the impact of children's participation in training courses on children's intelligence (measured by IQ), the exposure is a matrix, whose rows represent different age groups, columns represent the types of trainging courses and each element represents the number of hours of class per week. The existing methods are not suitable for matrix exposure and there have been few research on this. Therefore, the goal of this paper is to develop a new method to estimate the causal effect function for matrix exposure.
	To estimate causal effects in observational studies, it is common to use propensity scores (<cit.>; <cit.>; <cit.>). There are several classes of propensity score-based methods, such as matching, weighting and subclassification, that have become part of applied researchers' standard toolkit across many scientific displines (<cit.>; <cit.>). In this article we focus on the weighting method. 
	In the past decade, various weighting methods have been proposed to balance covariates in the estimation procedure (<cit.>; <cit.>; <cit.>). The key idea of these methods is to estimate propensity score ( <cit.>; <cit.>; <cit.>; <cit.>).
	
	When using the parametric method to model the propensity score, the estimation bias of the causal effect will be large if the model is mis-specified. Therefore, some nonparametric methods for estimating the propensity score have been proposed, such as the kernel density estimation (<cit.>). In addition, in recent years, some studies have used optimized weighting methods to directly optimize the balance of covariates (<cit.>; <cit.>; <cit.>).
	
	These methods avoid the direct construction of the propensity scores, therefore the obtained estimate achieves higher robustness. One of the methods, the entropy balancing method, has been established as being doubly robust, in that a consistent estimate can still be obtained when one of the two models, either the treatment assignment model or the outcome model, is correctly specified (<cit.>).
	
	Furthermore, this method can be easily implemented by solving a convex optimization problem. Here we extend the entropy balancing method to the matrix treatment scenario to balance the covariates.
	The methods mentioned above assume that all balancing conditions hold exactly, that is, they are exact balancing methods. However, the balancing conditions cannot hold exactly when the dimension of covariate or treatment is high as there will be too many equations to hold simultaneously. For matrix treatment, it is even more difficult for the balancing conditions to hold exactly. To meet this challenge, literatures have shown that approximate balance can trade bias for variance whereas exact balance cannot and the former works well in practice in both low- and high-dimensional settings (<cit.>; <cit.>). The potential limitations of the existing approximate balancing methods are that they directly control univariate imbalance, which cannot guarantee the overall balance especially in the high-dimensional scenario. Besides, there is no principled way to select the threshold parameters simultaneously in practice. Another potential issue of the univariate approximate balancing methods is that it is difficult to handle high-dimensional constraints since the theoretical results require that the number of the balancing constraints should be much smaller than the sample size(<cit.>).
	To alleviate the limitations of univariate balancing methods, we propose an overall balancing approach, which is called Weighted Euclidean balancing method. The weight is obtained by optimizing the entropy function subject to a single inequality constraint, hence the issue of tuning multiple threshold parameters in univariate balancing methods is solved. The Weighted Euclidean distance is defined to measure the overall imbalance and a sufficient small value of the distance suggests that the covariates are approximately balanced from the overall perspective. Moreover, we propose an algorithm to deal with high-dimensional constraints, so that the proposed method is not restrictive to the low-dimensional setting.
	The main contributions of the paper are summarized as follows. First, an overall balancing method (Weighted Euclidean balancing method) is proposed, which extends the binary univariate entropy approximate balancing method to the matrix treatment scenario. Unlike univariate approximate balancing method, the Weighted Euclidean balancing method controls the imbalance  from the overall perspective. Moreover, to the best of our knowledge, it is the first time that matrix treatment is studied by weighting method in causal inference literature. Second, both parametric and nonparametric causal effect estimation methods for matrix treatment are proposed. Under the parametric framework, a weighted optimization estimation is defined and its theoretical properties are provided. Under the nonparametric framework, B-splines are used to approximate the causal effect function and the convergence rate of the estimation is provided. Third, the proposed method is applied to explore the impact of children's participation in training courses on their IQ and meaningful results are obtained.
	The remainder of this article is organized as follows: In Section 2, the preliminaries are introduced. In Section 3, the Weighted Euclidean balancing method (WEBM) is proposed. In Section 4, the theoretical properties of the WEBM method are shown. In section 5, a numerical simulation is performed to evaluate the performance of the WEBM method under finite samples. In Section 6, the WEBM method is applied to analyze a real problem. The conclusions and discussions are summarized in Section 7.
	
	
	
	
	
	

Â§ PRELIMINARY

	

 Â§.Â§ Notation and assumptions

	Suppose an independent and identically distributed sample (ğ™_1,â€¦,ğ™_n) is observed, where the support of ğ™ = (ğ“,ğ—,Y) is ğ’µ=(ğ’¯Ã—ğ’³Ã—ğ’´). Here ğ“âˆˆ R^pÃ— q denotes a matrix exposure, ğ—âˆˆ R^J denotes a vector of covariates, and Y âˆˆ R denotes the observed outcome. Since the causal effect is characterized by potential outcome notion, let Y(t) for all tâˆˆğ’¯ denotes the potential outcome that would be observed under treatment level ğ­, i.e. Y = Y(t) if T= t.  
	
	In this paper, our goal is to estimate the causal effect function ğ”¼(Y(ğ­)), which is defined in terms of potential outcomes that are not directly observed. Therefore, three assumptions that commonly employed for indentification are made (<cit.>; <cit.>).

	
	
	Assumption 1 (Ignorability):

	

	T_i âŠ¥ Y_i(t) |X_i, which implies that the set of observed pre-treatment covariates ğ—_i, is sufficiently rich such that it includes all confounders , i.e. there is no unmeasured confounding.
	

	
	Assumption 2 (Positivity):

	

	f_T|X(T_i = t|X_i ) > 0 for all tâˆˆğ’¯, which means that treatment is not assigned deterministically.

	
	
	Assumption 3 (SUTVA):

	

	Assume that there is no interference among the units, which means that each individual's outcome depends only on their own level of treatment intensity.
	

	
	Under the above assumptions, we first define the stabilized weight as

	
    w_i = f(T_i)/f(T_i |X_i),

	then one can estimate the causal effect function based on the stabilized weight with observational data.
	
	

 Â§.Â§ Exact entropy balancing and approximate entropy balancing for matrix exposure

	The entropy balancing method (<cit.>)
	
	is used to determine the optimal weight for inferring causal effects. It has been used for univariate treatment and here this method is extended to matrix exposure and to balance covariates approximately.	
	Note that the stabilized weight 

	
    w_i = f(T_i)/f(T_i |X_i)

	satisfies the following conditions for any suitable functions u(ğ“) and v(ğ—):
	
    ğ”¼(w_iu(ğ“_i)v(ğ—_i)) =    âˆ¬f(T_i)/f(T_i |X_i)u(ğ“_i)v(ğ—_i)f(ğ“_i, X_i)dT_idX_i 
       =âˆ«{f(T_i)/f(T_i |X_i)u(T_i)f(T_i |X_i)dT_i } v(X_i)f(X_i)dX_i 
       =ğ”¼(u(ğ“_i))ğ”¼(v(ğ—_i))

	Besides, it also satisfies that 
	
    ğ”¼(w_i) = âˆ¬f(T_i)/f(T_i |X_i)f(T_i, X_i)dT_idX_i = 1.

	However, Equation (2) implies an infinite number of moment conditions, which is impossible to solve with a finite sample of observations. Hence, the finite dimensional sieve space is considered to approximate the infinite dimensional function space. Specifically, let 
	
    u_K1(ğ“) = (u_K1,1(ğ“), u_K1,2(ğ“),â€¦, u_K1,K1(ğ“))^', 
    
    		v_K2(ğ—) = (v_K2,1(ğ—), v_K2,2(ğ—), â€¦, v_K2,K2(ğ—))^'

	denote the known basis functions, then 
	
    ğ”¼(w_i u_K1(ğ“_i) v_K2(ğ—_i)^') = ğ”¼(u_K1(ğ“_i))ğ”¼(v_K2(ğ—_i)^').

	
	In practice, the covariate balancing conditions given in Equation (4) cannot hold exactly with high dimensional covariates or treatments. It is even more difficult to hold exactly for matrix exposure. To overcome this difficulty, approximate balance is considered rather than exact balance, which has been demonstrated to work well in practice in both low- and high-dimensional settings (<cit.>; <cit.>; <cit.>). Specifically, the balancing weights that approximately satisfy the conditions in Equation (4) are the global minimum of the following optimization problem:
	
    min_ğ°âˆ‘_i=1^nw_ilog(w_i)

	s.t.
	
    |1/nâˆ‘_i=1^nw_i u_K1,l(ğ“_i)v_K2,lÌƒ(ğ—_i) - (1/nâˆ‘_i=1^n u_K1,l(ğ“_i)) (1/nâˆ‘_i=1^nv_K2,lÌƒ(ğ—_i)) |â‰¤Î´_l,lÌƒ,

	where u_K1,l(ğ“_i) and v_K2,lÌƒ(ğ—_i) denote the lth and lÌƒth components of u_K1(ğ“_i) and v_K2(ğ—_i), respectively.
	Let m_K(ğ“_i, ğ—_i) = vec(1/n u_K1(ğ“_i)v_K2(ğ—_i)^') and mÌ…_K = vec (1/nuÌ…_K1vÌ…_K2^')  denote two column vectors with dimension K, where K= K1 K2, the lth and lÌƒth components of uÌ…_K1 and vÌ…_K2 are defined as
	
    uÌ…_K1,l = 1/nâˆ‘_i=1^n u_K1,l(ğ“_i)  and vÌ…_K2,lÌƒ = 1/nâˆ‘_i=1^n v_K2,lÌƒ(ğ—_i),

	then condition (5) is equivalent to 
	
    min_w âˆ‘_i=1^nw_ilog(w_i)

	s.t.
	
    |âˆ‘_i=1^nw_i m_K,k(ğ“_i, ğ—_i)  -  nmÌ…_K,k|â‰¤Î´_k,  k= 1,â€¦,K.

	However, there is a large number of tuning parameters (Î´_1,â€¦,Î´_K) which is very time-consuming to determine and there is lack of guideline on tuning these parameters simultaneously in practice. 
	
	
	

Â§ METHODOLOGY

	Due to the potential issues of univariate approximate balancing methods, the weighted Euclidean balancing method is proposed in this section, whose key idea is to control the overall imbalance in the optimization problem (7). 
	

 Â§.Â§ Weighted Euclidean balancing method

	
	Define the following weighted Euclidean imbalance measure (WEIM) as a weighted version of the squared Euclidean distance:
	
    WEIM = âˆ‘_k=1^K{Î»_k^2 [âˆ‘_i=1^n w_i (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k)]^2 }.

	The weighted Euclidean balancing obtains the balancing weights that approximately satisfy the condition (4) by solving the following convex optimization problem:
	
    min_w âˆ‘_i=1^nw_ilog(w_i)

	s.t.
	
    âˆ‘_k=1^K{Î»_k^2 [âˆ‘_i=1^n w_i (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k)]^2 }â‰¤Î´,

	where (Î»_1,â€¦, Î»_K) is a pre-sepecified weight vector and Î´â‰¥ 0 is a threshold parameter. Assume that condition (3) holds exactly, whose sample condition is 1/nâˆ‘_i=1^n w_i = 1.
	
	
	Note that univariate exact balance is equivalent to the overall exact balance, in the sense that âˆ‘_i=1^n w_i (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k)=0, k= 1,â€¦,K  is equivalent to WEIM=0. However, the univariate approximate balance does not imply the overall approximate balance since it is possible that âˆ‘_i=1^n w_i (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k), k= 1,â€¦,K is small while the WEIM is large.
	The pre-specified vector  (Î»_1,â€¦, Î»_K) reflects the importance of each univariate constraint. In this paper, we set Î»_k= Ïƒ_k^-1, where Ïƒ_k^2 is the variance of m_K,k(ğ“,ğ—). Since problem (9) is difficult to solve numerically, its dual problem is considered here, which can be solved by numerically efficient algorithms. Theorem 1 provides the dual formulation of problem (9) as an unconstrained problem.
	

	
	Theorem 1.  Assume that max_i (max_k |Î»_km_K,k(ğ“_i,ğ—_i) |) < âˆ, the dual of problem (9) is equivalent to the following unconstrained problem 
	
    min_Î¸âˆˆ R^Kâˆ‘_i=1^nexp( âˆ‘_k=1^KÎ¸_j Î»_j (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k))  + âˆš(Î´)||Î¸||_2,

	
	and the primal solution Åµ_i is given by 
	
    Åµ_i = exp{âˆ‘_k=1^KÎ¸Ì‚_k Î»_k (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k) -1} ,  i=1,â€¦,n,

	where Î¸Ì‚ is the solution to the dual optimization problem (10). 
	The proof of Theorem 1 is in Appendix A.1. 
	
	

  Â§.Â§.Â§ Selection of tuning parameter

	Another practical issue that arises with weighted Euclidean weights is how to choose the degree of approximate balance Î´. A tuning algorithm is proposed as follows. First, determine a range of positive values ğ’Ÿ for Î´, then the optimal value of Î´ is selected by the following algorithm.
	

	
	 Algorithm 1. Selection of Î´.

	For each Î´âˆˆğ’Ÿ,
	
		
  1.  Compute the dual parameters Î¸Ì‚ by solving the dual problem (10);
		
  2.  Compute the estimated weights Åµ_i using equation (11);
		
  3.  Calculate WEIM in (8) using Åµ_i;
	
	Output Î´^* that minimizes WEIM.
	
	

  Â§.Â§.Â§ Weighted Euclidean balancing with high-dimensional covariates

	In the high-dimensional or ultra high-dimensional covariate setting with K relatively large compared to n or K>>n, it becomes difficult to control the overall imbalance using the Weighted Euclidean balancing method. To meet this challenge, we propose an algorithm to select a small subset of the covariates in the sparse setting. Specifically, consider v_K2(ğ—) = (1, ğ—) in the high-dimensional setting. Let Bcor_j be the ball correlation (<cit.>) between X_j and ğ“, j=1,â€¦,L. Rank X_1, â€¦,X_L as X_(1),â€¦,X_(L) such that X_(1) has the largest Bcor value, X_(2) has the second largest Bcor value, and so forth. The covariates are added successively according to the rank of ball correlation until there is a break point of WEIM's, and the set added before the break point appears is the target set. The key idea hings on WEIM, which represents the contribution of the j most imbalanced covariates to the overall imbalance after WEBM weighting.  If WEIM remains stable as j inceases, it indicates that the overall imbalance can be controlled. However, if there is a break point at Step j, it implies that adding the  jth covariates greatly inceases WEIM, which is harmful to the control of the overall imbalance. Therefore, the algorithm should be stopped and print the outputs at Step j-1. Specifically, procedures to select the subset of covariates are given by the following algorithm.

	
	
	 Algorithm 2. Subset selection of covariates in the high dimensional case.

	For each j âˆˆ{ 1,â€¦, L},
	
		
    compute the estimated weights Åµ_i^(j) using v_K2(ğ—) = (1,X_(1),â€¦, X_(j));
		
     calculate WEIM^(j) in (8) using Åµ_i^(j);
		
    add (j,WEIM^(j)) to the x-y plot and observe whether there is a break point at (j,WEIM^(j)):
		
			
    If no, let j=j+1;
			
    If yes, stop and output L_0 = j-1.
		
	
	The selected subset of the covariates is (X_(1),â€¦, X_(L_0)).
	
	
	
	

 Â§.Â§ Causal effect estimation

	
	In this subsection, both parametric and nonparametric approaches are developed to estimate the causal effect function. A weighted optimization estimation is defined under the parametric framework and broadcasted nonparametric tensor regression method (<cit.>) is used to estimate the causal effect function under the nonparametric framework.
	

  Â§.Â§.Â§ Parametric approach

	The causal effect function is parametrized as s(ğ­;Î²), assume that it has a unique solution  Î²^* defined as
	
    Î²^* = agrmin_Î²âˆ«_ğ’¯ğ”¼[Y(t)- s(ğ­;Î²) ]^2f_T(t)dt.

	
	The difficulty of solving Equation (12) is that the potential outcome Y(t) is not observed for all t. Hence, Proposition 1 is proposed to connect the potential outcome with the observed outcome. 

	
	Proposition 1  Under Assumption 1, it can be shown that
	
    ğ”¼[w(Y- s(ğ­;Î²) )^2] = âˆ«_ğ’¯ğ”¼[Y(t)- s(ğ­;Î²) ]^2f_T(t)dt.

	The proof of Proposition 1 can be found in Appendix A.2. Note that Y(t) on the right hand side of Equation (13) represents the potential outcome and Y on the left hand side represents the observed outcome. Proposition 1 indicates that by having w on the left hand side of Equation (13), one can represent the objective function with the potential outcome (right side) by that with the observed outcome (left side). Therefore, the true value Î²^* is also a solution of the weighted optimization problem:
	
    Î²^* = argmin_Î²ğ”¼[w(Y- s(ğ­;Î²))^2].

	This result implies that the true value Î²^* can be identified from the observational data.
	One can obtain the estimator based on the sample, which is
	
    Î²Ì‚ = argmin_Î²âˆ‘_i=1^nÅµ_Ì‚Ã®(Y_i- s(ğ“_i;Î²) )^2.

	
	

  Â§.Â§.Â§ Nonparametric approach

	Suppose ğ”¼(Y(ğ­)) = s(ğ­). In a similar manner to the proof of Proposition 1, it can be shown that 
	
    ğ”¼(wY |ğ“=t) = ğ”¼(Y(ğ­)).

	Existing work of nonparametric tensor regression suffers from a slow rate of convergence due to the curse of dimensionality. Even if one flattens the tensor covariate into a vector and applies common nonparametric regression models such as additive models or single-index models to it, this issue still exists. Besides, when dealing with a vectorized tensor covariate, one would ignore the latent tensor structure and this might result in large bias. To meet these challenges, we adopt the broadcasted nonparametric tensor regression method (<cit.>) to estimate the causal effect function s(ğ­).
	The main idea of the broadcasted nonparametric tensor regression is to use the (low-rank) tensor structure to discover important regions of the tensor so as to broadcast a nonparametric modeling on such regions. Specifically, assume that 
	
    s(ğ“) = c+1/pqâˆ‘_r=1^R<Î²_1^(r)âˆ˜Î²_2^(r), F_r(ğ“)>,

	where câˆˆ R,  ğ“âˆˆ R^p Ã— q,  F_r(ğ“) = â„¬(f_r, ğ—), â„¬ is a broadcasting operator, which is defined as
	
    (â„¬(f,ğ“))_i_1,i_2 = f(T_i_1,i_2),  for all i_1,i_2.

	The broadcasted functions f_r, r=1,â€¦,R, will be approximated by B-spline functions, i.e.,
	
    f_r(x) â‰ˆâˆ‘_d=1^DÎ±_r,db_d(x),

	where ğ›(x) = (b_1(x),â€¦,b_D(x))^' is a vector of B-spline basis functions and Î±_r,d's are the corresponding spline coefficients. Define Î±_r = (Î±_r,1,â€¦,Î±_r,D)^' and (Î¦(ğ“))_i_1,i_2,d = b_d(T_i_1,i_2), the regression function (16) can be approximated by 
	
    s(ğ“) â‰ˆ c+1/pqâˆ‘_i=1^R<Î²_1^(r)âˆ˜Î²_2^(r)âˆ˜Î±_r, Î¦(ğ“)>.

	To separate out the constant effect from f_r's, the condition âˆ«_0^1 f_r(x) dx=0 is imposed, which leads to 
	
    âˆ«_0^1âˆ‘_d=1^DÎ±_r,db_d(x)dx=0,  r=1,â€¦, R.

	Then the following optimization problem is considered:
	
    argmin_c,ğ† âˆ‘_i=1^n (Åµ_iY_i- c-1/pq<ğ†,Î¦(ğ“_i)>)^2

	s.t.
	
    ğ† = âˆ‘_r=1^RÎ²_1^(r)âˆ˜Î²_2^(r)âˆ˜Î±_r  
    âˆ‘_d=1^DÎ±_r,dâˆ«_0^1 b_d(x)dx =0,  r=1,â€¦,R,

	and the estimated regression function is 
	
    Å(ğ“) = Ä‰+1/pq <ğ†Ì‚,Î¦(ğ“)>,

	where (Ä‰,ğ†Ì‚) is a solution of (20). 
	Since optimization problem (20) contains too many constraints, it is not computationally efficient to solve it directly. To further simplify the optimization problem, an equivalent truncated power basis (<cit.>) is used to reduce the constraints. Specifically, let bÌƒ_d(x), d=1,â€¦,D denote the truncated basis:
	
    bÌƒ_1(x)=1,bÌƒ_2(x)=x,â€¦, bÌƒ_Ï‚(x) = x^Ï‚-1,
    bÌƒ_Ï‚+1(x) = (x-Î¾_2)_+^Ï‚-1,â€¦, bÌƒ_D(x)=(x-Î¾_D-Ï‚+1)_+^Ï‚-1,

	where Ï‚ and (Î¾_2,â€¦,Î¾_D-Ï‚+1) are the order and the interior knots of the aforementioned B-spline, respectively. Based on these basis functions, consider the following optimization
	
    argmin_cÌƒ,ğ†Ìƒ âˆ‘_i=1^n (Åµ_iY_i- cÌƒ-1/pq<ğ†Ìƒ,Î¦Ìƒ(ğ“_i)>)^2

	s.t.
	
    ğ†Ìƒ = âˆ‘_r=1^RÎ²_1^(r)âˆ˜Î²_2^(r)âˆ˜Î±Ìƒ_r,

	where Î¦Ìƒ(ğ“)_i_1,i_2,d = bÌƒ_d+1(ğ“_i_1,i_2), d=1,â€¦,D and Î±Ìƒ_r âˆˆ R^D-1 is the vector of coefficients. Compared with (20), the mean zero constraints are removed by reducing one degree of freedom in the basis functions. According to <cit.>, Lemma B.1, one can show that
	
    Å(ğ“) = Ä‰ÌƒÌ‚+1/pq <ğ†Ì‚ÌƒÌ‚,Î¦Ìƒ(ğ“)>,

	where (Ä‰ÌƒÌ‚,ğ†Ì‚ÌƒÌ‚) is the solution of (22). 
	The optimization problem (22) can be solved by the scaled-adjusted block-wise descent algorithm (<cit.>).
	
	

Â§ THEORETICAL PROPERTIES

	In this section, the large sample properties of the proposed estimators in section 3 are established. First the consistency of the estimated weight in section 3.1 is shown, then the consistency of the parametric estimator in section 3.2.1 and the convergence rate of the nonparametric estimator in section 3.2.2 are shown. The following assumptions are made. 
	

	Assumption 4
	
	
	
		
		
		
		
		
  * There exists a constant c_0 such that 0 < c_0 < 1, and c_0 â‰¤exp (z-1) â‰¤ 1-c_0 for any z= MÌƒ_K(ğ­,ğ±)^'Î¸ with Î¸âˆˆint(Î˜). Besides, exp (z-1) = O(1) in some neighborhood of z^* = MÌƒ_K(ğ­,ğ±)^'Î¸^*, where MÌƒ_K(ğ­,ğ±)= Î› (m_K(ğ­,ğ±)-mÌ…_K) and Î›= diag(Î»_1,â€¦,Î»_K).
		
  * There exists a constant C such that 
		E {MÌƒ_K(ğ“_i, ğ—_i)MÌƒ_K(ğ“_i, ğ—_i)^'}â‰¤ C. 
		
  * Î´ = o(n).
		
  * sup_(ğ“,ğ—) exp{âˆ‘_j=1^KÎ¸_j^* Î»_j [m_K,j(ğ“,ğ—)-Em_K,j(ğ“,ğ—)] } = O(1).
	 
	
	
	Assumption 5
	
		
  * The parameter space Î˜_1 is a compact set and the true parameter Î²_0 is in the interior of Î˜_1.
		
  * (Y-s(T;Î²))^2 is continuous in Î², ğ”¼[sup_Î²(Y-s(T;Î²))^2] < âˆ and sup_Î²ğ”¼[(Y-s(T;Î²))^4]  < âˆ.
	 
	
	
	Assumption  6
	
		
  * s(ğ­;Î²) is twice continuously differentiable in Î²âˆˆÎ˜_1 and let h(ğ­;Î²) â‰¡â–½_Î² s(ğ­;Î²).
		
  * ğ”¼{ w(Y-s(ğ“;Î²))h(ğ“;Î²) } is differentiable with respect to Î² and 

		U â‰¡ - â–½_Î²ğ”¼{ w(Y-s(ğ“;Î²))h(ğ“;Î²) }|_Î²=Î²^* is nonsingular.
		
  * ğ”¼[sup_Î²| Y-s(T;Î²) |^2+Î´] < âˆ for some Î´ >0 and there extists some finite positive constants a and b such that ğ”¼[sup_Î²_1:  ||Î²_1-Î²||  < Î´_1| s(T;Î²_1) - s(T;Î²) |^2]^1/2 < aÂ·Î´_1^b for any Î²âˆˆÎ˜_1 and any small Î´_1 >0.
	
	
	
	
	Assumption 7
	
		
  * The treatment ğ“âˆˆ [0,1]^pÃ— q has a continuous probability density function f, which is bounded away from zero and infinity.
		
  * The vector of random errors, Ïµ = (Ïµ_1,â€¦,Ïµ_n)^', has independent and identically distributed entries. Each Ïµ_i is sub-Gaussian with mean 0 and sub-Gaussian norm Ïƒ < âˆ.
		
  * The true broadcasted functions f_0râˆˆâ„‹, r= 1,â€¦,R_0. Here â„‹ is the space of functions from [0,1] to R satisfying the HÃ¶lder condition of order Ï‰, i.e.,
		
    â„‹ = { g: | g^(l) (x_1)-g^(l) (x_2) |â‰¤ S_1| x_1-x_2 |^Ï‰, âˆ€ x_1,x_2 âˆˆ [0,1] },

		for some constant S_1>0, where g^(l) is the l-th derivative of g, such that Ï‰âˆˆ (0,1] and Ï„ = l+Ï‰ >1/2.
		
  * The order of the B-spline used in (16) satisfies Ï‚â‰¥Ï„+1/2. Let 0= Î¾_1 < Î¾_2 <â€¦ < Î¾_D-Ï‚+2=1 denote the knots of B-spline basis and assume that
		
    h_n = max_d=1,â€¦, D-Ï‚+1|Î¾_d+1
    			-Î¾_d |â‰ D^-1 and h_n/min_d=1,â€¦, D-Ï‚+1|Î¾_d+1
    				-Î¾_d |â‰¤ S_2

		for some constant S_2>0.
	 
	
	Assumption 4(1) enables consistency of Î¸Ì‚ to translate into consistency of the weights.  Assumption 4(2) is a standard technical condition that restricts the magnitude of the basis functions; see also Assumption 4.1.6 of <cit.> and Assumption 2(2) of <cit.>. Assumption 4(3) requires that the threshold parameter Î´ should be much smaller than the sample size. Assumption 4 (4) is needed for consistency of the estimated weight. Assumption 5(1) is a commonly used assumption in nonparametric regression. Assumption 5(2) is an envelope condition applicable to the uniform law of large numbers. Assumption 6(1) and (2)  impose sufficient regularity conditions on the causal effect function and its derivative function. Assumption 6(3) is a stochastic equicontinuity condition, which is needed for establishing weak convergence (<cit.>). Assumption 7(1), (3) and (4) are common in nonparametric regression models. In particular, Assumption 7(3) and (4) regularize the space where the true broadcasted functions lie in and guarantee that they can be approximated welll by B-spline functions. Similar assumptions can be found in <cit.> and <cit.>. Assumption 7(2) is a standard tail condition of the error. Based on these assumptions, the following theorems are established. 

	

	
	Theorem 2. 	Let Î¸Ì‚ denote the solution to Problem (10) and 
	
    Åµ_i = exp{âˆ‘_k=1^KÎ¸Ì‚_k Î»_k (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k) -1} ,  i=1,â€¦,n,

	then under Assumptions 1-4,

	
		
		
  * âˆ«|ğ°Ì‚-ğ°^* |^2 dF(ğ­,ğ±) = O_p(n^-1).
		
  * 1/nâˆ‘_i=1^n|Åµ_i- w_i^* |^2 = O_p(n^-1).
	
	Based on Theorem 2, we can establish the consistency of the parametric estimator and the convergence rate of the nonparametric estimator.

	
	Theorem 3  
	
		
  * Under Assumptions 1-5, ||Î²Ì‚-Î²^* || â†’_p 0. 
		
  * Under Assumptions 1-6, âˆš(n)(Î²Ì‚-Î²^*) â†’_d N(0,V), where 
		
    V = 4U^-1Â·ğ”¼{ w^2(Y-s(ğ“;Î²^*))^2h(ğ“;Î²^*)h(ğ“;Î²^*)^'}Â· U^-1

	
	
	Theorem 4  If Assumptions 1-4 and 7 hold, Râ‰¥ R_0, and 
	
    n > S_1 h_n^-2-2/log(h_n)(log^-2(h_n))(R^3+R(p+q)+RD)

	for some large enough constant S_1>0, then 
	
	
	
	
	
	
	
	
    ||Å(ğ“)-s_0(ğ“) ||^2 = O_p  ( R^3+R(p+q)+RD/n )+O_p  ( {âˆ‘_r=1^R_0||vec(ğ_0r)||_1/pq}^2 1/D^2Ï„ ),

	where s_0(ğ“) = c_0+1/pqâˆ‘_r=1^R_0<Î²_1^(0r)âˆ˜Î²_2^(0r), F_0r(ğ“)> represents the true regression function.
	The proofs of Theorem 2, 3 and 4 can be found in Appendix A.3, A.4 and A.5, respectively. 
	
	
	

Â§ SIMULATION

	To evaluate the finite sample performance of the proposed method, simulation studies are carried out under different data settings. The main motivation of the simulation is to compare the proposed method with three other methods when the outcome model are linear and nonlinear in various ways.
	

 Â§.Â§ The low-dimensional covariate setting

	In this subsection, we compare the performance of the proposed method (WEBM) with the unweighted method (Unweighted), entropy balancing method (EB) and univariate approximate balancing method (MDABW) in the low-dimensional covariate setting, where EB refers to the method proposed by <cit.> that balances covariates exactly and MDABW refers to the method proposed by <cit.> that balances covariates approximately.
	Since the covariates are shared across all scenarios, their data generating process is first described. Specifically, we independently draw 5 covariates from a multivariate normal distribution with mean 0, variance 1 and covariance 0.2, that is,
	
	
    ğ— = (X_1,....,X_5)^'âˆ¼  N_5(Î¼,Î£)  with Î¼= [ 0; â‹®; 0 ]and Î£=[     1   0.2     â€¦   0.2;   0.2     1 0.2 â€¦   0.2;     4;   0.2   0.2     â€¦     1 ]_5Ã—5.

	

	
	Consider a linear treatment assignment model, which is defined as
	
    ğ“_i= X_i1ğ_1+X_i2ğ_2+X_i3ğ_3+ğ„_i,

	where ğ_j = [ 1 0; 0 1; 1 1 ]_3Ã—2, j=1,2,3 denotes the jth coefficient matrix, and ğ„_i âˆˆ R^3Ã— 2 denotes the error matrix, whose element follows a standard normal distribution. For the outcome model, we consider four scenarios and conduct 100 Monte Carlo simulations for each scenario. The first two scenarios assume an outcome model that is linear in treatment and the others assume a nonlinear relationship.
	In scenario 1, the linear outcome model is defined as
	
    Y_i = 1+ <ğ, ğ“_i>+X_i1+(X_i2+1)^2+X_i4^2+Ïµ_i,

	where ğ = [ 1 0; 0 1; 1 1 ]_3Ã—2 and Ïµ_i âˆ¼ N(0,2^2). In this scenario, v_k2(ğ—) = (1, ğ—, ğ—*ğ—)^', where * represents the Hadamard product of two matrices, and the corresponding element of ğ—*ğ— is (X*X)_ij = (x_ijx_ij).
	
	In scenario 2, the linear outcome model is defined as
	
    Y_i = 1+ <ğ, ğ“_i>+X_i2+X_i3+X_i1X_i2+â€¦+X_i4X_i5+Ïµ_i,

	where ğ and Ïµ_i are the same as in Equation (21). In this scenario, the interaction terms are strong confounders, hence set v_k2(ğ—) = (1, ğ—, X_jX_k)^', 1â‰¤ j < k â‰¤ 5.  
	
	In scenarios 3 and 4, the nonlinear outcome models are considered and are defined as
	
    Y_i = 1+ <ğ, F_1(ğ“_i)>+X_i1+(X_i2+1)^2+X_i4^2+Ïµ_i,

	and
	
    Y_i = 1+ <ğ, F_1(ğ“_i)>+X_i2+X_i3+X_i1X_i2+â€¦+X_i4X_i5+Ïµ_i,

	respectively.
	Here ğ and Ïµ_i are the same as in Equation (24), (F_1(ğ“))_k_1,k_2= f_1(T_k_1,k_2)= T_k_1,k_2+0.6sin{2Ï€(T_k_1,k_2-0.5)^2 }. For all four scenarios, consider u_K1(ğ“) =(1, vec(ğ“)^')^' for simplicity.
	For each method, the mean RMSE and its standard deviation of the coefficient estimates for the linear outcome model, and those of the fitted values for the nonlinear outcome model are reported based on 100 data replications. 
	
	Table 1 shows the mean RMSE and its standard deviation of the coefficient matrix for the linear outcome model. Observe that the mean RMSE of WEBM is the smallest among the four methods, and the standard deviation of WEBM is the second smallest while that of Unweighted is the smallest in both scenario 1 and scenario 2. Besides, the results of scenario 2 indicate that MDABW and EB  have poor performance when the basis function of covariates includes interaction entries, and their mean RMSEs are even larger than those of Unweighted. The mean RMSE and standard deviation of all methods decreases as the sample size increases.
	Table 2 shows the mean RMSE and its standard deviation of the fitted values of the nonlinear outcome model. As can be seen, the results for both scenario 3 and 4 are similar to those in Table 1, that is, WEBM performs the best with both the smallest mean RMSE in all cases. Similarly, MDABW and EB methods have poor performance when the basis function of covariates includes interaction entries and the mean RMSE and standard deviation of all methods decreases as the sample size increases.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

 Â§.Â§ The high-dimensional covariate setting

	In this subsection, since the MDABW and EB methods can only deal with the low-dimensional covariate case, we compare the performance of the proposed method (WEBM) with the Unweighted method and the method (Mapping) proposed by <cit.>, which selects a small important subset of covariates by a joint screening procedure, 
	in the high-dimensional covariate setting. Since the Mapping method can only deal with linear outcome model, these two methods are compared in the linear outcome model setting.
	For both methods, set the sample size n=500 and the dimension of ğ“ to be 3 Ã— 2. Consider two scenarios (scenario 5-6) with the dimension of covariates  L=49 and L=99, respectively. The motivation for such settings is to consider the number of constraints K < n (scenario 5) and K>n (scenario 6), respectively. For the basis functions, u_K1(ğ“)= (1,vec(ğ“)^')^' and v_K2(ğ—) = (1,ğ—^')^' are considered.
	Additionally, covariates are drawn from a multivariate normal distribution with mean 0, variance 1 and covariance 0.2. For each scenario, consider a linear treatment assignment model, which is defined as
	
    ğ“_i= X_i1ğ_1+X_i2ğ_2+â€¦+X_i5ğ_5+ğ„_i,

	where ğ_j = [ 1 0; 0 1; 1 1 ]_3Ã—2, j=1,2,3,4,5 denotes the jth coefficient matrix, and ğ„_i âˆˆ R^3Ã— 2 denotes the error matrix, whose element follows a standard normal distribution. 
	Moreover, the linear outcome model is defined as
	
    Y_i = 1+ <ğ, ğ“_i>+X_i1+X_i2+â€¦+X_i5+Ïµ_i,

	
	
	
	
	
	where ğ=[ 1 0; 0 1; 1 1 ]_3Ã—2, Ïµ_i âˆ¼ N(0,2^2).
	For each method, the mean RMSE and its standard deviation of the tensor regression estimation for the linear outcome model are reported based on 100 data replications.  
	Table 3 shows the mean RMSE and its standard deviation of coefficent matrix for the linear outcome model in the high-dimensional covariate setting. The results indicate that WEBM performs the best with the smallest mean RMSE in all cases.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

Â§ APPLICATION

	Intelligence Quotient (IQ) is based on biological attributes, most of which are inherited from parents. It mainly refers to a person's cognitive abilities, such as logical reasoning, pattern recognition, and short-term memory. Of course, due to genetic mutations, parents with average IQ may have offspring with superior intelligence, and vice versa. But IQ also has social attributes. Studies have found that IQ has obvious plasticity, and environmental factors affect IQ levels. From 1947 to 2002, the IQ level of developed countries rose steadily at a rate of 3% every 10 years, which is called the "Flynn effect." This effect has been repeatedly observed in various countries, various age groups, and a large number of different environments, and has become a reliable evidence that "environment affects IQ" (<cit.>). In particular, literatures have shown that taking training lessons, such as music, sports, chess and so on, can significantly enhance children's IQ (<cit.>; <cit.>; <cit.>; <cit.>). 
	Unfortunately, there are some limitations of existing studies. First, existing studies have only analyzed the relevant effects of attending single training course at a fixed age on children's IQ. In practice, however, a child may attend different training courses at the same age and participation may also vary by age. Second, existing studies mainly investigated the impact of whether or not to attend training classes, ignoring the effect of class duration. Third, existing studies only analyzed the correlation between children's participation in training courses on children's IQ, while their causal relationship is much more of a concern. To overcome the aforementioned limitations, the proposed method is applied to a matrix treatment, which contains the structural information of children's participation in training courses, to investigate their causal impact on children's IQ.
	The data are obtained from the Brain Science Innovation Institute of East China Normal University's Child Brain Intelligence Enhancement Project, whose goal is to explore brain development and help improve brain power. The treatment we are interested in is a 2 Ã— 5 matrix about children's participation in training courses, whose rows represent age groups, including 3-6 years old and 6-9 years old, columns represent the types of training courses, including knowledge education (Chinese, mathematics and English, etc.), art (music, art, calligraphy, etc.), sports (swimming, ball games, etc.), hands-on practice (STEM, Lego, etc.) and thinking training (logical thinking, EQ education, attention, etc.), and each element of the matrix represents the number of hours of class per week. The outcome is children's IQ and the pre-treatment covariates include children's gender as well as parental education, which have been shown to be associated with both the treatment and outcome variables (<cit.>; <cit.>; <cit.>; <cit.>). A complete-case analysis is conducted with a sample of 103 participants.  
	Before estimating the causal effect, we first examine the covariate balancing of WEBM, MDABW and EB methods based on the WEIM statistics. The statistic WEIM defined in equation (8) is 0.1050 for the WEBM method, 0.3761 for the MDABW method and 0.9881 for the EB method, which implies that WEBM balances covariates well while EB does not. Assume a linear tensor outcome model and the bootstrap method with 200 replicates is used to obtain confidence intervals for the parameter estimates. The results are shown in Table 4.
	
		
		
		
	
	Table 4 shows the estimated causal effects of the duration of attending different classes at different ages on children's IQ. It can be seen that most methods (except EB) suggest that the duration of attending hands-on practice courses at 6-9 years old has a siginificantly positive impact on children's IQ. This finding is not only consistent with previous findings that participation in hands-on practice classes can improve children's IQ (<cit.>; <cit.>; <cit.>),  but further suggests that longer participation in hands-on practice classes is more beneficial to children's IQ.
	
	The results imply that future work of an intervention study about attending hands-on practice training courses, which in turn may improve children's IQ, is suggested. Besides, the width of confidence interval of the estimated causal effect based on WEBM is the smallest, which implies that the estimation accuracy of WEBM is the highest. 
	
	
	
	
	

Â§ CONCLUSION AND DISCUSSION
	
	In this study, the weighted Euclidean balancing method is proposed, which obtains stabilized weights by adopting a single measure that represents the overall imbalance. An algorithm for the high-dimensional covariate setting is also proposed. Furthermore, parametric and nonparametric methods are developed to estimate the causal effect and their theoretical properties are provided. The simulation results show that the proposed method balances covariates well and produces a smaller mean RMSE compared to other methods under variaous scenarios. In the real data analysis, the WEBM method is applied to investigate causal effect of children's participation in training courses on their IQ. The results show that the duration of attending hands-on practice at 6-9 years old has a siginificantly positive impact on children's IQ.
	Since the causal effect function ğ”¼(Y(t)) is more general, we mainly consider it as the estimand for matrix treatment in this paper. Actually, one can also consider the average treatment effect  (ğ”¼(Y(t+ t)-Y(t))) or average partial effect (ğ”¼Y(t+ t)-ğ”¼Y(t)/ t)  , which can be easily estimated based on the estimates of causal effect function  (<cit.>). Indeed, the causal effect function provides a complete description of the causal effect, rather than a summary measure. Moreover, parametric and nonparametric methods are developed to estimate the causal effect function. Parametric method is recommended when reasonable assumptions can be made about the true model since it is easier to implement and requires less sample size. Despite nonparametric method has higher requirements of the sample size, one can choose to use it according to the real situations due to its higher flexibility. Besides, this paper mainly focuses on the small-scale matrix treatment. Large-scale matrix treatment with low-rank structure can also be considered. In such case, one may control the overall imbalance by only balancing their non-zero elements based on some decomposition technology, and this will be investigated in future work.
	
	
	
	*
	apalike
	
	
	

Â§ APPENDIX

	

 Â§.Â§ A.1.Proof of Theorem 1
 
	The primal problem is
	
    min_ğ°âˆ‘_i=1^nw_ilog(w_i)

	s.t.
	
    âˆ‘_j=1^K{Î»_j^2 [âˆ‘_i=1^n w_i (m_K,j(ğ“_i,ğ—_i)-mÌ…_K,j)]^2 }â‰¤Î´.

	
	Let ||Î¸||_2 = âˆš(Î¸_1^2+â€¦+Î¸_K^2) be the l_2 norm for an arbitrary K-dimensional vector Î¸ =(Î¸_1,â€¦,Î¸_K)^' and Î› = diag(Î»_1,â€¦,Î»_K), then the inequality constraint in the primal problem can be rewritten as ||âˆ‘_i=1^n w_i Î› (m_K(ğ“_i,ğ—_i)-mÌ…_K) ||_2 â‰¤âˆš(Î´). Let ğ’œâŠ† R^K be a convex set such that ğ’œ = { a âˆˆ R^K: || a||_2 â‰¤âˆš(Î´)}. Define I_ğ’œ(a) = 0 if a âˆˆğ’œ and I_ğ’œ(a) = âˆ otherwise. Then, the primal problem (15) is equivalent to the following optimaization problem:
	
    min_ğ° âˆ‘_i=1^nw_ilog(w_i)+I_ğ’œ( âˆ‘_i=1^n w_i Î› (m_K(ğ“_i,ğ—_i)-mÌ…_K)).
 
	Let h(w) = âˆ‘_i=1^n w_i log(w_i), the conjugate function of h is 
	
    h^*(w)    = sup_t(âˆ‘_i=1^n w_i t_i-âˆ‘_i=1^n w_i log(w_i)) 
       = sup_t âˆ‘_i=1^n (w_i t_i-w_i log(w_i)) 
       = âˆ‘_i=1^n sup_t_i(w_i t_i-w_i log(w_i)) 
       =  âˆ‘_i=1^n f^*(w_i),

	where f^*(w_i) = sup_t_i(w_i t_i-w_i log(w_i)) is the conjugate function of f(w_i) = w_i log(w_i). Let g(Î¸) = I_ğ’œ(Î¸) for any Î¸âˆˆ R^K, then the conjugate function of g is 
	
    g^*(Î¸)    = sup_a (âˆ‘_k=1^KÎ¸_ka_k -T_ğ’œ(a) ) 
       = sup_|| a ||_2 â‰¤âˆš(Î´) (âˆ‘_k=1^KÎ¸_ka_k) 
       = sup_|| a ||_2 â‰¤âˆš(Î´)  (||Î¸||_2 || a ||_2) 
       = âˆš(Î´)||Î¸||_2.

	Define the mapping H: R^n â†’ R^K such that Hw = âˆ‘_i=1^n w_i Î› (m_K(ğ“_i,ğ—_i)-mÌ…_K), then H is a bounded linear map. Let H^* be the adjoint operator of H, then for all Î¸ = (Î¸_1,â€¦,Î¸_K)^'âˆˆ R^K,
	
    H^*Î¸ = (âˆ‘_k=1^KÎ¸_k Î»_k (m_K,k(ğ“_1,ğ—_1)-mÌ…_K,k), â€¦, âˆ‘_k=1^KÎ¸_k Î»_k (m_K,k(ğ“_n,ğ—_n)-mÌ…_K,k) )^'.

	Define Î¸Ìƒ = HwÌƒ = 1/n^râˆ‘_i=1^nÎ› (m_K(ğ“_i,ğ—_i)-mÌ…_K), where wÌƒ = (1/n^r,â€¦,1/n^r)^'âˆˆ dom(F). Here, we choose b to be sufficiently large such that ||Î¸Ìƒ||_2 â‰¤âˆš(Î´), then we obtain that g(Î¸Ìƒ) = 0 and g is continuous at Î¸Ìƒ. Therefore, Î¸Ìƒâˆˆ H(dom(F) âˆ© cont(g)), which implies that H(dom(F) âˆ© cont(g)) â‰ âˆ…. Here, dom(F) and cont(g) denotes the domain of F and the continuous set of g, respectively. Therefore, the strong duality condition of the Fenchel duality theorem is verified. Moreover,
	
    F(H^*Î¸)+g^*(-Î¸) = âˆ‘_i=1^n f^*(âˆ‘_k=1^KÎ¸_k Î»_k (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k))+âˆš(Î´)||Î¸||_2.

	According to the Fenchel duality theorem (Mohri et al. (2018), Theorem B.39), we have 
	
    min_w  âˆ‘_i=1^nw_ilog(w_i)+I_ğ’œ( âˆ‘_i=1^n w_i Î› (m_K(ğ“_i,ğ—_i)-mÌ…_K)) 
       = min_w  âˆ‘_i=1^n f^*(âˆ‘_k=1^KÎ¸_k Î»_k (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k))+âˆš(Î´)||Î¸||_2.
 
	Furthermore, since the strong duality condition holds, we can conclude that H^*Î¸Ì‚ is a subgradient of F at Åµ. That is, 
	
    âˆ‘_k=1^KÎ¸Ì‚_k Î»_k (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k) = log(Åµ_i)+1.

	Therefore, Åµ_i = exp(âˆ‘_k=1^KÎ¸Ì‚_k Î»_k (m_K,k(ğ“_i,ğ—_i)-mÌ…_K,k)-1). The proof of theorem 1 is completed.
	
	
	

 Â§.Â§ A.2.Proof of Proposition 1

	Using the law of total expectation and Assumption 1, we can deduce that
	
    ğ”¼[w(Y- s(ğ“;Î²) )^2]
       =E[f(T)/f(T|X)(Y-s(ğ“;Î²)  )^2]
       = ğ”¼(ğ”¼[f(T)/f(T|X)(Y- s(ğ“;Î²)  )^2] |T=t,X=x )
       = ğ”¼(f(t)/f(t|x)ğ”¼([(Y- s(ğ“;Î²)  )^2] |T=t,X=x) )
       = âˆ«_ğ’¯Ã—ğ’³f(t)/f(t|x)ğ”¼[(Y(T)- s(ğ“;Î²)  )^2 |T = t, X= x]f(t|x)dtdx
       =âˆ«_ğ’¯Ã—ğ’³ğ”¼[(Y(T)- s(ğ“;Î²)  )^2 |T = t, X= x]f(t)f(x)dtdx
       = âˆ«_ğ’¯Ã—ğ’³ğ”¼[(Y(t)- s(ğ“;Î²) )^2 |X= x]f(t)f(x)dtdx   (using Assumption 1)
       = âˆ«_ğ’¯ğ”¼[(Y(t)- s(ğ“;Î²)  )^2] f(t)dt  .

	Hence, we complete the proof of Proposition 1.
	
	

 Â§.Â§ A.3.Proof of Theorem 2

	The first order optimality condition for the dual probblem (10) is 
	
    âˆ‘_i=1^nexp{âˆ‘_j=1^KÎ¸Ì‚_jÎ»_jM_K,j(ğ“_i,ğ—_i) }Â·Î»_jM_K,j(ğ“_i,ğ—_i) +âˆš(Î´)Î¸Ì‚_j/||Î¸Ì‚||_2 =0,   j=1,â€¦,K,

	where M_K,j(ğ“_i,ğ—_i) = m_K,j(ğ“_i,ğ—_i)- mÌ…_K,j, M_K(ğ“_i,ğ—_i)= (M_K,1(ğ“_i,ğ—_i), â€¦, M_K,K(ğ“_i,ğ—_i))^'.
	Let Î› = diag(Î»_1,â€¦,Î»_K) and 
	
    1/nâˆ‘_i=1^nÎ¦(ğ“_i,ğ—_i;Î¸) = 1/nâˆ‘_i=1^nexp{âˆ‘_j=1^KÎ¸_jÎ»_j[m_K,j(ğ“_i,ğ—_i)-ğ”¼(m_K,j)] }Î› [m_K(ğ“_i,ğ—_i) -ğ”¼(m_K)],

	which is a set of K estimating functions. Note that
	
    |ğ”¼( Î¦(ğ“_i,ğ—_i;Î¸^*)) |
       = |ğ”¼{exp{âˆ‘_j=1^KÎ¸_j^*Î»_j[m_K,j(ğ“_i,ğ—_i)-Em_K,j] }Î› [m_K(ğ“_i,ğ—_i) -Em_K] }|
       â‰¤sup_(ğ“_i,ğ—_i) exp{âˆ‘_j=1^KÎ¸_j^*Î»_j[m_K,j(ğ“_i,ğ—_i)-Em_K,j] }Â·|ğ”¼Î› [m_K(ğ“_i,ğ—_i) -Em_K] |
       â‰¤ O(1) Â·0 = 0,

	hence we have ğ”¼( Î¦(ğ“_i,ğ—_i;Î¸^*))=0, which implies that Î¸^* is the unique solution of ğ”¼( Î¦(ğ“_i,ğ—_i;Î¸))=0. Therefore, by the estimating equation theory (Van der Vaart (2000)), the solution of the estimating equations 
	
    1/nâˆ‘_i=1^nÎ¦(ğ“_i,ğ—_i;Î¸) = 0,
 
	denoted by Î¸Ìƒ, is asymptotically consistent for Î¸^*. Furthermore, by the Taylor expansion, we have
	
    âˆš(n)(Î¸Ìƒ - Î¸^*) â†’_d N(0, Î£),

	where Î£ = { E(âˆ‚Î¦/âˆ‚Î¸^') }^-1 E(Î¦Î¦^'){ E(âˆ‚Î¦/âˆ‚Î¸) }^-1.
	Moreover, by the assumption that Î´ = o(n), we have 1/nâˆš(Î´)Î¸_j/||Î¸||_2 = o_p(n^-1/2) for any Î¸âˆˆ int(Î˜). Therefore, by the Slutsky's theorem, we obtain that 
	
    âˆš(n)(Î¸Ì‚ - Î¸^*) â†’_d N(0, Î£).

	Let MÌƒ_K,j(ğ“_i,ğ—_i) = Î»_j(m_K,j(ğ“_i,ğ—_i)- mÌ…_K,j),
	then 
	
    Åµ_i    = exp{âˆ‘_j=1^KÎ¸Ì‚_j Î»_j (m_K,j(ğ“_i,ğ—_i)-mÌ…_K,j) -1}
       = exp{MÌƒ_K(ğ“_i,ğ—_i)^'Î¸Ì‚ -1}

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	By Mean Value Theorem, we can deduce that
	
    âˆ«|Åµ-w^*|^2dF(ğ­,ğ±) 
       â‰¤sup_(ğ­,ğ±) |exp{MÌƒ_K(ğ­,ğ±)^'Î¸_1 -1}|^2 Ã—âˆ«|MÌƒ_K(ğ­,ğ±)^'(Î¸Ì‚-Î¸^*) |^2 dF(ğ­,ğ±) 
       â‰¤ O_p(1) Â·âˆ«|MÌƒ_K(ğ­,ğ±)^'(Î¸Ì‚-Î¸^*) |^2 dF(ğ­,ğ±),

	where Î¸_1 lies between Î¸Ì‚ and Î¸^*.
	Since 
	
    âˆ«|MÌƒ_K(ğ­,ğ±)^'(Î¸Ì‚-Î¸^*) |^2 dF(ğ­,ğ±) 
        	= âˆ«MÌƒ_K(ğ­,ğ±)^'(Î¸Ì‚-Î¸^*)(Î¸Ì‚-Î¸^*)^'MÌƒ_K(ğ­,ğ±) dF(ğ­,ğ±)
       = tr{ (Î¸Ì‚-Î¸^*)(Î¸Ì‚-Î¸^*)^'âˆ«MÌƒ_K(ğ­,ğ±)MÌƒ_K(ğ­,ğ±)^'dF(ğ­,ğ±) }
       â‰¤ C tr{ (Î¸Ì‚-Î¸^*)(Î¸Ì‚-Î¸^*)^'}
       = C||Î¸Ì‚-Î¸^*||^2 
       = O_p(n^-1).

	Then we have 	
	
    âˆ«|Åµ-w^*|^2dF(ğ­,ğ±) = O_p(n^-1).

	Furthermore, one can show that 
	
    1/nâˆ‘_i=1^n|MÌƒ_K(ğ­,ğ±)^'(Î´Ì‚-Î´^*) |^2 - âˆ«|MÌƒ_K(ğ­,ğ±)^'(Î´Ì‚-Î´^*) |^2 dF(ğ­,ğ±)=o_p(1).

	Hence,
	
    1/nâˆ‘_i=1^n|Åµ_i- w_i^*|^2 
       â‰¤sup_(ğ­,ğ±) |exp{MÌƒ_K(ğ­,ğ±)^'Î¸_1 -1}|^2 Â·1/nâˆ‘_i=1^n|MÌƒ_K(ğ­,ğ±)^'(Î¸Ì‚-Î¸^*) |^2 
       â‰¤ O_p(1) âˆ«|MÌƒ_K(ğ­,ğ±)^'(Î¸Ì‚-Î¸^*) |^2 dF(ğ­,ğ±) +o_p(1) 
       = O_p(n^-1)

	Therefore, the proof of Theorem 2 is completed.
	
	

  Â§.Â§.Â§ A.4.Proof of Theorem 3

	
	
	We first show that the conclusion of Theorem 3(1). 

	
	Since Î²Ì‚ (as a estimator of Î²^*) is a unique minimizer of 1/nâˆ‘_i=1^nÅµ_i(Y_i-s(T_i;Î²))^2(regarding ğ”¼[w(Y-s(T;Î²))^2], according to the theory of M-estimation (van der Vaart, 2000, Theorem 5.7), if
	
    sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^nÅµ_Ì‚Ã®(Y_i-s(T_i;Î²))^2-ğ”¼[w(Y-s(T;Î²))^2]) |â†’_p 0,

	then Î²Ì‚â†’_p Î²^*.
	Note that
	
    sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^nÅµ_Ì‚Ã®(Y_i-s(T_i;Î²))^2-ğ”¼[w(Y-s(T;Î²))^2]) |
    â‰¤sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^n(Åµ_Ì‚Ã®-w_i)(Y_i-s(T_i;Î²))^2 |
    
    		+sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^nw_i(Y_i-s(T_i;Î²))^2-ğ”¼[w(Y-s(T;Î²))^2]) |.

	We first show that sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^n(Åµ_Ì‚Ã®-w_i)(Y_i-s(T_i;Î²))^2 | is o_p(1). Using the Causchy-Schwarz inequality and the fact that Åµâ†’^L^2 w, we have
	
    sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^n(Åµ_Ì‚Ã®-w_i)(Y_i-s(T_i;Î²))^2 |   â‰¤{1/nâˆ‘_i=1^n(Åµ_Ì‚Ã®-w_i)^2 }^1/2sup_Î²âˆˆÎ˜_1{1/nâˆ‘_i=1^n(Y_i-s(T_i;Î²))^2 }^1/2
       â‰¤ o_p(1){sup_Î²âˆˆÎ˜_1ğ”¼[w(Y-s(T;Î²))^2]+o_p(1) }^1/2
       =o_p(1).

	Thereafter, under Assumption 5, we can conclude that sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^nw_i(Y_i-s(T_i;Î²))^2-ğ”¼[w(Y-s(T;Î²))^2]) | is also o_p(1) (Newey and McFadden (1994), Lemma 2.4). Hence, we complete the proof for Theorem 3(1). Next, we give the proof of Theorem 3(2). Define
	
    Î²Ì‚^* = argmin_Î²âˆ‘_i=1^n w_i(Y_i-s(ğ“_i;Î²))^2.

	Assume that 1/nâˆ‘_i=1^n w_i(Y_i - s(ğ“_i;Î²Ì‚^*))h(ğ“_i;Î²Ì‚^*)) = o_p(n^-1/2) holds with probablility to one as n â†’âˆ
	
	By Assumption 5 and the uniform law of large number, one can get that
	
    1/nâˆ‘_i=1^n w_i(Y_i-s(T_i;Î²))^2 â†’ğ”¼{ w(Y-s(T;Î²))^2 } in probability uniformly over  Î²,

	which implies ||Î²Ì‚^* -Î²^* ||â†’_p 0. Let
	
    r(Î²) = 2ğ”¼{ w(Y-s(T;Î²))h(T;Î²) },

	which is a differentiable function in Î² and r(Î²^*) = 0. By mean value theorem, we have
	
    âˆš(n)r(Î²Ì‚^*)- â–½_Î² r(Î¶) Â·âˆš(()n)(Î²Ì‚^* - Î²^*) =âˆš(n)r(Î²^*) =0

	where Î¶ lies on the line joining Î²Ì‚^* and Î²^*. Since â–½_Î² r(Î²) is continuous at Î²^* and ||Î²Ì‚^* -Î²^* ||â†’_p 0, then
	
    âˆš(n)(Î²Ì‚^* - Î²^*)  = â–½_Î² r(Î²^*)^-1Â·âˆš(n) r(Î²Ì‚^*) +o_p(1)

	Define the empirical process
	
    G_n(Î²)= 2/âˆš(n)âˆ‘_i=1^n{ w_i(Y_i-s(T_i;Î²))h(T_i;Î²) - ğ”¼{ w(Y-s(T;Î²))h(T;Î²)  }}.

	Then we have
	
    âˆš(n)(Î²Ì‚^* - Î²^*)
       =  â–½_Î² r(Î²^*)^-1Â·{âˆš(n) r(Î²Ì‚^*) -  2/âˆš(n)âˆ‘_i=1^n{ w_i(Y_i-s(T_i;Î²Ì‚^*))h(T_i;Î²Ì‚^*)  + 2/âˆš(n)âˆ‘_i=1^n{ w_i(Y_i-s(T_i;Î²Ì‚^*))h(T_i;Î²Ì‚^*)  }
       =  -â–½_Î² r(Î²^*)^-1Â· G_n(Î²Ì‚^*)+o_p(1)
       = U^-1Â·{ G_n(Î²Ì‚^*)-G_n(Î²^*) +G_n(Î²^*) } +o_p(1).

	By Assumption 5, 6, Theorem 4 and 5 of Andrews(1994), we have G_n(Î²Ì‚^*)-G_n(Î²^*) â†’_p 0. Thus,
	
    âˆš(n)(Î²Ì‚^* - Î²^*) = U^-12/âˆš(n)âˆ‘_i=1^n{ w_i(Y_i-s(T_i;Î²^*))h(T_i;Î²^*) } +o_p(1),

	then we can get that the asymptotic variance of âˆš(n)(Î²Ì‚^* - Î²^*) is V.
	Therefore, âˆš(n)(Î²Ì‚^* - Î²^*) â†’_d N(0,V). Next, we will prove Î²Ì‚â†’_p Î²Ì‚^*.
	Since
	
	
    sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^nÅµ_Ì‚Ã®(Y_i-s(T_i;Î²))^2-1/nâˆ‘_i=1^nw_i(Y_i-s(T_i;Î²))^2) |
    â‰¤sup_Î²âˆˆÎ˜_1|1/nâˆ‘_i=1^n(Åµ_Ì‚Ã®-w_i)(Y_i-s(T_i;Î²))^2 |
    â‰¤{1/nâˆ‘_i=1^n(Åµ_Ì‚Ã®-w_i)^2 }^1/2sup_Î²âˆˆÎ˜_1{1/nâˆ‘_i=1^n(Y_i-s(T_i;Î²))^2 }^1/2
    â‰¤ o_p(1){sup_Î²âˆˆÎ˜_1ğ”¼[w(Y-s(T;Î²))^2]+o_p(1) }^1/2
    
    		=o_p(1),

	which implies Î²Ì‚^* â†’_p Î²Ì‚. Then by Slutskey's Theorem, we can draw the conclusion that âˆš(n)(Î²Ì‚ - Î²^*) â†’_d N(0,V). Therefore, we have completed the proof of Theorem 3. 
	
	

  Â§.Â§.Â§ A.5.Proof of Theorem 4

	For convenience, we use a mapping Î©: R^pÃ— q Ã— DÃ— R â†’ R^pÃ— q Ã— D to represent the operator of absorbing the constant into the coefficients of B-spline basis for the first predictor. More precisely, Î© is defined by 
	
    ğ†^b= Î©(ğ†,c),

	where ğ†_i_1,i_2,d= ğ†_i_1,i_2,d for (i_1,i_2) â‰  (1,1) and ğ†_1,1,d=ğ†_1,1,d+pqc, d=1,â€¦,D. It then follows from the property of B-spline functions that
	
    c+1/pq<ğ†,Î¦(ğ“)> = 1/pq <ğ†^b,Î¦(ğ“)>.

	We also write ğ†_0= âˆ‘_r=1^R_0ğ_0râˆ˜Î±_0r, r= 1,â€¦, R_0. Suppose ğ†Ì‚,Ä‰) is a solution to (19) and 
	
    ğ†Ì‚ = âˆ‘_r=1^RÎ²Ì‚_1^(r)âˆ˜Î²Ì‚_2^(r)âˆ˜Î±Ì‚_r,

	then by <cit.>, Lemma B.1, there exists Äâˆˆ R and 
	
    ğ†ÌŒ = âˆ‘_r=1^RÎ²Ì‚_1^(r)âˆ˜Î²Ì‚_2^(r)âˆ˜Î±ÌŒ_r,

	such that
	
    Ä+1/pq<ğ†ÌŒ ,Î¦(ğ“)> = Ä‰+1/pq<ğ†Ì‚ ,Î¦Ìƒ(ğ“)>,

	where Î±ÌŒ_r= (Î±ÌŒ_r,1,â€¦,Î±ÌŒ_r,D )^' satisfying
	
    âˆ‘_d=1^DÎ±ÌŒ_r,du_d=0

	with u_d= âˆ«_0^1 b_d(x)dx.
	Using (27), 
	we have
	
    âˆ‘_i=1^n (Åµ_iy_i-Ä-1/pq<ğ†ÌŒ,Î¦(ğ“_i)>)^2 â‰¤âˆ‘_i=1^n (Åµ_iy_i-c_0-1/pq<ğ†_0,Î¦(ğ“_i)>)^2.
 
	Let ğ†ÌŒ^b= Î©(Î±ÌŒ,Ä) and ğ†_0^b = Î©(ğ†_0,c_0), then 
	
    âˆ‘_i=1^n (Åµ_iy_i-1/pq<ğ†ÌŒ^b,Î¦(ğ“_i)>)^2 â‰¤âˆ‘_i=1^n (Åµ_iy_i-1/pq<ğ†_0^b,Î¦(ğ“_i)>)^2.

	Therefore, we have
	
    âˆ‘_i=1^n ((Åµ_i-w_i+w_i)y_i-1/pq<ğ†ÌŒ^b,Î¦(ğ“_i)>)^2 â‰¤âˆ‘_i=1^n ((Åµ_i-w_i+w_i)y_i-1/pq<ğ†_0^b,Î¦(ğ“_i)>)^2,

	which leads to 
	
    âˆ‘_i=1^n (w_iy_i-1/pq<ğ†ÌŒ^b,Î¦(ğ“_i)>)^2    â‰¤âˆ‘_i=1^n (w_iy_i-1/pq<ğ†_0^b,Î¦(ğ“_i)>)^2 
       +2 âˆ‘_i=1^n (Åµ_i-w_i)y_i(1/pq<ğ†ÌŒ^b-ğ†_0,Î¦(ğ“_i)).

	Let ğ†^# =ğ†ÌŒ^b-ğ†_0^b,  ğš^# =vec(ğ†^# ),  ğš_0^b =vec(ğ†_0^b), ğšÌŒ^b= vec(ğ†ÌŒ^b) and ğ™
	= (ğ³_1,â€¦,ğ³_n)^'âˆˆ R^nÃ— pqD, where ğ³_i = vec(Î¦(ğ“_i)),i=1,â€¦,n. 
	Let y_Åµ= (Åµ_1y_1,â€¦, Åµ_ny_n)^' and y_w = (w_1y_1,â€¦, w_ny_n)^', then using (31) and working out the squares, we obtain
	
    1/p^2q^2||ğ™ğš^#||^2   â‰¤  2<1/pqğ™ğšÌŒ^b,y_w>-2<	1/pqğ™ğš_0^b,y_w>
       -21/p^2q^2<ğ™ğš^#,ğ™ğš_0^b>+2<1/pqğ™ğš^#, y_Åµ-y_w>  
       = 2<1/pqğ™ğš^#,y_Åµ-y_w>+2<1/pqğ™ğš^#,Ïµ>+2<	1/pqğ™ğš^#,y_w-Ïµ-1/pqğ™ğš_0^b>

	First, we show the upper bound of <1/pqğ™ğš^#,y_Åµ-y_w>. Using the Cauchy-Schwarz inequality, we have
	
    <1/pqğ™ğš^#,y_Åµ-y_w> 
       â‰¤||Åµ-w ||_2 Â·||1/pqğ™ğš^#||_2 
       â‰¤C_1âˆš(nh_n)/pqÂ·||Åµ-w ||_2 Â·||ğš^#||_2

	
	By the conclusion of Theorem 2(2), we have 
	
    ||Åµ-w ||_2 = O_p(1).

	Applying (37) to (36), we can obtain that
	
    <1/pqğ™ğš^#,y_Åµ-y_w> â‰¤C_2âˆš(nh_n)/pq||ğš^#||_2.

	Second, by the conclusion of <cit.>, (A.18) and (A.20), we can obtain the upper bound of
	<1/pqğ™ğš^#,Ïµ> and <	1/pqğ™ğš^#,y_w-Ïµ-1/pqğ™ğš_0^b>, which are
	
    <1/pqğ™ğš^#,Ïµ> â‰¤C_3/pq||ğš^#||_2 { nh_n(R^3+R(p+q)+RD)}^1/2.

	and
	
    <1/pqğ™ğš^#,y_w-Ïµ-1/pqğ™ğš_0^b> â‰¤C_4/pq||ğš^#||_2 {âˆ‘_r=1^R_0||vec(ğ_0r)||_1/pq}nâˆš(h_n)/D^Ï„

	Therefore, applying (38), (39) and (40) to (35), we have
	
    C_5/pq||ğš^#||_2^2 â‰¤ R_1 ||ğš^#||_2,

	where  R_1 = C_6âˆš(D/n)+C_7  {D(R^3+R(p+q)+RD)/n}^1/2+C_8{âˆ‘_r=1^R_0||vec(ğ_0r)||_1/pq}1/D^Ï„-1/2.
	
	By solving the second order inequality (41), we have
	
    C_5/pq||ğš^#||_2 â‰¤ R_1

	Further, by Assumption 6 and <cit.>, (A.38) of Lemma A.2, we have
	
    ||Å(ğ“) -s(ğ“) ||^2   â‰¤ C_9 h_n 1/p^2q^2||ğš^#||^2 
       = C_10R_1^2/D
       = O_P(1/n)+O_p(R^3+R(p+q)+RD/n)+O_p({âˆ‘_r=1^R_0||vec(ğ_0r)||_1/pq}^2 1/D^2Ï„) 
       =O_p(R^3+R(p+q)+RD/n)+O_p({âˆ‘_r=1^R_0||vec(ğ_0r)||_1/pq}^2 1/D^2Ï„).

	Hence, the proof of Theorem 4 is completed. 
	
	

Â§ APPENDIX REFERENCE

	Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2018). Foundations of ma- chine learning. MIT press.
	0.2cm
	Newey, W. K. and McFadden, D. (1994). Large sample estimation and hypoth- esis testing. Handbook of econometrics, 4:2111â€“2245.
	0.2cm
	Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press.
	0.2cm
	Zhou, Y., Wong, R., and He, K. (2020). Broadcasted nonparametric tensor regression.
