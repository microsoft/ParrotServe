
Modelling self-consistently beyond General Relativity
    Luis Lehner
    March 30, 2023
=====================================================




We propose a novel approach to concentration for non-independent random variables. 
  The main idea is to â€œpretendâ€ that the random variables are independent and pay a multiplicative price measuring how far they are from actually being independent. This price is encapsulated in the Hellinger integral between the joint and the product of the marginals, which is then upper bounded leveraging tensorisation properties. Our bounds represent a natural generalisation of concentration inequalities in the presence of dependence: we recover exactly the classical bounds (McDiarmid's inequality) when the random variables are independent. Furthermore, in a â€œlarge deviationsâ€ regime, we obtain the same decay in the probability as for the independent case, even when the random variables display non-trivial dependencies.
  To show this, we consider a number of applications of interest. First, we provide a bound for Markov chains with finite state space. Then, we consider the Simple Symmetric Random Walk, which is a non-contracting Markov chain, and a non-Markovian setting in which the stochastic process depends on its entire past. To conclude, we propose an application to Markov Chain Monte Carlo methods, where our approach leads to an improved lower bound on the minimum burn-in period required to reach a certain accuracy. In all of these settings, we provide a regime of parameters in which our bound fares better than what the state of the art can provide.





Â§ INTRODUCTION

It is well-known that, given a sequence X^n=(X_1,â€¦, X_n) of independent, but not necessarily identically distributed, random variables with joint measure _X^n, one can prove that for every function f satisfying proper Lipschitz assumptions:
	
    _X^n(|f-_X^n(f)|â‰¥ t) â‰¤ 2exp(-t^2/k â€– fâ€–_Lip^2).

	Here, â€– fâ€–_Lip^2 depends on the metric structure of the measure space, and k is a constant depending on the approach used to prove the inequality, e.g., transportation-cost inequalities, log-Sobolev inequalities, martingale method, see the surveyÂ <cit.>. 
	One notable example is McDiarmid's inequality for functions with â€œbounded jumpsâ€: i.e., if for every x^n,xÌ‚ and every 1â‰¤ i â‰¤ n one has that 
    |f(x_1,â€¦,x_i,â€¦,x_n)-f(x_1,â€¦,xÌ‚,â€¦,x_n)|â‰¤ 
    c_i ,
 then the following holds <cit.>:
	
    _X^n(|f-_X^n(f)|â‰¥ t) â‰¤ 2exp(-2t^2/âˆ‘_i=1^n c^2_i).





	This represents the â€œgolden standardâ€ of concentration. Interestingly, as underlined above, McDiarmid's inequality does not require the X_i's to be identically distributed; it does, however, require the random variables to be independent.
Most of the methods in the literature that tried to relax the latter assumption 
 required the development of novel techniques. However, existing results generally do not recover the rate of decay provided in the independent setting.
 
 In this paper, we present a novel approach that 
 outperforms the state of the art in various settings and regimes. Specifically, we show improved bounds for 
 finite-state space Markov chains (<Ref>), the Simple Symmetric Random Walk (SSRW, <Ref>), a non-Markovian process (<Ref>), and Monte Carlo Markov Chain (MCMC, <Ref>).
  In the case of the SSRW, our improvements are the most dramatic: in sharp contrast with existing techniques, we are able to capture the correct scaling between the distance from the average t, the number of variables n, and the decay probability in the concentration bound. 
We remark that our new method â€“ based on a change of measure argument â€“ is rather flexible and can be employed in most settings. In fact, it only requires the absolute continuity between the joint and the product of the marginals, while existing approaches generally have more restrictive assumptions (e.g., Markovity with stationary distributionÂ <cit.> or contractivityÂ <cit.>). The key idea is to shift the focus 
from proving concentration to bounding an information measure (i.e., the Hellinger integral, seeÂ <Ref>) between the joint distribution and the product of the marginals. Crucially, the Hellinger integral satisfies tensorisation properties that allow us to easily upper bound it, even in high-dimensional settings (seeÂ <Ref>).
We highlight that our approach provides a natural generalisation of the existing concentration of measure results to dependent random variables, in the sense that we recover exactly McDiarmid's inequality when the random variables are independent. Furthermore, for sufficiently large t, namely, in a â€œlarge deviationsâ€ regime, we approach the decay rate (<ref>) for the independent case, even when the random variables are actually dependent.

The rest of the paper is organized as follows. InÂ <Ref>, we discuss related work in the area.
inÂ <Ref> we cover the preliminaries, namely, information measures (<Ref>), Markov kernels (<Ref>), and strong data-processing inequalities (SDPIs, <Ref>). We then provide the main result of this work inÂ <Ref>, which is then applied inÂ <Ref> to four different settings: finite-state space Markov chains (<Ref>), the SSRW (<Ref>) a non-Markovian stochastic process (<Ref>), and MCMC methods (<Ref>). Concluding remarks are provided inÂ <Ref>. Part of the proofs and additional discussions are deferred to the appendices. 















 Â§.Â§ Related Work

 The problem of concentration for dependent random variables has been addressed in multiple ways. The first results in the area are due to MartonÂ <cit.> who heavily relied on transportation-cost inequalities (Pinsker-like inequalities) and an elegant mixture of information-theoretic and geometric approaches. Another important contribution, building upon Marton's work, was given by Samson inÂ <cit.> where some of Marton's results were extended to include Î¦-mixing processes. More recent advances, complementing and generalising the work by Samson and Marton, were provided inÂ <cit.>, where the Martingale method was employed to prove concentration for dependent (but defined on a countable space) processes and inÂ <cit.>, where Marton's couplings were exploited. In particular, the results derived inÂ <cit.> are equivalent to the ones advanced inÂ <cit.> but obtained through couplings rather than linear programming. All of these approaches measure the degree of dependence by looking at distances between conditional distributions (organised in matrices whose norms are then computed, seeÂ <Ref>) or by constructing â€œminimal couplingsâ€ between conditional distributions (seeÂ <Ref>). The resulting quantities, which are necessary in order to 
 analyse the corresponding probabilities, can be difficult to compute, especially in non-Markovian settings. 
 Another approach similar in spirit to ours is given inÂ <cit.>, where a generalisation of Hoeffding's inequality for stationary Markov chains is provided. Other related work can be found inÂ <cit.>. These results aim to establish Hoeffding-like inequalities for Markov chains by relating it to a different Markov chain whose cumulant generating function can be bounded under different assumptions:
 
 Â <cit.> are restricted to discrete and ergodic Markov chains, whileÂ <cit.> extends to general state-space but requires geometric ergodicity. All these generalisations of Hoeffding's inequality do not, however, allow for arbitrary functions of a sequence of random variables, but they are restricted to (sums of) bounded functions applied to each individual sample and they all require the existence of a stationary distribution.  Given that the approach presented inÂ <cit.> is more general than the one proposed inÂ <cit.>, our results will be compared directly with <cit.>.
    Another (less related) approach can be found inÂ <cit.>, where the strength of the dependence is measured in a different way with respect to both this work and the related work mentioned above. Moreover, the approach inÂ <cit.> is mostly restricted to empirical averages of bounded random variables and includes an additional additive factor that grows with the number of samples. Finally, we remark that 
	<cit.> exploits a technique similar to what is pursued in this work, in order to extend McDiarmid's inequality to the case where the function f depends on the random variables themselves (while the random variables remain, in fact, independent). Said result was then applied to a learning setting.


 

Â§ PRELIMINARIES


 In this section, we will define the main objects utilised throughout the document and define the relevant notation.
	We will 
 adopt a measure-theoretic framework.
	Given a measurable space (,â„±) and two measures Î¼,Î½ which render it a measure space, if Î½ is absolutely continuous with respect to Î¼ (denoted with Î½â‰ªÎ¼), then we will represent with dÎ½/dÎ¼ the Radon-Nikodym derivative of Î½ with respect to Î¼. Given a (measurable) function f:â†’â„ and a measure Î¼, we denote with Î¼(f) = âˆ« f dÎ¼ the 
Lebesgue integral of f with respect to the measure Î¼. 
The Radon-Nikodym derivatives represent the main building-block of the following fundamental objects.

	

 Â§.Â§ Hellinger integral, Î±-norm and RÃ©nyi's Î±-divergence

 An important ingredient of this work are information measures. In particular, we will focus on Hellinger integrals which can be seen as a transformation of the L^Î±-norms and of 
 RÃ©nyi's Î±-divergences. 
	

  Â§.Â§.Â§ RÃ©nyi's Î±-divergences

	Introduced by RÃ©nyi as a generalization of the KL-divergence, the Î±-divergence has found many applications in 
 statistical inferenceÂ <cit.>, and it has  
 
 several 
 operational interpretations (e.g., hypothesis testing, and the cut-off rate in block coding <cit.>).
	It can be defined as followsÂ <cit.>.
	
		Let (Î©,,),(Î©,,) be two probability spaces. Let Î±>0 be a positive real number different from 1. Consider a measure Î¼ such that â‰ªÎ¼ and â‰ªÎ¼ (such a measure always exists, e.g., Î¼=(+)/2)) and denote with p,q the densities of , with respect to Î¼. Then, the Î±-divergence of  from  is defined as 
		
    D_Î±():=1/Î±-1logâˆ« p^Î± q^1-Î± dÎ¼.

	

		Definition <ref> is independent of the chosen measure Î¼. 
		In fact, 
		âˆ« p^Î±q^1-Î± dÎ¼ = âˆ«(q/p)^1-Î±d and, whenever â‰ª or 0<Î±<1, we have âˆ« p^Î±q^1-Î± dÎ¼= âˆ«(p/q)^Î±d, see <cit.>. Furthermore, it can be shown that, if Î±>1 and â‰ªÌ¸, then D_Î±()=âˆ. The behavior of the measure for Î±âˆˆ{0,1,âˆ} can be defined by continuity. In general, one has that D_1() = D() which denotes the KL-divergence between  and ; furthermore, if D()=âˆ or there exists Î²>1 such that D_Î²()<âˆ, then lim_Î±â†“1D_Î±(Q)=D()Â <cit.>. For an extensive treatment of Î±-divergences and their properties, we refer the reader toÂ <cit.>. 
 
	
	
	
	

  Â§.Â§.Â§ Ï†-divergences

	Another generalization of the KL-divergence is obtained by considering a generic convex function Ï†:â„^+â†’â„ 
 with the 
 constraint 
 Ï†(1)=0. The constraint can be ignored as long as Ï†(1)<+âˆ by simply considering a new mapping Ï†Ìƒ(x) = Ï†(x) - Ï†(1). 
	
		Let (Î©,,),(Î©,,) be two probability spaces. Let Ï†:â„^+â†’â„ be a convex function such that Ï†(1)=0. Consider a measure Î¼ such that â‰ªÎ¼ and â‰ªÎ¼. Denoting with p,q the densities of the measures with respect to Î¼, the Ï†-divergence of  from  is defined as 
		
    D_Ï†():=âˆ« q Ï†(p/q) dÎ¼.

	
	Despite the fact that the definition uses Î¼ and the densities with respect to this measure, 
 Ï†-divergences are 
 independent from the dominating measure. In fact, 
 when absolute continuity between , holds, i.e., â‰ª,[We will make this assumption throughout the paper.] 
 we obtain 
 <cit.>
	
    D_Ï†()= âˆ«Ï†(d/d)d.


The KL-divergence is retrieved by setting Ï†(t)=tlog(t). Other common examples are the Total Variation distance (Ï†(t)=1/2|t-1|), the Hellinger distance (Ï†(t)=(âˆš(t)-1)^2), and Pearson Ï‡^2-divergence (Ï†(t)=t^2-1). We remark that 
Ï†-divergences do not include the family of RÃ©nyi's Î±-divergences.

	Particularly relevant to us will be the family of parametrized divergences that stems from Ï†_Î±(x)=x^Î± for Î±>1. The function Ï†_Î±(x) is convex on the positive axis for every Î±>1. However, it does not satisfy the property that Ï†(1)=0. Said requirement 
 can 
 be lifted with the consequence of losing the property that _Ï†(Î½Î¼)=0 if and only if Î½ = Î¼. We will call the family of divergences stemming from such functions the Hellinger integrals of order Î±.
	
		Let (Î©,,Î½),(Î©,,Î¼) be two probability spaces, and let 
  Ï†_Î±:â„^+â†’â„ be defined as 
  Ï†_Î±(x)=x^Î±. Let Î¼ and Î½ be two probability measures such that Î½â‰ªÎ¼, then the Hellinger integral of order Î± is given by
		
    H_Î±(Î½Î¼):= D_Ï†_Î±(Î½Î¼) = âˆ«(dÎ½/dÎ¼)^Î± dÎ¼.

	

	    Let us highlight that 
     we are not considering Hellinger divergences of order Î± (including the so-called Ï‡^2-divergence) which consists of divergences stemming from 
     x^Î±-1/(Î±-1), but rather a transformation of said family. In fact, the Hellinger divergences are 
     equal to 0 if and only if the measures coincide. In contrast, the Hellinger integral is equal to 1 if the two (probability) measures coincide. 


	Another notable object for this work will be the 1/Î± power of the Hellinger integral, i.e.,
	
    H_Î±^1/Î±(Î½Î¼)= â€–dÎ½/dÎ¼â€–_L^Î±(Î¼),

	which 
 represents the L^Î±-norm of the Radon-Nikodym derivative with the respect to the measure Î¼.
    Moreover, the following holds 
    <cit.>:
    
    H_Î±^1/Î±(Î½Î¼) = exp(Î±-1/Î±D_Î±(Î½Î¼)).

   
    

 Â§.Â§ Markov kernels

    Most of the comparisons with the state of the art will be drawn in 
    Markovian settings. In this section, we will define the main objects necessary in order to carry out said confrontation.
    
    Let (Î©,â„±) be a measurable space. A Markov kernel K is a mapping K:â„±Ã—Î©â†’ [0,1] such that:
    
        
  * for every xâˆˆÎ©, the mapping Eâˆˆâ„±â†’ K(E|x) is a probability measure on (Î©,â„±);
        
  * for every Eâˆˆâ„± the mapping x âˆˆÎ©â†’ K(E|x) is an â„±-measurable real-valued function.
    
    
    A Markov kernel can be seen as acting on measures â€œfrom the rightâ€, i.e., given a measure Î¼ on (Î©, â„±), 
    
    Î¼ K(E) = Î¼(K(E|Â·)) = âˆ« dÎ¼(x) K(E|x),

    and on functions â€œfrom the leftâ€,  i.e., given a function f:Î©â†’â„,
    
    K f(x) = âˆ« dK(y|x) f(y).

    Given a sequence of random variables (X_n)_nâˆˆâ„•, one says that it represents a Markov chain if, given iâ‰¥ 1,  there exists a Markov kernel K_i such that for every measurable event E:
    
    â„™(X_i âˆˆ E | X_1,â€¦,X_i-1) = â„™(X_i âˆˆ E | X_i-1) = K_i(E|X_i-1)     almost surely.

    If for every iâ‰¥ 1, K_i = K for some Markov kernel K, then the Markov chain is said to be time-homogeneous. Whenever the index is suppressed from K, we will be referring to a time-homogeneous Markov chain. The kernel K of a Markov chain describes the probability of getting from x to E in one step, i.e., for every iâ‰¥ 1, 
    K(E|x) = â„™(X_i âˆˆ E| X_i-1=x). One can then define (inductively) the Îº-step 
    kernel K^Îº as follows:
    
    K^Îº(E|x) = âˆ« K^Îº-1(E|y)dK(y|x).

    Note that K^Îº is also a Markov kernel, and it represents the probability of getting from x to E in Îº steps: K^Îº(E|x)= â„™(X_Îº+1âˆˆ E|X_1=x).
    If (X_n)_nâˆˆâ„• is the Markov chain associated to the kernel K and X_0 âˆ¼Î¼, then Î¼ K^m denotes the measure of X_m+1 at every mâˆˆâ„•. Furthermore, a probability measure Ï€ is a stationary measure for K if Ï€ K(E) = Ï€(E) for every measurable event E. We also note that, if the state space is discrete, then K can be represented using a stochastic matrix. 
    
    Given this dual perspective on Markov operators (acting on measures or functions), one can then study their contractive properties. In particular, let us define 
    
    â€– Kâ€–_Î±â†’Î± := sup_fâ‰  0â€– K fâ€–_Î±/â€– f â€–_Î±.

    Then, Markov kernels are generally contractiveÂ <cit.>, meaning that â€– Kâ€–_Î±â†’Î±â‰¤ 1 and, consequently, â€– Kf â€–_Î±â‰¤â€– f â€–_Î± for every f.
        Similarly, given Î³â‰¤Î±, one can define the following quantity 
    â€– Kâ€–_Î±â†’Î³ := sup_fâ‰  0â€– K fâ€–_Î±/â€– f â€–_Î³.

        It has been proven that many Markovian operators are hyper-contractiveÂ <cit.>, meaning that â€– Kâ€–_Î±â†’Î³â‰¤ 1
        for some Î³ < Î±.  Given a kernel K and Î±>1, we denote by Î³^â‹†_K(Î±) the smallest Î³ such that K is hyper-contractive, i.e., such that  â€– Kâ€–_Î±â†’Î³â‰¤ 1. Said coefficient has been characterised for some Markov operatorsÂ <cit.>. In case the Markov kernel is not time-homogeneous, in order to simplify the notation, instead of denoting the corresponding coefficient with Î³^â‹†_K_i(Î±), we will simply denote it with Î³^â‹†_i(Î±). 
        
        Given a Markov kernel K and a measure Î¼, one can also define the adjoint/dual operator (or backward channel) K^â† as the operator such that
         âŸ¨ g, KfâŸ© = âŸ¨ K^â† g, fâŸ© for all g and fÂ <cit.>. While one can define dual Markovian operators more generally, here we will focus on discrete settings where they can be explicitly specified via K and Î¼Â <cit.>:
    
    K_Î¼^â†(y|x) = K(y|x)Î¼(x)/Î¼ K (y).

     

 Â§.Â§ Strong Data-Processing Inequalities

    An important property shared by divergences is the Data-Processing Inequality (DPI): given two measures Î¼,Î½ and a Markov kernel K, one has that, for every convex Ï†,
    
    D_Ï†(Î½ KÎ¼ K) â‰¤ D_Ï†(Î½Î¼).

    This property holds as well for RÃ©nyi's Î±-divergences, despite them not being a Ï†-divergenceÂ <cit.>.
    DPIs represent a widely used tool and a line of work has focused on tightening them. In particular, in many settings of interest, given a reference measure Î¼, one can show that D_Ï†(Î½ KÎ¼ K) is strictly smaller than D_Ï†(Î½Î¼)  unless Î½=Î¼. Furthermore, the characterization of the ratio D_Ï†(Î½ KÎ¼ K)/D_Ï†(Î½Î¼) has lead to the study of â€œstrong Data-Processing Inequalitiesâ€Â <cit.>.  
    

    Given a probability measure Î¼, a Markov kernel K and  a convex function Ï†, we say that K satisfies a Ï†-type Strong Data-Processing Inequality (SDPI) at Î¼ with constant câˆˆ[0,1) if
    
    D_Ï†(Î½ KÎ¼ K) â‰¤ cÂ· D_Ï†(Î½Î¼),

    for all Î½â‰ªÎ¼. The tightest such constant c is denoted by

    Î·_Ï†(Î¼,K)    = sup_Î½â‰ Î¼D_Ï†(Î½ KÎ¼ K) /D_Ï†(Î½Î¼), 
    Î·_Ï†(K)    = sup_Î¼Î·_Ï†(Î¼,K).


 [SDPI for the KL and the BSC]
     Let Î¼=Ber(1/2), Ïµ<1/2 and K=BSC(Ïµ), i.e., K(y|x) =Ïµ if x=y and K(y|x) =1-Ïµ otherwise. Then, one has that Î·_xlog x(Î¼,K)=(1-2Ïµ)^2Â <cit.>, which implies that  Î·_xlog x(Î¼,K) < 1 for all Ïµ>0. 
  
 While Î·_Ï† can be a difficult object to compute even for simple channels, some universal upper and lower bounds are knownÂ <cit.>:
 
    Î·_Ï†(K) â‰¤sup_x,xÌ‚â€– K(Â·|x)-K(Â·|xÌ‚)â€–_TV = Î·_|x-1|(K)=Î·_TV(K),

  
    Î·_Ï†(Î¼,K) â‰¥Î·_(x-1)^2(Î¼,K) = Î·_Ï‡^2(Î¼,K).

 We remark that these bounds hold for functions Ï† such that Ï†(1)=0 or, equivalently, when the divergence D_Ï†(Î½Î¼) is defined to be Î¼(Ï†(dÎ½/dÎ¼))-Ï†(1). For general convex functions Ï†, as well as for RÃ©nyi's divergences, the DPI holds and SDPI constants are still defined analogously, however one cannot use common techniques to bound said quantities. The following counter-example highlights the issue.
 [Counter-example for Hellinger integrals and RÃ©nyi's divergences] 
     Let Î½=(1/3,2/3) and K_1=BSC(1/3). Then, the stationary distribution Ï€ is given by (1/2,1/2) and Î½ K_1= (5/9,4/9). A direct calculation gives that H_2(Î½ K_1Ï€ K_1) = 82/81 and H_2(Î½Ï€) = 10/9. Moreover, if K=BSC(Î»), one has that Î·_TV(K)=|1-2Î»| (seeÂ <cit.>  andÂ <Ref>). Thus,  
        
    H_2(Î½ K_1Ï€ K_1)/H_2(Î½Ï€) = 41/45 > Î·_TV(K_1) = 1/3,

        which means that the inequality (<ref>) is violated. This is due to the fact that Ï†_2(x)=x^2 is not equal to 0 at x=1. In fact, 
        renormalising H_2 leads to the Ï‡^2-divergence, which satisfies 
    H_2(Î½ K_1Ï€ K_1)-1/H_2(Î½Ï€)-1=Ï‡^2(Î½ K_1Ï€ K_1)/Ï‡^2(Î½Ï€ )=1/9 < Î·_TV(K_1) = 1/3.

        
  Similarly, let K_2=BSC(1/5), which gives that Î·_TV(K_2)=3/5. Consider now D_Î±(K_2(Â·|0)Ï€)=D_Î±(Î´_0 K_2 Ï€ K) = 1/Î±-1log ( 2^1-Î±( 0.2^Î± +(0.8)^Î±)). Moreover, D_Î±(Î´_0Ï€) = log(2). Thus, by setting Î±=6, one has that
     
    Î·_D_Î±(K_2) > D_Î±(Î´_0 K_2 Ï€ K_2)/D_Î±(Î´_0Ï€) = 0.6138 > Î·_TV(K_2) = 0.6,

     which violates again the inequality (<ref>).
 
    

Â§ MAIN RESULT


    
    Let _X^n be the joint distribution of (X_1,â€¦, X_n), _X_i the marginal corresponding to X_i, and _âŠ—_i=1^n X_i the joint measure induced by the product of the marginals.     
  Suppose that (X_1,â€¦, X_n) are Markovian under _X^n, i.e., _X_i|X^i-1 = _X_i|X_i-1 almost surely. If _X^nâ‰ª_âŠ—_i=1^n X_i, for any function f satisfyingÂ <Ref>, any t>0 and 
  Î±>1, 
  one has
  
    
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t )   â‰¤ 2^1/Î²exp(-2t^2/Î²âˆ‘_i=1^n c_i^2)H_Î±^1/Î±(_X^n_âŠ—_i=1^n X_i) 
       â‰¤ 2^1/Î²exp(-2t^2/Î²âˆ‘_i=1^n c_i^2)(âˆ_i=2^nH^Î±_i)^1/Î±,

    with Î²=Î±/(Î±-1), H_i^Î± = _X_i-1^1/Î²_i-1(H_Î±Î±_i^Î²_i-1/Î±_i(_X_i|X_i-1_X_i)), Î±_i>1 for iâ‰¥ 0, Î²_0=1, Î±_n=1 , and Î²_i = Î±_i/(Î±_i-1). 
    
    The proof ofÂ <Ref> is inÂ <Ref>. 
    
    
    If the function f satisfiesÂ <Ref> with c_i=1/n, like in the case of the empirical mean, one obtains 
    
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t ) â‰¤ 2^1/Î²exp(-n(2t^2/Î²-1/nÎ±log H_Î±(_X^n_âŠ—_i=1^n X_i))).

    This means that if 
    t > âˆš(1/2n(Î±-1)log H_Î±(_X^n_âŠ—_i=1^n X_i)),

    thenÂ <Ref> guarantees an exponential decay. If the sign of the inequalityÂ (<ref>) is reversed, then the bound actually becomes trivial, for n large enough. The threshold behavior just described characterises the main difference of this bound with respect to existing approaches: while there are no restrictive assumptions required (other than absolute continuity of the measures at play), the bound can be trivial if the joint distribution is â€œtoo farâ€ from the product of the marginals. In contrast, other approaches, like the one described inÂ <cit.>, do not generally exhibit such a â€œphase transition-likeâ€ behavior. Next, we will characterize the behaviour of the key quantity H_Î±(_X^n_âŠ—_i=1^n X_i) as a function of n 
    in the concrete examples of <Ref>. Before doing that, a 
    few additional remarks are in order. 
    
        The expression on the RHS ofÂ <Ref> can be complicated to compute, especially due to the presence of {Î±_i}_i=2^âˆ. 
        Making a specific choice, which meaningfully reduces the number of parameters (i.e., taking Î±_iâ†’ 1 for every iâ‰¥ 2),Â <Ref> boils down to the following, simpler, expression:
   
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t ) â‰¤ 2^1/Î²exp(-2t^2/Î²âˆ‘_i=1^n c_i^2)Â·âˆ_i=2^n max_x_i-1 H_Î±^1/Î±(_X_i|X_i-1=x_i-1_X_i).

    Moreover,Â <Ref> can be re-written as follows:
    
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t ) â‰¤ 2^1/Î²exp(1/Î²(-2t^2/âˆ‘_i=1^n c_i^2+âˆ‘_i=2^n max_x_i-1 D_Î±(_X_i|X_i-1=x_i-1_X_i))).

    Â <Ref> allows to exploit the SDPI coefficient for D_Î± (seeÂ <Ref>), which in some settings improves upon leveragingÂ <Ref> along with hypercontractivity, seeÂ <Ref>.
    
    
<Ref> can be proved in more generality. Indeed, for any measurable event E, one can say that, for every Î±>1, 
    _X^n(E) â‰¤^1/Î²_âŠ—_i=1^n X_i(E)Â· H_Î±(_X^n_âŠ—_i=1^n X_i).
 Thus, our framework is not restricted to a McDiarmid-like setting, but it can be used to generalise any concentration of measure approach to dependent random variables. The idea is that concentration holds when random variables are independent, namely, _âŠ—_i=1^n X_i(E) decays exponentially in n under suitable assumptions. Then,Â <Ref> shows that a similar 
exponential decay 
holds also in the presence of dependence, as long as the measure of the joint is not â€œtoo farâ€ from the product of the marginals. The â€œdistanceâ€ between joint and product of the marginals is captured by the Hellinger integral H_Î±. In particular, if the joint measure corresponds to the product of the marginals, then H_Î±^1/Î±(_X^n_âŠ—_i=1^nX_i)=1 for every Î±. Thus, taking the limit of Î±â†’âˆ, one recovers 

    _âŠ— X_i(E)= _X^n(E ) â‰¤_âŠ— X_i(E).



On the RHS of bothÂ <Ref>, the probability term is raised to the power Î±-1/Î± and multiplied by the Î±-norm of the Radon-Nikodym derivative. On the one hand, as Î± grows, the Î±-norm grows as well, which increases the Hellinger integral; on the other hand, as Î± grows, Î±-1/Î± tends to 1, which reduces the probability. This introduces a trade-off between the two quantities that renders the optimisation over Î± non-trivial. We highlight that considering the limit of Î±â†’âˆ provides the fastest exponential decay and it recovers the probability for independent random variables. This has the cost of rendering the multiplicative constant larger, and we will discuss in detail the choice of Î± in the various examples ofÂ <Ref>.



 Note that the first inequality in <Ref> holds without the Markovity assumption. The second inequality, instead, leverages tensorisation properties of H_Î± which are particularly suited for Markovian settings. In the general case,  
one can still 
reduce 
H_Î±(_X^n_âŠ—_i=1^n X_i) (a divergence between n-dimensional measures) to n one-dimensional objects, see <Ref> for details about the tensorisation of both the Hellinger integral H_Î± and RÃ©nyi's Î±-divergence D_Î±. 
In particular, following the approach undertaken to deriveÂ <Ref>, one obtains 
   
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t ) â‰¤ 2^1/Î²exp(-2t^2/Î²âˆ‘_i=1^n c_i^2)  Â·âˆ_i=2^n max_x^i-1 H_Î±^1/Î±(_X_i|X^i-1=x^i-1_X_i).

Note thatÂ <Ref>
gives a natural generalisation of concentration inequalities to the case of arbitrarily dependent random variables (just likeÂ <Ref> generalises them to Markovian settings). Indeed, if _X^n=_âŠ—_i X_i, then taking the limit of Î²â†’ 1 in bothÂ <Ref> andÂ <Ref>, one recovers the classical concentration bound for independent random variables (see the discussion inÂ <Ref> recalling that Î²=Î±/(Î±-1)). 

    
    We will now compare our bounds with a number of related works: 1) the approach proposed inÂ <cit.> and based on the martingale method; 2) the approach proposed inÂ <cit.> which leverages properties of contracting Markov chains; and finally 3) the approach advanced inÂ <cit.> showing concentration around the median via transportation-cost and isoperimetric inequalities. 
  

Â§ APPLICATIONS

    Let us now applyÂ <Ref> to four settings:
    
        
  * InÂ <Ref>, we consider a discrete Markovian setting. Here, we specialiseÂ <Ref>  leveraging the  properties of the Markov kernel along with the discrete structure of the problem, 
        thus showing that 
        in certain parameter regimes our bound fares better than what the state of the art can provide;
        
  * InÂ <Ref>, we consider a non-contracting Markovian setting that  does not admit a stationary distribution. Both these properties do not allow the application of most of the existing work in the literature. In contrast, not only our approach can be applied, but it provides exponentially decaying probability bounds, whileÂ <cit.> can only provide an upper bound that does not vanish as n grows;
        
  * InÂ <Ref>, we consider a non-Markovian setting where the entire past of the process influences each step.  Here, to the best of our knowledge, we provide the first bound that exponentially decays in n and has a closed-form expression, while existing approaches either cannot be employed or require the computation of complicated quantities (e.g.,Â <Ref>);
        
  * Finally, inÂ <Ref>, we applyÂ <Ref> to provide error bounds on Markov Chain Monte Carlo methods. Similarly to the other settings, we propose a regime of parameters in which our results fare better and, consequently, provide an improved lower bound on the minimum burn-in period necessary to achieve a certain accuracy in MCMC.
    
  We will hereafter assume, for simplicity of exposition, that c_i=1/n inÂ <Ref> like in the case of the empirical mean. All the results hold for general c_i's, but the expressions and comparisons would become more cumbersome.
 
    

 Â§.Â§ Discrete Markov chains
 
    
    Consider a discrete setting and a Markov chain (X_n)_nâˆˆâ„• determined by a sequence of transition matrices (K_n)_nâˆˆâ„•. Assume that X_1âˆ¼ P_1 and let X_i denote the random variable whose distribution is given by P_1 K_1â€¦ K_i-1.[One can also see X_i as the outcome of X_i-1 after being passed through the channel K_i-1.] 
   
   
    
        For iâ‰¥ 1, suppose K_i is a discrete-valued Markov kernel, and let Î³^â‹†_i(Î±) be the smallest parameter making it hyper-contractive, seeÂ <Ref>.
        Then, for every function f satisfyingÂ <Ref> with c_i=1/n and every Î±>1, 
        
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t) â‰¤ 2^1/Î²exp(-2nt^2/Î²+âˆ‘_i=1^n-1( logâ€– K_i^â†â€–_Î±â†’Î³^â‹†_i(Î±) - 1/Î³Ì…_i^â‹†(Î±)min_jâˆˆsupp(_i)log P_i(j))).

        Moreover, if the Markov kernel is time-homogeneous, i.e., K_i=K for every iâ‰¥ 1, then 
        
    _X^n   (| f-_âŠ—_i=1^n X_i(f)|â‰¥ t)
       â‰¤ 2^1/Î²exp(-2nt^2/Î²+(n-1) logâ€– K^â†â€–_Î±â†’Î³^â‹†_K(Î±) - 1/Î³Ì…^â‹†_K(Î±)âˆ‘_i=1^n-1(min_jâˆˆsupp(_i)log P_i(j)))
       â‰¤ 2^1/Î²exp(-2nt^2/Î²+(n-1) logâ€– K^â†â€–_Î±â†’Î³^â‹†_K(Î±) - n-1/Î³Ì…^â‹†_K(Î±)(min_i=1,â€¦,(n-1)min_jâˆˆsupp(_i)log P_i(j))).

    In the above equations Î³Ì…_i^â‹†(Î±) and Î³Ì…_K^â‹†(Î±) denote the HÃ¶lder conjugates of, respectively, Î³_i^â‹†(Î±) and Î³_K^â‹†(Î±).
    


        The main object one has to bound, according toÂ <Ref>, is the following:
    
    max_x_i-1 H_Î±(_X_i|X_i-1=x_i-1_X_i),  with  iâ‰¥ 2.

From the properties of the Markov kernel, one has that
P_X_i = P_X_i-1 K_i-1. Furthermore, P_X_i|X_i-1=x_i-1 can be seen as Î´_x_i-1 K_i-1 where Î´_x_i-1 is a Dirac-delta measure centered at x_i-1. Thus, recalling the definition of 
Î³^â‹†_i-1(Î±) from 
<Ref>, 

    H^1/Î±_Î±(_X_i|X_i-1=x_i-1_X_i)     = H^1/Î±_Î±(Î´_x_i-1K_i-1_X_i-1K_i-1)
       = â€–dÎ´_x_i-1K_i-1/d_X_i-1K_i-1â€–_L^Î±(_X_i-1K_i-1)
       â‰¤â€– K_i-1^â†â€–_Î±â†’Î³^â‹†_i-1(Î±) H_Î³^â‹†_i-1(Î±)^1/Î³^â‹†_i-1(Î±)(Î´_x_i-1_X_i-1),

whereÂ <Ref> follows fromÂ <cit.>. 
Moreover, for every Îº>1,
 
    H^1/Îº_Îº(Î´_x_i-1_X_i-1) = _X_i-1({x_i-1})^1-Îº/Îº = _X_i-1({x_i-1})^-1/ÎºÌ…,

 where ÎºÌ… denotes the HÃ¶lder's conjugate of Îº and _X_i-1({x_i-1}) the measure that _X_i-1 assigns to the point x_i-1.
 Thus, the following sequence of steps, along withÂ <Ref>, concludes the argument:
 
    âˆ_i=2^n max_x_i-1 H^1/Î±_Î±(_X_i|X_i-1=x_i-1_X_i)    â‰¤âˆ_i=2^nmax_x_i-1â€– K^â†_i-1â€–_Î±â†’Î³^â‹†_i-1(Î±) H^1/Î³^â‹†_i-1(Î±)_Î³^â‹†_i-1(Î±)(Î´_x_i-1_X_i-1) 
        = (âˆ_i=2^nâ€– K^â†_i-1â€–_Î±â†’Î³^â‹†_i-1(Î±))(âˆ_i=2^n  max_x_i-1(_X_i-1({x_i-1})^-1/Î³Ì…^â‹†_i-1(Î±)))
       = (âˆ_i=1^n-1â€– K^â†_i â€–_Î±â†’Î³^â‹†_i(Î±))(âˆ_i=1^n-1(min_x_i_X_i({x_i}))^-1/Î³Ì…^â‹†_i(Î±)).

 Moreover, given the discrete setting, one can replace the measure _X_i with the corresponding pmf which is denoted by P_i.
 the norm which gives the for a given 

    H^1/Î±_Î±(_X_i|X_i-1=x_i-1_X_i) = H^1/Î±_Î±(Î´_x_i-1K_X_i-1K) â‰¤ c H^1/Î±_Î±(Î´_x_i-1_X_i-1),

 with câ‰¤ 1. The inequality inÂ <Ref> follows from interpreting H_Î± in one of two ways:
 
     
  * as a Ï†-divergence, then the inequality follows from the Data-Processing Inequality that H_Î± satisfies (a consequence itself of the convexity of x^Î± and Jensen's inequality). In this case c can be seen as the â€œStrong Data-Processing Inequality Coefficientâ€ of H_Î± raised to the 1/Î± powerÂ <cit.>;
     
  * as the L^Î±-norm of the Radon-Nikodym derivative and the inequality following from contractivity of the Markov Oeprators K and K^â† (along with the fact that dÎ½ K/dÎ¼ K = K^â†(dÎ½/dÎ¼)Â <cit.>). In this case c can be seen as the contractive coefficient of the operators K and the induced adjoint K^â† according toÂ <Ref>.  
 
 
   
        If 
        the Markov kernel K_i is only contractive (and not hyper-contractive), then 
        Î³^â‹†_i(Î±)=Î± and Î³Ì…^â‹†_i(Î±)=Î², which allows to simplify <Ref>. 
 
  
   
    
     
        
      
       
       
       





   In this case, 
if 
   
    t^2    â‰¥Î²/2n-1/nlogâ€– K^â†â€–_Î±â†’Î± - n-1/2n(min_i min_j log P_i(j))
       = (1+o_n(1))(Î²/2logâ€– K^â†â€–_Î±â†’Î±^Î² -1/2(min_i min_j log P_i(j)) ),

   then <Ref> gives 
   exponential (in n) concentration even in the case of dependence. 
   
   
   
    
    Another perspective naturally 
    stems fromÂ <Ref>. Indeed, similarly toÂ <Ref>, one has 
    
        
    _X^n(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t)     â‰¤ 2^1/Î²exp(-1/Î²(2nt^2-âˆ‘_i=2^n max_x_i-1 D_Î±(_X_i|X_i-1=x_i-1_X_i)))
       â‰¤ 2^1/Î²exp(-1/Î²(2nt^2-Î·_Î±(K)  âˆ‘_i=2^nmax_x_i-1 D_Î±(Î´_x_i-1_X_i-1))) 
        = 2^1/Î²exp(-1/Î²(2nt^2+Î·_Î±(K)  âˆ‘_i=2^nmin_x_i-1log P_i-1(x_i-1))).

    We remark that 
    D_Î± can also be hyper-contractive with respect to some Markovian operators, meaning that inÂ <Ref> for instance, one could consider D_Î³(Î´_x-1_X_i-1) with Î³ < Î± (this is equivalent to hyper-contractivity of Markov operators,Â <cit.>). One such example is the Ornsteinâ€“Uhlenbeck channel with noise parameter t, cf.Â <cit.>, for which one can prove hyper-contractivity with respect to D_Î±Â <cit.>. Moreover, in some settings, leveraging SDPIs for D_Î± can provide an improvement overÂ <Ref>, see <Ref> for a detailed
    comparison.
    
   
    We now compare the concentration bound provided by <Ref> with existing bounds in the literature. In this section, the comparison concerns a general Markov kernel, and the explicit calculations for a binary kernel are deferred to <Ref>. 
    
    
    

  Â§.Â§.Â§ Comparison with <cit.>
  Let us consider the same setting as inÂ <Ref>. Then, <cit.> gives 

    
    â„™(|f-_X^n(f)|â‰¥ t ) â‰¤ 2 exp(-nt^2/2 M_n^2),

    where M_n = max_1â‰¤ iâ‰¤ n-1(1+âˆ‘_j=i^n-1âˆ_k=i^j Î·_KL(K_k) ) and we recall that Î·_KL(K_i)=sup_x,xÌ‚TV(K_i(Â·|x),K_i(Â·|xÌ‚)) is the contraction coefficient of the Markov kernel K_i. First, note that, 
    if the random variables are independent and thus _X^n=_âŠ— X_i, thenÂ <Ref> reduces to 
    _X^n(E) â‰¤ 2 exp(-nt^2/2),
 whileÂ <Ref> with Î³^â‹†_K(Î±)=Î±â†’âˆ and Î³Ì…^â‹†_K(Î±)=Î²â†’ 1 recovers McDiarmid's inequality with the correct constant in front of n, i.e.,
    
    _X^n(E) â‰¤ 2 exp(-2nt^2).

 
    
    Assume now that the Markov kernel is time-homogeneous and has a contraction coefficient Î·_TV(K) < 1. Then, M_n = max_1â‰¤ iâ‰¤ n-11-Î·_TV(K)^n+1-i/1-Î·_TV(K)= 1-Î·_TV(K)^n/1-Î·_TV(K). For compactness, define P_i^â‹†(j^â‹†):=min_imin_j P_i(j). Making a direct comparison, one has that if 
    
    t^2    > n-1/n(2M_n^2/4M_n^2-Î²)logâ€– K^â†â€–_Î±â†’Î±^Î²/P_i^â‹†(j^â‹†)
       = (1+o_n(1)) (2/4-Î²(1-Î·_TV(K))^2)logâ€– K ^â†â€–_Î±â†’Î±^Î²/P_i^â‹†(j^â‹†),

    thenÂ <Ref> (with Î³^â‹†_K(Î±)=Î± and Î³Ì…^â‹†_K(Î±)=Î²) provides a faster exponential decay thanÂ <Ref>. The explicit calculations for the special case of a binary kernel are provided inÂ <Ref>.

  
  Considering this setting,  one has the following bound:

    
    _X^n(E) â‰¤ 2^1/Î²exp(-1/Î²(2nt^2- (n-1)log(m â€– Kâ€–_Î±â†’Î±^Î²))).

    Consequently, one can guarantee an exponential decay whenever 
    
    t^2 > (1+o_n(1))log(m â€– Kâ€–_Î±â†’Î±^Î²)/2.

    However, in this setting one can say something more specific. In particular, one can compute the ratio 
    H^1/Î±_Î±(_X_i|X_i-1=x_i-1_X_i)/H^1/Î±_Î±(Î´_x_i-1_X_i-1) = r_Î±(i)
 explicitly. 
    K is a mÃ— m matrix characterised by a vector Î»Ì… of m parameters Î»_j such that âˆ‘_j=1^m Î»_j=1 and each row is a permutation of these parameters in such a way that every column also sums to 1 (Da elaborarci su, conseguenza del Birkhoffâ€“von Neumann theorem? Non sono 100% sicuro che sia corretto o se sia da assumere. Anche la notazione non mi piace r_Î±(i) ma poi ci riflettiamo). Hence one can see that 
    
    r_Î±(i) = â€–Î»Ì…â€–_â„“^Î± = (âˆ‘_j=1^m Î»_j^Î±)^1/Î±.
  This implies that one does not need the quantity â€– K â€–_Î±â†’Î± and can provide the following, tighter bound:
    
    _X^n(E) â‰¤ 2^1/Î²exp(-1/Î²(2nt^2- (n-1)log(m â€–Î»Ì…â€–^Î²))).

    We will now compareÂ <Ref>, with m=2 for simplicity (Controllo poi se posso ricalcolare tutto per m generale negli altri lavori e fare un confronto senza settare m=2. Un altro motivo per considerare m=2 viene dal fatto che si puÃ² calcolare esplicitamente l'ipercontrattivitÃ  perche il caso della DSBS Ã¨ stato ampliamente studiato)., with the current state of the art. 
    One can then consider the limit of Î±â†’âˆ which renders Î²â†’ 1. On the one hand, this implies a larger multiplicative coefficient (as H_Î± grows with Î±, seeÂ <Ref>) and, consequently, it increases the minimum value of t one can consider inÂ <Ref>. On the other hand, it guarantees a faster exponential decay inÂ eq:generalResultDiscreteHomogeneousHypereq:generalResultDiscreteHomogeneous2Hyper. 
 In fact, as Î²â†’ 1, the RHS ofÂ (<ref>) scales as exp(-2nt^2(1+o_n(1))) for large enough t, which matches the behavior ofÂ <Ref>. 
 Let us highlight that, to the best of our knowledge, our approach is the first to recover the same exponential decay rate obtained in the independent case, even in the presence of correlation among the random variables. 
 
We remark that the convergence results in <Ref> and <Ref> are with respect to different constants: <Ref> considers the concentration of f around _âŠ—_i=1^n X_i(f), whileÂ <Ref> around _X^n(f). 
However, given the faster rate of convergence guaranteed by our framework â€“ for this example and, even more impressively so, for the one inÂ <Ref> â€“ the mean of f under the product of the marginals might be regarded as a natural object to consider when proving concentration results for these processes.  
    
    This hypothesis is corroborated by the approach presented inÂ <cit.>, where concentration for stationary Markov chains is provided around the mean with respect to the stationary measure Ï€. Indeed, when X_1âˆ¼Ï€, the product of the marginals reduces to the tensor product of the stationary measure Ï€^âŠ— n. 
   
 
    To make a direct comparison withÂ <cit.>, one can leverageÂ <cit.> (reproduced in <Ref>) and either reduce both results to concentration bounds around the median, or transformÂ <Ref> in a result on concentration around the mean with respect to _X^n. This would introduce additional constants, rendering the comparison cumbersome and outside the scope of this work. 
 

     

  Â§.Â§.Â§ Comparison with <cit.>
 
     
<cit.> considers a more restricted setting in which f is the sum of n bounded functions that separately act on each of the n random variables, i.e., f=âˆ‘_i=1^n f_i(X_i) with f_i âˆˆ [a_i,b_i].[This choice of f inÂ <Ref> transforms the result in a generalisation of Hoeffding's inequality to dependent settings. It is easy to see that if f_iâˆˆ [a_i,b_i] thenÂ (<ref>) holds with c_i = (b_i-a_i) and the statement follows.] By assuming further that a_i=0 and b_i=1/n for every i (e.g., empirical mean), we are in a setting in whichÂ <Ref> holds as it is. Moreover, the setting in <cit.> requires the Markov chain to be time-homogeneous and admit a stationary distribution Ï€ (notice that none of these assumptions are necessary for TheoremsÂ <ref> andÂ <ref> to hold). In this caseÂ <cit.> gives:

    _X^n(|f -  Ï€^âŠ— n(f)| â‰¥ t ) â‰¤ 2 exp(-1-Î»/1+Î»2nt^2),

where (1-Î») denotes the absolute spectral gap of the Markov chain, seeÂ <cit.>, which characterizes the speed of convergence to the stationary distribution. Comparing withÂ <Ref> with Î³^â‹†_K(Î±)=Î±â†’âˆ and Î³Ì…^â‹†_K(Î±)=Î²â†’ 1,[As mentioned earlier, this optimizes the rate of convergence, at the expense of the value of t from which we obtain an improvement.] one has that if

    t^2 > (1+o_n(1))1+Î»/4Î»logâ€– K^â†â€–_âˆâ†’âˆ/P_i^â‹†(j^â‹†),

thenÂ <Ref> provides a faster decay thanÂ <Ref>. A more explicit comparison in which the absolute spectral gap is computed for a binary Markov kernel can be found inÂ <Ref>.
   


  Â§.Â§.Â§ Comparison with <cit.>
    
    Let us now derive the corresponding result of concentration around the median in order to compare withÂ <cit.>. LeveragingÂ <cit.> (see also <Ref>) along withÂ <Ref> (again, with Î³^â‹†_K(Î±)=Î±â†’âˆ and Î³Ì…^â‹†_K(Î±)=Î²â†’ 1), we have 
    
    _X^n(|f-m_f|â‰¥ t) 
     â‰¤ 2 exp(-2n(t-âˆš(ln4+C_n/2n))^2+C_n),

    where C_n=(n-1)log(â€– K^â†â€–_âˆâ†’âˆ/P_i^â‹†(j^â‹†)). 
    This also implies (see, e.g.,Â <cit.> orÂ <cit.>) that, if t>âˆš(log4+C_n/2n) and given any event E such that _X^n(E)â‰¥1/2, then 
    
    _X^n(E^c_t)    â‰¤ 2 exp(-2n(t-âˆš(log4+C_n/2n))^2+C_n)
       = 1/2exp(-2nt^2+2tâˆš(2n(log4+C_n))),

    where E_t = {yâˆˆ^n : d(x,y)â‰¤ t  for some  xâˆˆ E} and d denotes the normalised Hamming metric. 
    
    Let us denote 
    a:=1-max_i max_x,xÌ‚ TV(_X_i|X_i-1=x,_X_i|X_i-1=xÌ‚).
    
    Hence, assuming t>1/aâˆš(log(1/_X^n(E))/n), <cit.> give
    
    _X^n(E^c_t)    â‰¤exp(-2n(at -âˆš(log(1/_X^n(E))/2n))^2) 
       â‰¤exp(-2nt^2a^2+2taâˆš(2nlog2)).

    Ignoring multiplicative constants and comparing the exponents ofÂ (<ref>) andÂ (<ref>), one can see that, whenever
     
    t â‰¥(1+o_n(1))âˆš(2log(â€– K^â†â€–_âˆâ†’âˆ/P_i^â‹†(j^â‹†)))/(1-a^2)
    ,
 
    
    t â‰¥âˆš(2ln2)(1-a)-âˆš(2(ln4+C_n))/âˆš(2n)(1-(1-a)^2),
    then our approach improves upon <cit.>. 
    
    The Dobrushin coefficient of the kernel, captured by the quantity (1-a), measures the degree of dependence of the stochastic model. The smaller 1-a is, the less â€œdependentâ€ the model is. If _X^n reduces to a product distribution then a=1. In this case, <Ref> 
    boils down to McDiarmid's inequality. In contrast, the larger 1-a is, the worse the behavior ofÂ <Ref>. If a=0, then the Markov chain is not contracting and violates the assumption ofÂ <cit.>. Our approach, instead, can still provide meaningful results, as we will see inÂ <Ref>. A more explicit comparison for the case of a binary kernel can be found inÂ <Ref>.
    
    

 Â§.Â§ A non-contracting Markov chain

     In order to provide concentration for Markov chains, existing work 
    requires either contractivity of the Markov chain
    Â <cit.>, stationarity
    Â <cit.> or some form of mixingÂ <cit.>.
    A well-known Markov chain that evades most of these concepts is the SSRW. Suppose to have a sequence of i.i.d. Rademacher random variables, i.e., â„™(X_i =  -1)=â„™(X_i= +1)=1/2, for iâ‰¥ 1; the initial condition is X_0=0 w.p. 1. Then, a SSRW is the Markov chain (S_i)_iâˆˆâ„• defined as S_i = S_i-1 + X_i. 
    This Markov chain does not admit a stationary distribution, it is not contracting, but it is expanding (in this case, both towards the positive and the negative axes of the real line). Let us denote with K_i the kernel at step i. Then, at each step i>2, one can always find two different realisations of S_i-1, let us call them s_1,s_2, such that  supp(K_i(Â·|s_1))âˆ©supp(K_i(Â·|s_2))=âˆ…, i.e., the support of S_i is constantly growing. 
    This implies that the approaches inÂ <cit.> cannot be employed, whileÂ <cit.> yields:  
    
    â„™(| f-_X^n(f)|â‰¥ t) â‰¤ 2exp(-t^2/2n).

    A meaningful regime (given also the expanding nature of the Markov chain along the integers)[The standard deviation of S_n is âˆš(n), hence it is expected that S_n is Â± O(âˆš(n)).] 
    arises when considering t â‰³âˆš(n).
    Let us now compute the Hellinger integral in this specific setting. This is done in the lemma below, proved inÂ <Ref>.
    
    Let iâ‰¥ 1, xâˆˆsupp(S_i-1), 0â‰¤ jâ‰¤ i, and Î±â‰¥ 1. Then,
    
    H_Î±^1/Î±(_S_i|S_i-1=x_S_n) â‰¤ 2^i 1/Î²-1+1/Î±,

        and thus



    
    H_Î±^1/Î±(_S^n_âŠ—_j=1^n S_j) â‰¤ 2^n(n-1)/2Î².

    
   
   Denoting  with 1/Î²= Î±-1/Î±,Â <Ref> implies that
    
    H_Î±^1/Î±(_S^n_âŠ—_j=1^n S_j)    â‰¤âˆ_i=2^n max_xâˆˆsuppS_i-1 H_Î±^1/Î±(_S_i|S_i-1=x_S_i) 
       â‰¤âˆ_i=2^n 2^1/Î±-1 + i/Î²
       = 2^1/Î²âˆ‘_j=1^n-1 j = 2^n(n-1)/2Î².
CombiningÂ <Ref> andÂ <Ref>, one has that
    
    â„™(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t )â‰¤ 2^1/Î²exp(-2nt^2/Î²+ n(n-1)/2Î²ln 2).

    It is easy to see that, whenever t > âˆš((n-1)ln(2))/2, <Ref> gives an exponential decay. For instance, choosing t =âˆš(n),  
    one retrieves
    
    â„™(| f-_âŠ—_i=1^n X_i(f)|â‰¥âˆš(n)) â‰¤ 2^1/Î²exp(-n^2/Î²(2 - ln2/2 +ln2/2n)).

    In contrast, the same choice inÂ <Ref> gives
    
    â„™(| f-_X^n(f)|â‰¥âˆš(n)) â‰¤ 2exp(-1/2).

    More generally, selecting t of order âˆš(n) suffices to achieve an exponential decay inÂ <Ref>, while to obtain a similar speed of decay inÂ <Ref> t needs to be at least of order n. The approach advanced in this work can, thus, not only be employed in settings where most of the other approaches fail (e.g.,Â <cit.>), but it also brings a significant improvement over the rate of decay that one can provide.
    

 Â§.Â§ A non-Markovian Process
 
    Next, we consider a non-Markovian setting in which each step of the stochastic process depends on its entire past: 

    
    X_n = 
        +1,     with probability âˆ‘_i=0^n-1p_iX_i, 
    
        -1,     with probability  
        1 - âˆ‘_i=0^n-1p_iX_i,

    for nâ‰¥ 1, p_i>0 and X_0=+1 with probability 1.
    The choice of the parameters p_i is arbitrary (given the constraint that âˆ‘_i=0^n-1 p_i < 1) but for concreteness we will set p_i = 2^-i-1 for every iâ‰¥ 0. Then, â„™_X_1(1|x_0)=1/2=â„™_X_1(-1|x_0) and for each nâ‰¥ 1,
    
    â„™_X_n(1|x_0^n-1)=1/2+ âˆ‘_i=1^n-1p_ix_i= âˆ‘_i=0^n-1x_i 2^-i-1= 1- â„™_X_n(-1|x_0^n-1),
 
    
    with x_0=1.
    Consequently, following the calculations detailed inÂ <Ref>, we have
    
    H_Î±(_X_n(Â·|x_0^n-1) (1/2,1/2)) 
           < 2âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j(2âˆ‘_i=1^n-1p_ix_i)^2j,


which, as p_i = 2^-i-1, gives 
    
    max_x_1^n-1H_Î±(_X_n(Â·|x_0^n-1) (1/2,1/2))  < 2^Î±.
 Thus, H_Î±^1/Î±(_X^n(1/2,1/2)^âŠ— n)< 2^n-1 and an application of Theorem <ref> yields:
    
    â„™({| f-_âŠ—_i=1^n X_i(f)|â‰¥ t}) â‰¤inf_Î²>1 2^1/Î²exp(-2n/Î²(t^2-n-1/nÎ²ln2/2)),

    with exponential decay whenever
    
    t^2 > (1+o_n(1))Î²ln2/2.

    
    Like before, choosing a larger Î² slows down the exponential decay, but it reduces the multiplicative coefficient introduced via H_Î±. For n 
    and t large enough, one can pick Î²â†’ 1 and retrieve a McDiarmid-like exponential decay.
     Given that this setting does not characterise a Markovian dependence (at each step the stochastic process depends on its entire past), one cannot employ the technique described inÂ <cit.> or inÂ <cit.>. One can, however, employ the technique described inÂ <cit.> andÂ <cit.>. Both these approaches require the computation of {Î¸Ì…_ij}_1â‰¤ i < j â‰¤ n with
    
    Î¸Ì…_ij:=   sup_x^i-1, w,Åµâ€–â„’(X_j^n|X^i=(x^i-1, w)) - â„’(X_j^n|X^i=(x^i-1,Åµ)) â€–_TV,

    where â„’(X_j^n|X^i=(x^i-1,w)) denotes the conditional distribution of X_j^n given X^i=(x^i-1,w). The Î¸Ì…'s are then organised in nÃ— n upper-triangular matrices whose norms are computed in order to provide an upper bound on the probability of interest. In particular,Â <cit.> requires the â„“_2-norm, whileÂ <cit.> requires the operator norm of the matrix.  
    
    â„’(X_j^n = x_j^n|X^i=x^i-1w) = âˆ‘_x_i+1^j-1âˆ_k=i+1^n (1/2 + x_k(âˆ‘_mâ‰  i^k-1 p_m x_m + p_iw )).

    Even for the simpler case j=i+1, computingÂ <Ref> is not easy. Indeed, one has  that:
    
    â„’(X_i+1^n = x_i+1^n|X^i=x^i-1w) =  âˆ_k=i+1^n (1/2 + x_k(âˆ‘_mâ‰  i^k-1 p_m x_m + p_iw )).

    Moreover,denoting with a_i+Ï„ = 1/2 + x_i+Ï„âˆ‘_mâ‰  i^k-1p_mx_m, with ğ’®^i = {1,â€¦, n-i} and with ğ’®^i_q the set of all the subsets of ğ’®^i of size q one can see that
    
    Î·Ì…_i,i+1   = sup_(x^i-1âˆˆ ,w,Åµ)1/2|âˆ‘_x_i+1^nâˆ‘_p=1^n-iâˆ‘_q=1^n-ip(w^|ğ’®^iâˆ–ğ’®^i_q|-Åµ^|ğ’®^iâˆ–ğ’®^i_q|) p_i^|ğ’®^iâˆ–ğ’®^i_q|âˆ_Ï„âˆˆğ’®^i_q a_i+Ï„âˆ_Ï„Ìƒâˆˆğ’®^iâˆ–ğ’®^i_q x_i+Ï„Ìƒ|

    and it is not clear, a priori, how to compute such a supremum with respect to w,Åµ and x^i-1. Picking a general j>i renders the computations even more difficult.Similarly, to employ the technique provided inÂ <cit.> one would need to compute the following quantity
    
    CÌ… = max_1â‰¤ i â‰¤ nsup_x^i-1âˆˆ^i-1
     w,Åµâˆˆinf_Ï€âˆˆÎ (â„’(X^n|x^i,w), â„’(X^n|x^i,Åµ))Ï€(d).

    Here, d denotes the normalised Hamming metric and Î (â„’(X^n|x^i,w), â„’(X^n|x^i,Åµ)) represents the set of all the couplings defined on ^nÃ—^n such that the corresponding marginals are â„’(X^n|x^i,w) and â„’(X^n|x^i,Åµ). However, computing any of these objects in practice can be complicated even in simple settings. In contrast, with the framework proposed here and thanks to the tensorisation properties of the Hellinger integral, one can easily bound the information measure and provide an exponentially  decaying probability (whenever the probability of the same event under independence decays exponentially and for opportune choices of the parameters).
    
    

 Â§.Â§ Markov Chain Monte Carlo

    An intriguing application of the method proposed inÂ <cit.> consists in providing error bounds for Markov Chain Monte Carlo methods. For instance, assume that one is trying to estimate the mean Ï€(f)  for some function f and some measure Ï€ which cannot be directly sampled. A common approach consists in considering a Markov chain {X_i}_iâ‰¥ 1 whose stationary distribution is Ï€ and estimating Ï€(f) via empirical averages of samples {X_i}_n_0+1^n_0+n, where n_0 characterises the so-called â€œburn-in periodâ€. This period ensures that enough time has passed and the Markov chain is sufficiently close to the stationary distribution Ï€ before sampling from it.Â <cit.> gives that 
    
    â„™(1/nâˆ‘_i=1^n f(X_n_0+i)-Ï€(f) > t ) â‰¤ C(Î½,n_0,Î±)exp(-1/Î²Â·1-max{Î»_r,0}/1+max{Î»_r,0}Â·2nt^2/(b-a)^2),

    where f:â†’ [a,b] is uniformly bounded, Î»_r represents the right-spectral gap (seeÂ <cit.>), Î±âˆˆ(1,+âˆ), Î² denotes its HÃ¶lder's conjugate and C is a constant depending on the burn-in period n_0, the Radon-Nikodym derivative between the starting measure Î½ and the stationary measure Ï€, and Î±.
    Using the tools provided in this work, we obtain:
      
    â„™(1/nâˆ‘_i=1^n f(X_n_0+i)-Ï€(f) > t )    â‰¤exp(-2nt^2/Î²(b-a)^2) H_Î±^1/Î±(Î½ K^n_0Ï€) âˆ_i=2^n max_x_n_0+i-1 H_Î±^1/Î±(K(Â·|x_n_0+i-1)Ï€)
       â‰¤ C(Î½,n_0,Î±)exp(-2nt^2/Î²(b-a)^2)max_xÏ€({x})^-n-1/Î² ,

   whereÂ <Ref> follows from the fact that H^1/Î±_Î±(Î½ K^n_0Ï€) represents the L^Î±(Ï€)-norm of the Radon-Nikodym derivative and can thus be bounded like inÂ <cit.>.
  Using the tools provided in this work, we obtain 
    
    â„™(1/nâˆ‘_i=1^n f(X_n_0+i)-Ï€(f) > t ) â‰¤exp(-2nt^2/Î² (b-a)^2)H_Î±^1/Î±(_X_n_0+1â€¦ X_n_0+nÏ€^âŠ— n).
The idea behind the result is as follows. Given that one is trying to estimate the mean of f under Ï€ using empirical averages, if one had samples taken in an i.i.d. fashion from Ï€, the exponential convergence would be guaranteed. However, the issue is that one does not have access to samples of Ï€. Thus, changing the measure to an n-fold tensor product of Ï€, one can still guarantee an exponential decay at the cost of a multiplicative price depending on how far the samples are from the stationary distribution.
    By making a direct comparison,  assuming Î»_r>0, one can see that if 
    t^2    â‰¥n-1/n(b-a)^2/21+Î»_r/2Î»_rlog(1/min_x Ï€({x}))
       =(1+o_n(1))(b-a)^2/21+Î»_r/2Î»_rlog(1/min_x Ï€({x})),
 then the RHS ofÂ <Ref> decays faster than the RHS ofÂ <Ref>. A comparison for the binary symmetric channel, with the computations of all the parameters,  can be found inÂ <Ref>. 
Let K be a discrete contractive Markov kernel (as inÂ <Ref>) and, for simplicity, take n_0=0 (no burn-in period). Pick qâ†’ 1 inÂ <Ref>, which gives that C= â€–dÎ½/dÏ€â€–_L^âˆ(Ï€), i.e.,
   
    â„™(1/nâˆ‘_i=1^n f(X_i)-Ï€(f) > t ) â‰¤exp(-1-max{Î»_r,0}/1+max{Î»_r,0}Â·2nt^2/(b-a)^2)â€–dÎ½/dÏ€â€–_L^âˆ(Ï€).

Furthermore, <Ref> can be rewritten as
     
    â„™(1/nâˆ‘_i=1^n f(X_i)-Ï€(f) > t )    â‰¤exp(-2nt^2/Î²(b-a)^2) H_Î±^1/Î±(Î½Ï€) âˆ_i=2^n max_x_i-1 H_Î±^1/Î±(K(Â·|x_n_0+i-1)Ï€)
       â‰¤exp(-2nt^2/Î²(b-a)^2)H_Î±^1/Î±(Î½Ï€) max_xÏ€({x})^-n-1/Î²  .

    Taking then the limit of Î±â†’âˆ (which allows to compare the fastest exponential decays between our approach and <cit.>), one has 
    
    â„™(1/nâˆ‘_i=1^n f(X_i)-Ï€(f) > t ) â‰¤exp(-2nt^2/(b-a)^2)  â€–dÎ½/dÏ€â€–_L^âˆ(Ï€)(1/min_xÏ€({x}))^n-1.



    
    
    Similarly toÂ <cit.>, one can also show that an exponential decay is guaranteed inÂ <Ref> if n_0 = Î©(log n). Furthermore, asÂ <Ref> improves the exponential decay for t satisfyingÂ <Ref>, in the same regime the induced lower bound over n_0 will be improved as well. Finally, we highlight that  
    <Ref> applies to a much larger family of functions f than what can be handled by  <cit.>. 
    
    
    
   
    
    n_0 > Î±/2log(1/Î»)log(2^2/Î²â€–dÎ½/dÏ€-1â€–_L^Î±(Ï€)/2nt^2/d^2Î²+n-1/Î²logÏ€^â‹†),

   where, similarly to before, d^2=(b-a)^2 and Ï€^â‹† denotes max_x Ï€({x}). Moreover, in the same regime of t as the one depicted inÂ <Ref> thenÂ <Ref> improves over the lower bound which can be achieved from Â <cit.>.
    L'espressione mi sembra abbia senso, se lo spectral  gap Î» Ã¨ piccolo, allora la MC converge velocemente alla stazionaria ed n_0 diminuisce. PiÃ¹ campioni considero (n grande) meno vincoli ho su n_0. Se Î½=Ï€ allora â€–dÎ½/dÏ€-1â€–_L^Î±(Ï€)=0 e non ho vincoli su n_0.
    
    Da qui in poi non so bene cosa voglio ottenere, piÃ¹ o meno capire cosa posso ottenere con SDPI
    
    We also know that 
    
    H^1/Î±_Î±(Î½ K^n_0Ï€ )    =H^1/Î±_Î±(Î½ K^n_0Ï€ K^n_0 ) 
       = ((Î±-1)â„‹_Î±(Î½ K^n_0Ï€ K^n_0) + 1)^1/Î±
       â‰¤ ((Î±-1)Î·_Î±(K^n_0)â„‹_Î±(Î½Ï€) + 1)^1/Î±
       = (Î·_Î±(K^n_0)(H_Î±(Î½Ï€)-1) + 1)^1/Î±
       = (Î·_Î±(K^n_0)H_Î±(Î½Ï€) - Î·_Î±(K^n_0) + 1)^1/Î±
       = H_Î±(Î½Ï€)^1/Î±(Î·_Î±(K^n_0)-Î·_Î±(K^n_0)/H_Î±(Î½Ï€)+1/H_Î±(Î½Ï€))^1/Î±
       =(Î·_Î±(K^n_0) H_Î±(Î½Ï€))^1/Î±(1- 1/H_Î±(Î½Ï€)+Î·_Î±(K^n_0)/H_Î±(Î½Ï€))^1/Î±
       â‰¤ (2(1-(1-Î·_Î±(K))^n_0)H_Î±(Î½Ï€))^1/Î±

   

   

Â§ CONCLUSIONS

   We introduced a novel approach to the concentration of measure for dependent random variables. The generality of our framework allows to consider arbitrary kernels without requiring either stationarity (as opposed toÂ <cit.>) or contractivity (as opposed toÂ <cit.>). Moreover, our technique applies to any family of functions which is known to concentrate when the random variables are actually independent. Said technique is employed and compared to the state of the art in four different settings: finite-state Markov chains, a non-contractive one (the SSRW), a non-Markovian process, and Monte Carlo Markov Chain methods. In each of these settings, we provide a regime of parameters in which we guarantee a McDiarmid-like decay and improve over existing results. 
    The improvement is the most striking in the case of the SSRW, where the only (closed-form) alternative approach gives a constant probability of deviation from the average, as opposed to the exponentially decaying probability guaranteed by our framework.
    The bounds provided are usually characterised by a phase-transition-like behavior in terms of the accuracy t, i.e., one can show concentration only for values of t larger than a threshold depending on the Hellinger integral (and its scaling with respect to the number of variables n). 
    Consider for instanceÂ <Ref>: if t^2> Î²/2nln H^1/Î±_Î±â‰ˆ (1+o_n(1))(Î²ln(2)/2), then the exponent is negative and one has exponential concentration, otherwise the exponent becomes positive and the bound trivialises to something larger than 1. We believe this to be an artifact of the analysis and not an intrinsic property of the concentration of measure phenomenon.
    

Â§ ACKNOWLEDGMENTS

The authors are partially supported by the 2019 Lopez-Loreta Prize. They would also like to thank Professor Jan Maas for providing valuable suggestions and comments on an early version of the work.
IEEEtran







sectionappendix
subsectionappendix



Â§ PROOF OFÂ <REF>



Assume that E={|f-_âŠ—_i=1^n X_i(f)|â‰¥ t}. Then, one has that

    _X^n(E)    = âˆ«1_E d_X^n
       =  âˆ«1_E d_X^n/d_âŠ—_i=1^n X_i d_âŠ—_i=1^n X_i
       â‰¤(âˆ«1_E d_âŠ—_i=1^n X_i)^Î±-1/Î±(âˆ«(d_X^n/d_âŠ—_i=1^n X_i)^Î± d_âŠ—_i=1^n X_i)^1/Î±
       = _âŠ—_i=1^n X_i^1/Î²(E) H_Î±^1/Î±(_X^n_âŠ—_i=1^n X_i),

where the inequality in the third line follows from HÃ¶lder's inequality. Moreover, 

    H_Î±(_X^n_âŠ—_i=1^n X_i)     =  _X_1(_âŠ—_i=2^n X_i(d_X_2^n|X_1/d_âŠ—_i=2^n X_i)^Î±) 
       =_X_1(_X_2((d_X_2|X_1/d_X_2)^Î±(_âŠ—_i=3^n X_i(d_X_3^n|X_2/d_âŠ—_i=3^n X_i)^Î±))) 
       â‰¤_X_1(_X_2((d_X_2|X_1/d_X_2)^Î±Î±_2)^1/Î±_2_X_2((_âŠ—_i=3^n X_i(d_X_3^n|X_2/d_âŠ—_i=3^n X_i)^Î±)^Î²_2)^1/Î²_2)  
       = H_Î±^2 Â·_X_2((_âŠ—_i=3^n X_i(d_X_3^n|X_2/d_âŠ—_i=3^n X_i)^Î±)^Î²_2)^1/Î²_2,

where the inequality follows from HÃ¶lder's inequality, the fact that the expectation is taken with respect to the product measure _âŠ—_i X_i as well as the Markovity of _X^n. H_Î±^2 = _X_1^1/Î²_1(H_Î±Î±_2^Î²_1/Î±_2(_X_2|X_1_X_2)) with Î²_1=1 will now be the only term depending on X_1. Repeating the same sequence of steps (an application of the Disintegration TheoremÂ <cit.> to a Markovian setting, like inÂ <cit.>, followed by HÃ¶lder's inequality) (n-2) more times leads to the product of H_Î±^i as defined in the statement of the theorem.
The result then follows by noticing that 
    ^1/Î²_âŠ—_i X_i(E)â‰¤ 2^1/Î²exp(-2nt^2/Î²),
 by McDiarmid's inequality.  




Â§ CONCENTRATION AROUND MEAN AND MEDIAN

  The following result is a useful tool that allows us to compare concentration around a constant, concentration around the mean and concentration around the median. It is used in order to compare our results with the ones proposed inÂ <cit.>. A proof can be found inÂ <cit.> and the statement is provided here for ease of reference.
    Let f be a measurable function on a probability space (,Î©,Î¼). Assume that, for some a_fâˆˆâ„ and a non-negative function h:â„_+â†’â„_+ such that lim_râ†’âˆ h(r)=0,
    
    Î¼({|f-a_f|â‰¥ r })â‰¤ h(r)
 for all r>0, then
    
    Î¼({|f-m_f|â‰¥ r +r_0})â‰¤ h(r),

    where m_f is the median of f and r_0 is such that h(r_0)<1/2. Moreover, if hÌ… = âˆ«_0^âˆ h(x)dx < âˆ, then f is Î¼-integrable, |a_f-Î¼(f)|â‰¤hÌ… and, for every r>0, 
    
    Î¼({|f-Î¼(f)|â‰¥ r +hÌ…})â‰¤ h(r).

    In particular, if h(r)â‰¤ Cexp(-cr^p) with 0<p<âˆ, then
    
    Î¼({|f-M|â‰¥ r })â‰¤ C'exp(-Îº_pcr^p),

    where C' depends only on C and p, Îº_p depends only on p, and M is either the mean or the median.
    
 

Â§ TENSORISATION

      Many information measures satisfy tensorisation properties, meaning that, if Î½ and Î¼ are measures acting on an n-dimensional space (typically an n-fold Cartesian product of one-dimensional spaces), it is possible to compute the divergence between Î½ and Î¼ using â€œone-dimensional projectionsâ€. This is particularly useful when the second measure is a product-measure. Indeed, if  and  are two probability measures on the space ^n and  is a product-measure, denoting with XÌ…^i the (n-1)-tuple (X_1,â€¦,X_i-1,X_i+1,â€¦,X_n), thenÂ <cit.> gives that
    
    D() â‰¤âˆ‘_i=1^n âˆ« d_XÌ…^i D(Q_X_i|XÌ…^iP_X_i),

    where D() denotes the KL-divergence between  and . 
    Hence, having access to a bound on D(Q_X_i|XÌ…^iP_X_i) for every 1â‰¤ iâ‰¤ n implies a bound on the KL-divergence between the two n-dimensional measures  and . This property is pivotal in proving concentration results in a variety of waysÂ <cit.>. Since of independent interest, we will now state the tensorisation properties of H_Î± as explicit results, as well as the corresponding RÃ©nyi's D_Î± analogues. 
    In particular, whenever the second measure is a product measure while the first one is Markovian, it is possible to prove the following.
    
    Let  and  be two probability measures on the space ^n such that â‰ª, and assume that  is a product measure (i.e., =âŠ—_i=1^n P_i). Assume also that  is a Markov measure induced by Q_1 and the kernels Q_i(Â·|Â·) with 1â‰¤ iâ‰¤ n, i.e., (x^n) = Q_1(x_1)âˆ_i=2^n Q_i(x_i|x_i-1). Moreover, given a constant c, let X_0=c (almost surely) be an auxiliary random variable, then, 
    
    H_Î±(Q) â‰¤âˆ_i=1^n_X_i-1^1/Î²_i-1(H_Î±Î±_i^Î²_i-1/Î±_i(Q_i(Â·|X_i-1)P_X_i)),

    where Î±_iâ‰¥ 1 for iâ‰¥ 0, Î²_0=1, Î±_n=1, and Î²_i = Î±_i/(Î±_i-1). 
    Moreover, selecting Î±_iâ†’ 1^+ which implies Î²_iâ†’âˆ for every iâ‰¥ 1, one recovers 
    
    H_Î±() â‰¤ H_Î±(Q_1P_1)Â·âˆ_i=2^n max_x_i-1âˆˆ H_Î±(Q_i(Â·|x_i-1) P_i).

    
    
        The proof follows from the steps undertaken inÂ eq:startTensoreq:endTensor along with the discussion immediately after, but replacing _X^n with ğ’¬. 
    
    
    Let us denote with h=d/d the density of  with respect to . Denote then with h_1 the marginal density h_1 = âˆ«_^n-1 h d
    
    H_Î±() = (h^Î±)    = ((h_1h_2^n)^Î±) 
       = P_1(h_1^Î± P^2â€¦ n((h_2^n)^Î±)) 
       â‰¤ P_1((h_1^Î±)^Î²)^1/Î² P_1((P^2â€¦ n((h_2^n)^Î±))^Î³)^1/Î³
       = P_1((h_1^Î±)) _P_1P^2â€¦ n((h_2^n)^Î±),

    whereÂ <Ref> follows from Fubini's theorem as well as the fact that  is a product-measure andÂ <Ref> follows from HÃ¶lder's inequality with 1/Î²+1/Î³=1 and Î²,Î³ >1.Â <Ref> follows from letting Î²â†’ 1 and, consequently, Î³â†’âˆ.
    The argument follows from repeating the same procedure in an iterated fashion and noticing that being  a Markovian product-measure, Q_2 (and the corresponding density with respect to P_2) will only depend on x_1 and x_2, Q_3 will only depend on x_2 and x_3 and so on. 
    
    (Could be more formal).Â <Ref>, which involves products of Hellinger integrals, can be re-written as a sum by considering RÃ©nyi's divergences, due to the relationship connecting the two quantities (seeÂ <Ref>). 
    
    Under the same assumptions as inÂ <Ref>, one has that
    
    D_Î±() â‰¤1/Î±-1âˆ‘_i=1^n 1/Î²_i-1log_X_i-1(exp((Î±Î±_i-1)Î²_i-1/Î±_i(D_Î±Î±_i(Q_i(Â·|X_i-1)P_X_i))),

     where Î±_iâ‰¥ 1 for iâ‰¥ 0, Î²_0=1, Î±_n=1 and Î²_i = Î±_i/(Î±_i-1). 
    Moreover, selecting Î±_iâ†’ 1^+ which implies Î²_iâ†’âˆ for every iâ‰¥ 1, one recovers 
        
    D_Î±() â‰¤ D_Î±(Q_1P_1) + âˆ‘_i=2^n max_x_i-1 D_Î±(Q_i(Â·|x_i-1) P_i).

    
    
   
  
    This result allows us to control from above the Hellinger integral of two n-dimensional objects using the Hellinger integral of simpler 1-dimensional objects.
    For instance, if the first measure represents the distribution induced by a time-homogeneous Markov chain, then all the kernels Q_i coincide and the expression becomes even easier to compute, as one can see inÂ <Ref>.
    
    A similar approach can also be employed to provide a result in cases where  is an arbitrary measure.
     
    Let  and  be two probability measures on the space ^n such that â‰ª, and assume that  is a product-measure (i.e., =âŠ—_i=1^n P_i). Then, 
    
    H_Î±() â‰¤ H_Î±(Q_1P_1)Â·âˆ_i=2^n max_x_1^i-1=x_1â€¦ x_i-1âˆˆ^i-1 H_Î±(Q_i(Â·|x_1^i-1) P_i).

    Similarly, one has that 
     
    D_Î±() â‰¤ D_Î±(Q_1P_1) + âˆ‘_i=2^n max_x_1^i-1=x_1â€¦ x_i-1âˆˆğ’³^i-1 D_Î±(Q_i(Â·|x_1^i-1) P_i).

    
    The proof follows from the same argument ofÂ <Ref>. The key difference is that, without making any additional assumptions on , the entire â€œpastâ€ of the process needs to be considered. 
    This is why writing an expression similar toÂ <Ref> without Markovity would be rather cumbersome, and we restrict ourselves to the setting where the additional parameters are all considered to be such that  Î²_jâ†’âˆ. 
    
    Finally, we show some lower bounds on Hellinger integrals and RÃ©nyi's divergences. 
    
        Let  and  be two probability measures on the space ^n such that â‰ª, and assume that  is a product-measure (i.e., =âŠ—_i=1^n P_i). Assume also that  is a Markov measure induced by Q_1 and the kernels Q_i(Â·|Â·) with 1â‰¤ iâ‰¤ n, i.e., (x^n) = Q_1(x_1)âˆ_i=2^n Q_i(x_i|x_i-1). Moreover, given a constant c, let X_0=c (almost surely) be an auxiliary random variable, then, 
    
    H_Î±(Q) â‰¥âˆ_i=1^n_X_i-1^1/Î²_i-1(H_Î±Î±_i^Î²_i-1/Î±_i(Q_i(Â·|X_i-1)P_X_i)),

    and
    
    D_Î±() â‰¥1/Î±-1âˆ‘_i=1^n 1/Î²_i-1log_X_i-1(exp((
            Î±_i)(Î±Î±_i-1)Î²_i-1/Î±_i(D_Î±Î±_i(Q_i(Â·|X_i-1)P_X_i))),

   where Î±_iâ‰¤ 1 for iâ‰¥ 1, Î²_0=1, Î±_n=1 and Î²_i = Î±_i/(Î±_i-1). 
    Moreover, selecting Î±_1â†’1^- which implies Î²_iâ†’-âˆ for every iâ‰¥ 1, one recovers 
    
    H_Î±() â‰¥ H_Î±(Q_1P_1)Â·âˆ_i=2^n min_x_i-1âˆˆ H_Î±(Q_i(Â·|x_i-1) P_i),

    which, in the case of RÃ©nyi's divergences, specialises to
    
    D_Î±() â‰¥ D_Î±(Q_1P_1) + âˆ‘_i=2^n min_x_i-1âˆˆğ’³ D_Î±(Q_i(Â·|x_i-1) P_i).

        
    
   
   
   
    
        The proof follows from similar arguments as  the proof ofÂ <Ref>. The sole difference is that, instead of using HÃ¶lder's inequality at each step, one uses reverse HÃ¶lder's inequality. 
    


Â§ EXPLICIT COMPARISON FOR A BINARY KERNEL

The setting is the following: let K be a time-homogeneous Markov chain  characterised by a doubly-stochastic transition matrix characterised by the vector of parameters Î»Ì…= (Î»_1,â€¦,Î»_m) (i.e., the rows and columns of K are permutations of Î»Ì… with the constraints  âˆ‘_i K_i,j = âˆ‘_j K_i,j = 1). In this case, the Markov chain admits a stationary distribution Ï€ which corresponds to the uniform distribution over the sample space. Hence, if one is considering an m-dimensional space, then Ï€({x})=1/m for xâˆˆ{1,â€¦,m}.
    In this case, if P_1 âˆ¼Ï€, then P_i âˆ¼Ï€ for every iâ‰¥ 1. Moreover, one can prove that K=K^â† and the bound ofÂ <Ref> reduces to 
    _X^n({| f-_âŠ—_n X_n(f)|â‰¥ t ) â‰¤ 2^1/Î²exp(-1/Î²(2nt^2- (n-1)log(m â€–Î»Ì…â€–_â„“^Î±^Î²))),

    Henceforth, we will consider m=2 for simplicity and, thus, â€–Î»Ì…â€–_â„“^Î±:=(âˆ‘_i=1^mÎ»_i^Î±)^1/Î± can be expressed as (Î»^Î± + (1-Î»)^Î±)^1/Î±. SpecialisingÂ <Ref> to this setting, one obtains the following result.
    
    
      Let n>1, and let X_1,â€¦,X_n be a sequence of random variables such that X_1âˆ¼Ï€. For every Î±>1 and every function f such thatÂ <Ref> holds true, one has that  
      
    â„™(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t )â‰¤ 2^1/Î²exp(-2nt^2/Î² +n-1/Î²ln(2((1-Î»)^Î±+Î»^Î±)^1/Î±-1)) .

      


 Â§.Â§ Comparison withÂ <cit.>
The bound obtained via the techniques ofÂ <cit.> is
      
    â„™(| f-_X^n(f)|â‰¥ t)   â‰¤
              2exp(-2Î»^2 nt^2/((1-2Î»)^n-1)^2).

      Let us denote Îº_Î± := ((1-Î»)^Î±+Î»^Î±))^1/Î±-1 <1. Then, by direct comparison, it is possible to see that, whenever 
    t^2>((1-2Î»)^n-1)^2(1-1/n)ln(2Îº_Î±)/2((1-2Î»)^n-1)^2-Î²Î»^2):=tÌ…^2,
 
    then the RHS ofÂ (<ref>) decays faster than the RHS ofÂ (<ref>). In particular, for a given Î»<1/2 and if  Î±>4/3, then
    
    lim_nâ†’+âˆ   ((1-2Î»)^n-1)^2(1-1/n)ln(2Îº_Î±)/2((1-2Î»)^n-1)^2-Î²Î»^2)
       =ln(2 Îº_Î±)/2(1-Î²Î»^2)<2/4-Î²ln2.

    tÌ…^2= (1+o_n(1))ln(2Îº_Î±)/2(1-Î²Î»^2) < (1+o_n(1))2ln2/4-Î².





    
 
   Here, one can explicitly see the trade-off between the probability term and the Hellinger integral, described inÂ <Ref> and mediated by the choice of Î±. Taking the limit Î±â†’âˆ in <Ref> leads to the following upper bound
    
    â„™(| f-_âŠ—_i=1^n X_i(f)|â‰¥ t )â‰¤ 2exp(-2nt^2 + (n-1)ln(2(1-Î»))) .

    In this setting, in order to improve overÂ <Ref>, one needs t^2>tÌ…^2, with
    
    tÌ…^2 = (1+o_n(1))ln(2(1-Î»))/2(1-Î»^2)< (1+o_n(1))2ln2/3.

    Clearly, 1-Î»>Îº_Î± for every Î±>1. Hence, on the one hand, <Ref> introduces a worse multiplicative constant (a larger Î± implies a larger Hellinger integral, and we are considering the limit of Î±â†’âˆ) and increases the minimum value of t one can consider for a given Î»<1/2 (<Ref> is increasing in Î±). On the other hand, it provides a faster exponential decay with n. In fact, as Î²â†’ 1, the RHS of (<ref>) scales as exp(-2nt^2(1+o_n(1))) for large enough t, which matches the behavior ofÂ <Ref>. 
    

 Â§.Â§ Comparison withÂ <cit.>

    In the setting considered above, <cit.> gives



    
    â„™(| f-Ï€^âŠ— n(f)|â‰¥ t ) â‰¤ 2exp(-2Î»/1-Î»nt^2).


    This means that, considering the decay provided byÂ <Ref> (which optimises the speed of decay for large enough t) whenever 
    t^2 > 1/2(1-Î»)/(1-Î»-Î²Î»)ln(2
        Îº_Î±)(1+o_n(1)),

    thenÂ <Ref> decays faster thanÂ <Ref>. 
    

  
   In particular, given Î²>1 one can choose Î» = 1/Î²+1<1/2 and renderÂ <Ref> arbitrarily large. However, with the characterisation provided inÂ <Ref> one can see that in this case, one has that whenever 
    
    t^2 > 1-Î»/2-4Î»ln(2(1-Î»))(1+o_n(1)),

    thenÂ <Ref> leads to a faster decay thanÂ <Ref>. 
    

 Â§.Â§ Comparison withÂ <cit.>
 
    For the kernel considered in this appendix, <Ref> holds with C_n= (n-1)log(2(1-Î»)). Furthermore, for every i and x, xÌ‚, we have that
    TV(_X_i|X_i-1=x,_X_i|X_i-1=xÌ‚)=(1-2Î»). Consequently, assuming t>1/(2Î»)âˆš(ln(1/_X^n(E))/n), <cit.> give
    
    _X^n(E^c_t)    â‰¤exp(-2n(t(2Î») -âˆš(ln(1/_X^n(E))/2n))^2) 
       â‰¤exp(-8nt^2Î»^2+8tÎ»âˆš(nln2/2)).

    ComparingÂ <Ref> with C_n=(n-1)log(2(1-Î»)) withÂ <Ref>, one can see that, whenever
     
    t â‰¥âˆš(2ln(2(1-Î»)))/(1-4Î»^2)(1+o_n(1)),

thenÂ <Ref> improves overÂ <Ref>. 

A similar comparison can be drawn with respect to the tools inÂ <cit.> (in which the degree of dependence is measured differently), but it would lead to a worse bound than that expressed inÂ <Ref>.


 Â§.Â§ MCMC

Considering the same setting detailed in the previous subsections, one can more explicitly characterise the parameters determiningÂ <Ref>. In particular, one has that Ï€ = (1/2,1/2) and the spectral gap is equal to 1-2Î». Consequently, if n_0=0 and one considers Î±â†’âˆ,Â <Ref> becomes:

    â„™(1/nâˆ‘_i=1^n f(X_n_0+i)-Ï€(f) > t ) â‰¤ 2exp(-Î»/1-Î»Â·2nt^2/(b-a)^2)Â·max{Î½({0}),Î½({1})},

whileÂ <Ref> boils down to the following:

    â„™(1/nâˆ‘_i=1^n f(X_n_0+i)-Ï€(f) > t ) â‰¤ 2exp(-2nt^2/(b-a)^2)(2-2Î»)^n-1Â·max{Î½({0}),Î½({1})}.

Making a direct comparison one can see that if

    t^2 â‰¥((1-1/n)ln(2-2Î»))(b-a)^2/21-Î»/1-2Î»,

thenÂ <Ref> provides a faster decay thanÂ <Ref>.


 Â§.Â§ Comparison between SDPI for D_Î±
and hypercontractivity


If Î¼=(1/2,1/2) and K=BSC(Î»), then Î¼ K= Î¼ and one has, following Wyner's notationÂ <cit.>, the so-called Doubly-Symmetric Binary Source â€œDSBS(Î»)â€. In this setting, the hyper-contractivity constant is given by <cit.> 
    Î³^â‹†(Î±)=(1-2Î»)^2(Î±-1)-1 .
Moreover, one can analytically see that, for every Î¼,

    D_Î±(KÎ¼ K)/D_Î±(Î´_0Î¼) < (1-2Î»)^(1+1/Î±)/(1-Î»)^(Î±-1)/Î±.

As Î±â†’1^+, the LHS ofÂ <Ref> approaches a ratio between KL-divergences, while the RHS approaches (1-2Î»)^2, which equals Î·_KL(K)Â <cit.>. Furthermore, taking the limit of Î±â†’âˆ (which optimises the exponential rate of decay), the expression inÂ <Ref> provides an improvement over simply using DPI, whileÂ <Ref> trivialises to Î³^â‹†(+âˆ) = +âˆ. Hence, denoting with E={|f-_âŠ— X_i(f)|â‰¥ t}, one has that, 
for every Î±>1,

    _X^n(E) â‰¤ 2^1/Î²exp(-2nt^2/Î²)Â·exp((1-2Î»)^2(Î±-1)-2/(1-2Î»)^2(Î±-1)-1(n-1)log2)   via <Ref>&<Ref>, 
    exp(1/Î²(1-2Î»)^(1+1/Î±)/(1-Î»)^(Î±-1)/Î± (n-1)log2)    via <Ref>&<Ref>.

One can see that, for Î± large enough, <Ref> improves uponÂ <Ref> for every Î». In fact, taking Î±â†’âˆ gives

    _X^n(E) â‰¤ 2 exp(-2nt^2  )Â·exp((n-1)log2)   via <Ref>&<Ref>, 
    exp((1-2Î»)/(1-Î») (n-1)log2)    via <Ref>&<Ref>.

 


Â§ PROOF OF <REF>



    For every nâ‰¥ 0, we have 
    supp(S_n)=â‹ƒ_j=0^n { n-2j}.
 Moreover, given 0â‰¤ j â‰¤ n, 

    â„™(S_n= n-2j) = nn-j2^-n.
 Furthermore, given xâˆˆsupp(S_n-1) and 0â‰¤ jâ‰¤ n,
    
    â„™_S_n|S_n-1=x(n-2j)= 1/21_{n-2j-x=1} + 1/21_{n-2j-x=-1}.

Therefore, the following chain of inequalities holds:
        
    H_Î± (P_S_n|S_n-1=xP_S_n)    = âˆ‘_j=0^n â„™^Î±_S_n|S_n-1=x(n-2j) Â·â„™_S_n^1-Î±(n-2j) 
       = 2^-Î±(â„™_S_n^1-Î±(x+1)+â„™_S_n^1-Î±(x-1))
       = 2^-Î±((nn+x+1/2 2^-n)^1-Î±+(nn+x-1/2 2^-n)^1-Î±)
       = 2^-Î±+n(Î±-1)((nn+x+1/2)^1-Î±+(nn+x-1/2)^1-Î±)
       â‰¤ 2^-Î±+n(Î±-1)((n/n+x+1/2)^n+x+1/2(1-Î±)+(n/n+x-1/2)^n+x-1/2(1-Î±)) 
    
           
               = 2^-Î±+n(Î±-1)((n+x+1/2n)^n+x+1/2(Î±-1)+(n+x-1/2n)^n+x-1/2(Î±-1))
       â‰¤ 2^-Î±+n(Î±-1)((n+x+1/2n)^n+x+1/2(Î±-1)+(n+x+1/2n)^n+x-1/2(Î±-1)) 
       = 2^-Î±+n(Î±-1)(n+x+1/2n)^n+x-1/2(Î±-1)Â·(1+(n+x+1/2n)^(Î±-1)) 
       â‰¤ 
            2^-Î±+n(Î±-1)+1.
      
        Here, the inequality (<ref>) follows from the fact that nkâ‰¥(n/k)^k along with 1-Î±â‰¤ 0; the inequality (<ref>) follows from the fact that n+x+1/2n>0. 
        Moreover, it is easy to see that  n+x+1/2= n+x-1/2+1 and thus the factorisation inÂ <Ref> follows. To conclude, it suffice to notice that  n+x+1/2nâ‰¤ 1.
        Denoting  with 1/Î²= Î±-1/Î±, the computations just above imply that
    
    H_Î±^1/Î±(_S^n_âŠ—_j=1^n S_j)    â‰¤âˆ_i=2^n max_xâˆˆsuppS_i-1 H_Î±^1/Î±(_S_i|S_i-1=x_S_i) 
       â‰¤âˆ_i=2^n 2^1/Î±-1 + i/Î² =  âˆ_i=2^n 2^-1/Î²+ i/Î² = 2^1/Î²âˆ‘_i=2^n (i-1) = 2^1/Î²âˆ‘_j=1^n-1 j = 2^n(n-1)/2Î².



    

Â§ PROOF OF THE INEQUALITIES IN (<REF>) AND (<REF>)

    
    
Given the setting, denoting with y_n = âˆ‘_i=1^n-1p_ix_i, one has that

    H_Î±(_X^n|X^n-1=x_1^n-1(1/2,1/2))    = (1/2)^1-Î±((1/2+âˆ‘_i=1^n-1p_ix_i)^Î±+(1/2-âˆ‘_i=1^n-1p_ix_i)^Î±) 
       â‰¤(1/2)^1-Î±((1/2+âˆ‘_i=1^n-1p_ix_i)^âŒŠÎ±âŒ‹+(1/2-âˆ‘_i=1^n-1p_ix_i)^âŒŠÎ±âŒ‹) 
       = (1/2)^1-Î±(âˆ‘_k=0^âŒŠÎ±âŒ‹âŒŠÎ±âŒ‹k(1/2)^âŒŠÎ±âŒ‹-ky_n^k + âˆ‘_k=0^âŒŠÎ±âŒ‹âŒŠÎ±âŒ‹k(1/2)^âŒŠÎ±âŒ‹-k(-y_n)^k) 
       = (1/2)^1-Î±(âˆ‘_k=0^âŒŠÎ±âŒ‹âŒŠÎ±âŒ‹k(1/2)^âŒŠÎ±âŒ‹-k(y_n^k +(-y_n)^k))


    = (1/2)^1-Î±(âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j(1/2)^âŒŠÎ±âŒ‹-2j2y_n^2j) 
       â‰¤ 2âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j (2y_n)^2j.

Moreover, setting p_i = 2^-i-1 gives 

    max_x_1^n-1H_Î±(_X^n|X^n-1=x_1^n-1(1/2,1/2))    â‰¤2max_x_1^n-1âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j(2âˆ‘_i=1^n-1p_ix_i)^2j
       =2 âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j(âˆ‘_i=1^n-12^-i)^2j
       =2âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j(1-2^-n+1)^2j
       â‰¤ 2âˆ‘_j=0^âŒŠÎ±/2âŒ‹âŒŠÎ±âŒ‹2j = 2^âŒŠÎ±âŒ‹â‰¤ 2^Î±.


